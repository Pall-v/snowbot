{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, glob, boto3, os\n",
    "import pdb\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://alexwlchan.net/2019/07/listing-s3-keys/\n",
    "def get_matching_s3_objects(bucket, prefix=\"\", suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Generate objects in an S3 bucket.\n",
    "\n",
    "    :param bucket: Name of the S3 bucket.\n",
    "    :param prefix: Only fetch objects whose key starts with\n",
    "        this prefix (optional).\n",
    "    :param suffix: Only fetch objects whose keys end with\n",
    "        this suffix (optional).\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    kwargs = {'Bucket': bucket}\n",
    "\n",
    "    # We can pass the prefix directly to the S3 API.  If the user has passed\n",
    "    # a tuple or list of prefixes, we go through them one by one.\n",
    "    if isinstance(prefix, str):\n",
    "        prefixes = (prefix, )\n",
    "    else:\n",
    "        prefixes = prefix\n",
    "\n",
    "    for key_prefix in prefixes:\n",
    "        kwargs[\"Prefix\"] = key_prefix\n",
    "\n",
    "        for page in paginator.paginate(**kwargs):\n",
    "            try:\n",
    "                contents = page[\"Contents\"]\n",
    "            except KeyError:\n",
    "                return\n",
    "\n",
    "            for obj in contents:\n",
    "                key = obj[\"Key\"]\n",
    "                if key.endswith(suffix):\n",
    "                    yield obj\n",
    "\n",
    "\n",
    "def get_matching_s3_keys(bucket, prefix=\"\", suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Generate the keys in an S3 bucket.\n",
    "\n",
    "    :param bucket: Name of the S3 bucket.\n",
    "    :param prefix: Only fetch keys that start with this prefix (optional).\n",
    "    :param suffix: Only fetch keys that end with this suffix (optional).\n",
    "    \"\"\"\n",
    "    for obj in get_matching_s3_objects(bucket, prefix, suffix):\n",
    "        yield obj[\"Key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "BUCKET_NAME = 'snowbot-pv'\n",
    "\n",
    "# S3 Connect\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "bucket = s3.Bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data/\"\n",
    "MERGED_FILENAME = \"merged_file.json\"\n",
    "merged_file = DATA_DIR + MERGED_FILENAME\n",
    "\n",
    "TEST_FILENAME = \"test_file.json\"\n",
    "merged_test_file = DATA_DIR + TEST_FILENAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_matching_jsons(save_file, suffix=\"\"):\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    for f in get_matching_s3_keys(BUCKET_NAME, suffix=suffix):\n",
    "\n",
    "        # Write the file from S3 into a local temp file\n",
    "        with open('temp', 'wb') as tfw:\n",
    "            bucket.download_fileobj(f, tfw)\n",
    "\n",
    "        # Append the local temp file into the result list\n",
    "        with open('temp', 'rb') as tfr:          \n",
    "            result.append(json.load(tfr))\n",
    "\n",
    "    os.remove(\"temp\")\n",
    "\n",
    "    # Fill the output file with the merged content\n",
    "    with open(save_file, \"w\") as outfile:\n",
    "         json.dump(result, outfile)\n",
    "            \n",
    "# TBD: more efficient to go straight to df w/o saving json to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_merged_json_as_df(merged_file):\n",
    "    #load the merged json as a dataframe\n",
    "    with open(merged_file, \"r\") as f:\n",
    "        d = json.load(f)\n",
    "        df = pd.DataFrame.from_dict(json_normalize(d, record_path='lifts', meta='timestamp'))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_lifts_df_datatypes(df):\n",
    "\n",
    "    # set datatypes for lift table\n",
    "    df = df.astype({\n",
    "        \"liftID\": 'category',\n",
    "        \"resortID\": 'category',\n",
    "        \"liftName\": 'category',\n",
    "        \"status\": 'category',\n",
    "        \"timeToRide\": \"int\"\n",
    "    })\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_status_changes(df):\n",
    "    '''Returns a dataframe that only includes the times when there was a change to a new status'''\n",
    "    \n",
    "    def calc_status_change(df):\n",
    "        firstrow = df.loc[df['timestamp'].idxmin()]\n",
    "        change_rows = df[df.status.ne(df.status.shift())]\n",
    "        keep_df = firstrow.to_frame().T.append(change_rows)\n",
    "        \n",
    "        # Remove so that we don't need to write another column to S3 as we scrape?\n",
    "        # Just calculate it when plotting and predicting?\n",
    "        # keep_df['time_diff'] = keep_df['timestamp'].diff(1).shift(-1)\n",
    "        \n",
    "        return keep_df\n",
    "        \n",
    "    \n",
    "    df = df.groupby('liftName', group_keys=False)\\\n",
    "           .apply(calc_status_change)\\\n",
    "           .reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# TBD: may need to convert timestamp to days (e.g. for Tableau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_matching_jsons(suffix=\"lifts.json\", save_file=merged_file)\n",
    "lifts_df = load_merged_json_as_df(merged_file)\n",
    "lifts_df = set_lifts_df_datatypes(lifts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Blackcomb Gondola Lower, Blackcomb Gondola Upper, Excalibur Gondola Lower, Excalibur Gondola Upper, Excelerator Express, ..., Franz's Chair, Peak Express, Harmony 6 Express, Symphony Express, T-Bars]\n",
       "Length: 28\n",
       "Categories (28, object): [Blackcomb Gondola Lower, Blackcomb Gondola Upper, Excalibur Gondola Lower, Excalibur Gondola Upper, ..., Peak Express, Harmony 6 Express, Symphony Express, T-Bars]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifts_df['liftName'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Blackcomb Gondola Lower, Blackcomb Gondola Upper, Excalibur Gondola Lower, Excalibur Gondola Upper, Excelerator Express, ..., Franz's Chair, Peak Express, Harmony 6 Express, Symphony Express, T-Bars]\n",
       "Length: 28\n",
       "Categories (28, object): [Blackcomb Gondola Lower, Blackcomb Gondola Upper, Excalibur Gondola Lower, Excalibur Gondola Upper, ..., Peak Express, Harmony 6 Express, Symphony Express, T-Bars]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifts_status_changes_df = get_status_changes(lifts_df)\n",
    "lifts_df['liftName'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** `timeToRide` is just the time is takes to ride the lift, not the current wait time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "liftName\n",
       "7th Heaven Express                 [6]\n",
       "Big Red Express                    [8]\n",
       "Blackcomb Gondola Lower            [7]\n",
       "Blackcomb Gondola Upper            [7]\n",
       "Catskinner Express                 [4]\n",
       "Coca-Cola Tube Park                [4]\n",
       "Creekside Gondola                  [7]\n",
       "Crystal Ridge Express              [7]\n",
       "Emerald 6 Express                  [6]\n",
       "Excalibur Gondola Lower            [3]\n",
       "Excalibur Gondola Upper            [5]\n",
       "Excelerator Express                [6]\n",
       "Fitzsimmons Express                [6]\n",
       "Franz's Chair                      [8]\n",
       "Garbanzo Express                   [7]\n",
       "Glacier Express                    [6]\n",
       "Harmony 6 Express                  [6]\n",
       "Horstman T-Bar                     [4]\n",
       "Jersey Cream Express               [5]\n",
       "Magic Chair                        [6]\n",
       "Olympic Chair                      [5]\n",
       "Peak 2 Peak Gondola               [12]\n",
       "Peak Express                       [3]\n",
       "Showcase T-Bar                     [3]\n",
       "Symphony Express                   [7]\n",
       "T-Bars                             [5]\n",
       "Whistler Village Gondola Lower     [5]\n",
       "Whistler Village Gondola Upper    [11]\n",
       "Name: timeToRide, dtype: object"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifts_df.groupby(\"liftName\")['timeToRide'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get status durations\n",
    "# TBD: optimize if needed via # 3 under:\n",
    "# https://towardsdatascience.com/pandas-tips-and-tricks-33bcc8a40bb9\n",
    "df = lifts_status_changes_df.sort_values(by=['liftID', 'timestamp'])\n",
    "df['time_diff'] = df.groupby('liftID')['timestamp'].diff(1).shift(-1)\n",
    "\n",
    "# Fill in the durations which will be missing for the most recent status changes\n",
    "missing_time_diffs_idx = df.loc[(df['time_diff'].isnull()) & (\n",
    "    df['timestamp'] >= df['timestamp'].min()), 'timestamp'].index.values\n",
    "\n",
    "df.loc[missing_time_diffs_idx, 'time_diff'] = df['timestamp'].max(\n",
    ") - df.loc[missing_time_diffs_idx, 'timestamp']\n",
    "\n",
    "# Convert to seconds\n",
    "df['time_diff_seconds'] = df['time_diff'].dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>liftID</th>\n",
       "      <th>resortID</th>\n",
       "      <th>liftName</th>\n",
       "      <th>status</th>\n",
       "      <th>timeToRide</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time_diff</th>\n",
       "      <th>time_diff_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-03 00:19:09.631011-08:00</td>\n",
       "      <td>1 days 13:59:59.794440</td>\n",
       "      <td>136799.794440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-04 14:19:09.425451-08:00</td>\n",
       "      <td>0 days 00:29:59.297179</td>\n",
       "      <td>1799.297179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-04 14:49:08.722630-08:00</td>\n",
       "      <td>0 days 22:00:01.500183</td>\n",
       "      <td>79201.500183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-05 12:49:10.222813-08:00</td>\n",
       "      <td>0 days 01:59:58.326201</td>\n",
       "      <td>7198.326201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-05 14:49:08.549014-08:00</td>\n",
       "      <td>0 days 18:56:18.469982</td>\n",
       "      <td>68178.469982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Upper</td>\n",
       "      <td>H</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-01-10 07:45:26.349643-08:00</td>\n",
       "      <td>0 days 00:44:59.829894</td>\n",
       "      <td>2699.829894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Upper</td>\n",
       "      <td>O</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-01-10 08:30:26.179537-08:00</td>\n",
       "      <td>0 days 06:45:00.093875</td>\n",
       "      <td>24300.093875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Upper</td>\n",
       "      <td>X</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-01-10 15:15:26.273412-08:00</td>\n",
       "      <td>0 days 16:30:00.262163</td>\n",
       "      <td>59400.262163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Upper</td>\n",
       "      <td>H</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-01-11 07:45:26.535575-08:00</td>\n",
       "      <td>0 days 00:30:04.092417</td>\n",
       "      <td>1804.092417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Upper</td>\n",
       "      <td>O</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-01-11 08:15:30.627992-08:00</td>\n",
       "      <td>0 days 01:59:56.195736</td>\n",
       "      <td>7196.195736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>557 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    liftID resortID                        liftName status timeToRide  \\\n",
       "0        3       13              7th Heaven Express      X          6   \n",
       "1        3       13              7th Heaven Express      O          6   \n",
       "2        3       13              7th Heaven Express      X          6   \n",
       "3        3       13              7th Heaven Express      O          6   \n",
       "4        3       13              7th Heaven Express      X          6   \n",
       "..     ...      ...                             ...    ...        ...   \n",
       "552     72       13  Whistler Village Gondola Upper      H         11   \n",
       "553     72       13  Whistler Village Gondola Upper      O         11   \n",
       "554     72       13  Whistler Village Gondola Upper      X         11   \n",
       "555     72       13  Whistler Village Gondola Upper      H         11   \n",
       "556     72       13  Whistler Village Gondola Upper      O         11   \n",
       "\n",
       "                           timestamp              time_diff  time_diff_seconds  \n",
       "0   2020-01-03 00:19:09.631011-08:00 1 days 13:59:59.794440      136799.794440  \n",
       "1   2020-01-04 14:19:09.425451-08:00 0 days 00:29:59.297179        1799.297179  \n",
       "2   2020-01-04 14:49:08.722630-08:00 0 days 22:00:01.500183       79201.500183  \n",
       "3   2020-01-05 12:49:10.222813-08:00 0 days 01:59:58.326201        7198.326201  \n",
       "4   2020-01-05 14:49:08.549014-08:00 0 days 18:56:18.469982       68178.469982  \n",
       "..                               ...                    ...                ...  \n",
       "552 2020-01-10 07:45:26.349643-08:00 0 days 00:44:59.829894        2699.829894  \n",
       "553 2020-01-10 08:30:26.179537-08:00 0 days 06:45:00.093875       24300.093875  \n",
       "554 2020-01-10 15:15:26.273412-08:00 0 days 16:30:00.262163       59400.262163  \n",
       "555 2020-01-11 07:45:26.535575-08:00 0 days 00:30:04.092417        1804.092417  \n",
       "556 2020-01-11 08:15:30.627992-08:00 0 days 01:59:56.195736        7196.195736  \n",
       "\n",
       "[557 rows x 8 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "liftID                                               object\n",
       "resortID                                             object\n",
       "liftName                                             object\n",
       "status                                               object\n",
       "timeToRide                                           object\n",
       "timestamp            datetime64[ns, pytz.FixedOffset(-480)]\n",
       "time_diff                                   timedelta64[ns]\n",
       "time_diff_seconds                                   float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses locale date formatting, otherwise Tableau will mix up month and day\n",
    "# alternatively, can export to json:\n",
    "# lifts_status_changes_df.to_json(DATA_DIR + \"lifts_status_changes.json\", orient='table')\n",
    "df.to_csv(DATA_DIR + \"lifts_status_changes.csv\", date_format='%c')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add:\n",
    "# \n",
    "# daily: for each chair calculate most open status of the day: O > H > X\n",
    "# Days since each chair was last seen open with timestamp of most recent open time.\n",
    "# snowfall since last open\n",
    "# save data for other mountains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage options testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(DATA_DIR + \"df_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastparquet import write\n",
    "\n",
    "# parquet engines don't handle shifted timezones\n",
    "import pytz\n",
    "TZ = pytz.timezone('America/Vancouver')\n",
    "df['timestamp'] = df.timestamp.dt.tz_convert(pytz.utc)\n",
    "\n",
    "# Note: May need snappy-python as a req to run on AWS Lambda\n",
    "df.to_parquet(DATA_DIR + \"df_test.parquet\", engine='fastparquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_df = pd.read_parquet(DATA_DIR + \"df_test.parquet\")\n",
    "load_df['timestamp'] = load_df.timestamp.dt.tz_convert(TZ) # convert back to correct timezone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "liftID                                    int64\n",
       "resortID                                  int64\n",
       "liftName                                 object\n",
       "status                                   object\n",
       "timeToRide                                int64\n",
       "timestamp     datetime64[ns, America/Vancouver]\n",
       "dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TBD convert back to correct datatypes\n",
    "load_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(DATA_DIR + \"df_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test file size results:\n",
    "- json: 800 Kb?\n",
    "- csv: 474 Kb\n",
    "- pickle: 145 Kb\n",
    "- parquet: 15 Kb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Lake Testing\n",
    "\n",
    "Requires apache spark instance.  For future use, could set one up to work with lambda using https://aws.amazon.com/emr/features/spark/?\n",
    "\n",
    "Otherwise databricks (similar to QxMD project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# json comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = json.loads(\"\"\"\n",
    "{\n",
    "    \"timestamp\": \"2020-01-03 00:19:09.631011-08:00\", \"lifts\": [{\"liftID\": 69, \"resortID\": 13, \"liftName\": \"Blackcomb Gondola Lower\", \"status\": \"X\", \"timeToRide\": \"7\"}, {\"liftID\": 70, \"resortID\": 13, \"liftName\": \"Blackcomb Gondola Upper\", \"status\": \"X\", \"timeToRide\": \"7\"}, {\"liftID\": 5, \"resortID\": 13, \"liftName\": \"Excalibur Gondola Lower\", \"status\": \"X\", \"timeToRide\": \"3\"}, {\"liftID\": 71, \"resortID\": 13, \"liftName\": \"Excalibur Gondola Upper\", \"status\": \"X\", \"timeToRide\": \"5\"}, {\"liftID\": 8, \"resortID\": 13, \"liftName\": \"Excelerator Express\", \"status\": \"X\", \"timeToRide\": \"6\"}, {\"liftID\": 6, \"resortID\": 13, \"liftName\": \"Magic Chair\", \"status\": \"X\", \"timeToRide\": \"6\"}, {\"liftID\": 4, \"resortID\": 13, \"liftName\": \"Jersey Cream Express\", \"status\": \"X\", \"timeToRide\": \"5\"}, {\"liftID\": 9, \"resortID\": 13, \"liftName\": \"Catskinner Express\", \"status\": \"X\", \"timeToRide\": \"4\"}, {\"liftID\": 22, \"resortID\": 13, \"liftName\": \"Peak 2 Peak Gondola\", \"status\": \"X\", \"timeToRide\": \"12\"}, {\"liftID\": 10, \"resortID\": 13, \"liftName\": \"Crystal Ridge Express\", \"status\": \"X\", \"timeToRide\": \"7\"}, {\"liftID\": 7, \"resortID\": 13, \"liftName\": \"Glacier Express\", \"status\": \"X\", \"timeToRide\": \"6\"}, {\"liftID\": 3, \"resortID\": 13, \"liftName\": \"7th Heaven Express\", \"status\": \"X\", \"timeToRide\": \"6\"}, {\"liftID\": 12, \"resortID\": 13, \"liftName\": \"Showcase T-Bar\", \"status\": \"X\", \"timeToRide\": \"3\"}, {\"liftID\": 11, \"resortID\": 13, \"liftName\": \"Horstman T-Bar\", \"status\": \"X\", \"timeToRide\": \"4\"}, {\"liftID\": 14, \"resortID\": 13, \"liftName\": \"Coca-Cola Tube Park\", \"status\": \"X\", \"timeToRide\": \"4\"}, {\"liftID\": 33, \"resortID\": 13, \"liftName\": \"Whistler Village Gondola Lower\", \"status\": \"X\", \"timeToRide\": \"5\"}, {\"liftID\": 72, \"resortID\": 13, \"liftName\": \"Whistler Village Gondola Upper\", \"status\": \"X\", \"timeToRide\": \"11\"}, {\"liftID\": 45, \"resortID\": 13, \"liftName\": \"Fitzsimmons Express\", \"status\": \"X\", \"timeToRide\": \"6\"}, {\"liftID\": 40, \"resortID\": 13, \"liftName\": \"Garbanzo Express\", \"status\": \"X\", \"timeToRide\": \"7\"}, {\"liftID\": 34, \"resortID\": 13, \"liftName\": \"Creekside Gondola\", \"status\": \"X\", \"timeToRide\": \"7\"}, {\"liftID\": 36, \"resortID\": 13, \"liftName\": \"Big Red Express\", \"status\": \"X\", \"timeToRide\": \"8\"}, {\"liftID\": 35, \"resortID\": 13, \"liftName\": \"Emerald 6 Express\", \"status\": \"X\", \"timeToRide\": \"6\"}, {\"liftID\": 22, \"resortID\": 13, \"liftName\": \"Peak 2 Peak Gondola\", \"status\": \"X\", \"timeToRide\": \"12\"}, {\"liftID\": 39, \"resortID\": 13, \"liftName\": \"Olympic Chair\", \"status\": \"X\", \"timeToRide\": \"5\"}, {\"liftID\": 44, \"resortID\": 13, \"liftName\": \"Franz's Chair\", \"status\": \"X\", \"timeToRide\": \"8\"}, {\"liftID\": 43, \"resortID\": 13, \"liftName\": \"Peak Express\", \"status\": \"X\", \"timeToRide\": \"3\"}, {\"liftID\": 37, \"resortID\": 13, \"liftName\": \"Harmony 6 Express\", \"status\": \"X\", \"timeToRide\": \"6\"}, {\"liftID\": 42, \"resortID\": 13, \"liftName\": \"Symphony Express\", \"status\": \"X\", \"timeToRide\": \"7\"}, {\"liftID\": 41, \"resortID\": 13, \"liftName\": \"T-Bars\", \"status\": \"X\", \"timeToRide\": \"5\"}]\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# Changed Blackcomb Gondola Lower status to \"O\"\n",
    "b = json.loads(\"\"\"\n",
    "{\n",
    "    \"timestamp\": \"2020-01-03 00:19:09.631011-08:00\", \"lifts\": [{\"liftID\": 69, \"resortID\": 13, \"liftName\": \"Blackcomb Gondola Lower\", \"status\": \"O\", \"timeToRide\": \"7\"}, {\"liftID\": 70, \"resortID\": 13, \"liftName\": \"Blackcomb Gondola Upper\", \"status\": \"X\", \"timeToRide\": \"7\"}, {\"liftID\": 5, \"resortID\": 13, \"liftName\": \"Excalibur Gondola Lower\", \"status\": \"X\", \"timeToRide\": \"3\"}, {\"liftID\": 71, \"resortID\": 13, \"liftName\": \"Excalibur Gondola Upper\", \"status\": \"X\", \"timeToRide\": \"5\"}, {\"liftID\": 8, \"resortID\": 13, \"liftName\": \"Excelerator Express\", \"status\": \"X\", \"timeToRide\": \"6\"}, {\"liftID\": 6, \"resortID\": 13, \"liftName\": \"Magic Chair\", \"status\": \"X\", \"timeToRide\": \"6\"}, {\"liftID\": 4, \"resortID\": 13, \"liftName\": \"Jersey Cream Express\", \"status\": \"X\", \"timeToRide\": \"5\"}, {\"liftID\": 9, \"resortID\": 13, \"liftName\": \"Catskinner Express\", \"status\": \"X\", \"timeToRide\": \"4\"}, {\"liftID\": 22, \"resortID\": 13, \"liftName\": \"Peak 2 Peak Gondola\", \"status\": \"X\", \"timeToRide\": \"12\"}, {\"liftID\": 10, \"resortID\": 13, \"liftName\": \"Crystal Ridge Express\", \"status\": \"X\", \"timeToRide\": \"7\"}, {\"liftID\": 7, \"resortID\": 13, \"liftName\": \"Glacier Express\", \"status\": \"X\", \"timeToRide\": \"6\"}, {\"liftID\": 3, \"resortID\": 13, \"liftName\": \"7th Heaven Express\", \"status\": \"X\", \"timeToRide\": \"6\"}, {\"liftID\": 12, \"resortID\": 13, \"liftName\": \"Showcase T-Bar\", \"status\": \"X\", \"timeToRide\": \"3\"}, {\"liftID\": 11, \"resortID\": 13, \"liftName\": \"Horstman T-Bar\", \"status\": \"X\", \"timeToRide\": \"4\"}, {\"liftID\": 14, \"resortID\": 13, \"liftName\": \"Coca-Cola Tube Park\", \"status\": \"X\", \"timeToRide\": \"4\"}, {\"liftID\": 33, \"resortID\": 13, \"liftName\": \"Whistler Village Gondola Lower\", \"status\": \"X\", \"timeToRide\": \"5\"}, {\"liftID\": 72, \"resortID\": 13, \"liftName\": \"Whistler Village Gondola Upper\", \"status\": \"X\", \"timeToRide\": \"11\"}, {\"liftID\": 45, \"resortID\": 13, \"liftName\": \"Fitzsimmons Express\", \"status\": \"X\", \"timeToRide\": \"6\"}, {\"liftID\": 40, \"resortID\": 13, \"liftName\": \"Garbanzo Express\", \"status\": \"X\", \"timeToRide\": \"7\"}, {\"liftID\": 34, \"resortID\": 13, \"liftName\": \"Creekside Gondola\", \"status\": \"X\", \"timeToRide\": \"7\"}, {\"liftID\": 36, \"resortID\": 13, \"liftName\": \"Big Red Express\", \"status\": \"X\", \"timeToRide\": \"8\"}, {\"liftID\": 35, \"resortID\": 13, \"liftName\": \"Emerald 6 Express\", \"status\": \"X\", \"timeToRide\": \"6\"}, {\"liftID\": 22, \"resortID\": 13, \"liftName\": \"Peak 2 Peak Gondola\", \"status\": \"X\", \"timeToRide\": \"12\"}, {\"liftID\": 39, \"resortID\": 13, \"liftName\": \"Olympic Chair\", \"status\": \"X\", \"timeToRide\": \"5\"}, {\"liftID\": 44, \"resortID\": 13, \"liftName\": \"Franz's Chair\", \"status\": \"X\", \"timeToRide\": \"8\"}, {\"liftID\": 43, \"resortID\": 13, \"liftName\": \"Peak Express\", \"status\": \"X\", \"timeToRide\": \"3\"}, {\"liftID\": 37, \"resortID\": 13, \"liftName\": \"Harmony 6 Express\", \"status\": \"X\", \"timeToRide\": \"6\"}, {\"liftID\": 42, \"resortID\": 13, \"liftName\": \"Symphony Express\", \"status\": \"X\", \"timeToRide\": \"7\"}, {\"liftID\": 41, \"resortID\": 13, \"liftName\": \"T-Bars\", \"status\": \"X\", \"timeToRide\": \"5\"}]\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_matching_jsons(suffix=\"test.json\", save_file=merged_test_file)\n",
    "df_test = load_merged_json_as_df(merged_test_file)\n",
    "df_test = set_lifts_df_datatypes(df_test)\n",
    "lifts_status_changes_df_test = get_status_changes(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>liftID</th>\n",
       "      <th>resortID</th>\n",
       "      <th>liftName</th>\n",
       "      <th>status</th>\n",
       "      <th>timeToRide</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-03 00:19:09.631011-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-04 14:19:09.425451-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-04 14:49:08.722630-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-05 12:49:10.222813-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-05 14:49:08.549014-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>Showcase T-Bar</td>\n",
       "      <td>X</td>\n",
       "      <td>3</td>\n",
       "      <td>2020-01-03 00:19:09.631011-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>42</td>\n",
       "      <td>13</td>\n",
       "      <td>Symphony Express</td>\n",
       "      <td>X</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-01-03 00:19:09.631011-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>41</td>\n",
       "      <td>13</td>\n",
       "      <td>T-Bars</td>\n",
       "      <td>X</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-01-03 00:19:09.631011-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>33</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Lower</td>\n",
       "      <td>X</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-01-03 00:19:09.631011-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Upper</td>\n",
       "      <td>X</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-01-03 00:19:09.631011-08:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   liftID resortID                        liftName status timeToRide  \\\n",
       "0       3       13              7th Heaven Express      X          6   \n",
       "1       3       13              7th Heaven Express      O          6   \n",
       "2       3       13              7th Heaven Express      X          6   \n",
       "3       3       13              7th Heaven Express      O          6   \n",
       "4       3       13              7th Heaven Express      X          6   \n",
       "..    ...      ...                             ...    ...        ...   \n",
       "24     12       13                  Showcase T-Bar      X          3   \n",
       "25     42       13                Symphony Express      X          7   \n",
       "26     41       13                          T-Bars      X          5   \n",
       "27     33       13  Whistler Village Gondola Lower      X          5   \n",
       "28     72       13  Whistler Village Gondola Upper      X         11   \n",
       "\n",
       "                          timestamp  \n",
       "0  2020-01-03 00:19:09.631011-08:00  \n",
       "1  2020-01-04 14:19:09.425451-08:00  \n",
       "2  2020-01-04 14:49:08.722630-08:00  \n",
       "3  2020-01-05 12:49:10.222813-08:00  \n",
       "4  2020-01-05 14:49:08.549014-08:00  \n",
       "..                              ...  \n",
       "24 2020-01-03 00:19:09.631011-08:00  \n",
       "25 2020-01-03 00:19:09.631011-08:00  \n",
       "26 2020-01-03 00:19:09.631011-08:00  \n",
       "27 2020-01-03 00:19:09.631011-08:00  \n",
       "28 2020-01-03 00:19:09.631011-08:00  \n",
       "\n",
       "[546 rows x 6 columns]"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifts_status_changes_df.append(lifts_status_changes_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current lift status info json\n",
    "# retrieve prior lift status info json\n",
    "# if current == prior end\n",
    "    # else:\n",
    "        # merge_json(current and prior)\n",
    "        # get status changes\n",
    "        # append status changes to S3 lift history parquet table (create if it doesn't exist)\n",
    "            # append via https://github.com/pandas-dev/pandas/issues/20638#issuecomment-386754025 ?\n",
    "        # save current lift status info json as prior\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"./snowbot_AWS_lambda/\")\n",
    "\n",
    "from scrape import get_data\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/paul/dev/snowbot\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsons_to_df(jsons):\n",
    "    # repeats. functionize first occurace?\n",
    "    return pd.DataFrame.from_dict(json_normalize(jsons, record_path='lifts', meta='timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current lift status info json\n",
    "lifts_current = get_data()['lifts'] # String.  \n",
    "lifts_current_json = json.loads(lifts_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded prior json data from S3\n",
      "No differences between current and prior data were found.\n"
     ]
    }
   ],
   "source": [
    "# retrieve prior lift status info json\n",
    "\n",
    "import botocore\n",
    "\n",
    "\n",
    "def save_prior(json_data):\n",
    "    bucket.put_object(Key=\"lifts_prior.json\", Body=bytes(json.dumps(json_data).encode('UTF-8')))\n",
    "\n",
    "\n",
    "lifts_prior_object = s3.Object(BUCKET_NAME, 'lifts_prior.json')\n",
    "lifts_history_object = s3.Object(BUCKET_NAME, 'lifts_history.json')\n",
    "\n",
    "try:\n",
    "    lifts_prior_object.load()\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == \"404\":\n",
    "\n",
    "        print(\"Prior doesn't exist\")\n",
    "        save_prior(lifts_current_json)  # Create the prior file\n",
    "        print(\"Created PRIOR_FILENAME_TBD\")\n",
    "    else:\n",
    "        # Something else has gone wrong.\n",
    "        raise\n",
    "else:\n",
    "    # The prior exists\n",
    "    file_content = lifts_prior_object.get()['Body'].read().decode('utf-8')\n",
    "    json_content = json.loads(file_content)\n",
    "    print(\"loaded prior json data from S3\")\n",
    "    \n",
    "    # compare jsons without their timestamps\n",
    "    if json_content['lifts'] == lifts_current_json['lifts']:\n",
    "        print(\"No differences between current and prior data were found.\")\n",
    "    else:\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Get a df with the status chages between the prior and current json data\n",
    "df = jsons_to_df([json_content, lifts_current_json])\n",
    "df = set_lifts_df_datatypes(df)\n",
    "df = get_status_changes(df)\n",
    "\n",
    "\n",
    "        # append status changes to S3 lift history parquet table (create if it doesn't exist)\n",
    "            # append via https://github.com/pandas-dev/pandas/issues/20638#issuecomment-386754025 ?\n",
    "        # save current lift status info json as prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "liftID                                      category\n",
       "resortID                                    category\n",
       "liftName                                    category\n",
       "status                                      category\n",
       "timeToRide                                     int64\n",
       "timestamp     datetime64[ns, pytz.FixedOffset(-480)]\n",
       "dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = set_lifts_df_datatypes(df)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastparquet import write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet engines don't handle shifted timezones\n",
    "import pytz\n",
    "TZ = pytz.timezone('America/Vancouver')\n",
    "\n",
    "\n",
    "def save_parquet(df, fname):\n",
    "    df.loc[:, 'timestamp'] = df.loc[:, 'timestamp'].dt.tz_convert(pytz.utc)\n",
    "\n",
    "    # Note: May need snappy-python as a req to run on AWS Lambda\n",
    "    df.to_parquet(DATA_DIR + fname + '.parquet',\n",
    "                  engine='fastparquet',\n",
    "                  partition_on=['timestamp'],\n",
    "                  file_scheme='mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_parquet(df[0:3].copy(), 'wb_lifts_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[20:22, :].copy().to_parquet(DATA_DIR + 'wb_lifts_history' + '.parquet',\n",
    "              engine='fastparquet',\n",
    "              partition_on=['timestamp'],\n",
    "              file_scheme='mixed',\n",
    "              append=True)\n",
    "# Catch exception that is doesn't exist here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 's3.Bucket' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-85d5926afd6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m df[0:3].to_parquet(bucket + 'wb_lifts_history' + '.parquet',\n\u001b[0m\u001b[1;32m      7\u001b[0m               \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fastparquet'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m               \u001b[0mpartition_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 's3.Bucket' and 'str'"
     ]
    }
   ],
   "source": [
    "import s3fs\n",
    "fs = s3fs.S3FileSystem()\n",
    "myopen = fs.open\n",
    "nop = lambda *args, **kwargs: None\n",
    "\n",
    "df[0:3].to_parquet(bucket + 'wb_lifts_history' + '.parquet',\n",
    "              engine='fastparquet',\n",
    "              partition_on=['timestamp'],\n",
    "              file_scheme='hive',\n",
    "              append=True,\n",
    "              open_with=myopen,\n",
    "              mkdirs=nop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 3 records to wb_lifts_history\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Seek only available in read mode",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-8da356e7a12e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mwrite_dataframe_to_parquet_on_s3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb_lifts_history'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-8da356e7a12e>\u001b[0m in \u001b[0;36mwrite_dataframe_to_parquet_on_s3\u001b[0;34m(dataframe, fname)\u001b[0m\n\u001b[1;32m     10\u001b[0m                          \u001b[0mappend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                          \u001b[0mopen_with\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyopen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                          mkdirs=nop)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_parquet\u001b[0;34m(self, fname, engine, compression, index, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m   2235\u001b[0m             \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2236\u001b[0m             \u001b[0mpartition_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartition_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2237\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2238\u001b[0m         )\n\u001b[1;32m   2239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mpartition_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartition_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, df, path, compression, index, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mwrite_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mpartition_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartition_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             )\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastparquet/writer.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(filename, data, row_group_offsets, compression, file_scheme, open_with, mkdirs, has_nulls, write_index, partition_on, fixed_text, append, object_encoding, times)\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfile_scheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'hive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'drill'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m             \u001b[0mpf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParquetFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen_with\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen_with\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_scheme\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'hive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'empty'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'flat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m                 raise ValueError('Requested file scheme is %s, but '\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastparquet/api.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fn, verify, open_with, root, sep)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'read'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;31m# file-like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_scheme\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'simple'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'empty'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 raise ValueError('Cannot use file-like input '\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastparquet/api.py\u001b[0m in \u001b[0;36m_parse_header\u001b[0;34m(self, f, verify)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_parse_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb'PAR1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fsspec/spec.py\u001b[0m in \u001b[0;36mseek\u001b[0;34m(self, loc, whence)\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Seek only available in read mode\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwhence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0mnloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Seek only available in read mode"
     ]
    }
   ],
   "source": [
    "def write_dataframe_to_parquet_on_s3(dataframe, fname):\n",
    "    \"\"\" Write a dataframe to a Parquet on S3 \"\"\"\n",
    "    dataframe.loc[:, 'timestamp'] = dataframe.loc[:, 'timestamp'].dt.tz_convert(pytz.utc)\n",
    "    print(\"Writing {} records to {}\".format(len(dataframe), fname))\n",
    "    output_file = \"s3://{BUCKET_NAME}/{fname}.parquet\"\n",
    "    dataframe.to_parquet(output_file,\n",
    "                         engine='fastparquet',\n",
    "                         partition_on=['timestamp'],\n",
    "                         file_scheme='mixed',\n",
    "                         #append=True,\n",
    "                         open_with=myopen,\n",
    "                         mkdirs=nop)\n",
    "\n",
    "\n",
    "write_dataframe_to_parquet_on_s3(df[0:3].copy(), 'wb_lifts_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file not found.  Creating file wb_lifts_history\n",
      "Writing 5 records to wb_lifts_history\n"
     ]
    }
   ],
   "source": [
    "##########   Working version    ####\n",
    " \n",
    "from fastparquet import ParquetFile\n",
    "\n",
    "\n",
    "def write_dataframe_to_parquet_on_s3(df, fname):\n",
    "    \"\"\" Write a dataframe to a Parquet on S3 \"\"\"\n",
    "    df.loc[:, 'timestamp'] = df.loc[:, 'timestamp'].dt.tz_convert(pytz.utc)\n",
    "    \n",
    "    def write_parquet(df, fname, app = True):\n",
    "        try:\n",
    "            output_file = f\"s3://{BUCKET_NAME}/{fname}.parquet\"\n",
    "            write(output_file,\n",
    "                  df,\n",
    "                  partition_on=['timestamp'],\n",
    "                  file_scheme='hive',\n",
    "                  append = app, # need to remove or catch exception to work when file doesn't exist\n",
    "                  open_with=myopen,\n",
    "                  mkdirs=nop)\n",
    "        except FileNotFoundError:\n",
    "            print(\"File not found.  Creating file {}\".format(fname))\n",
    "            write_parquet(df, fname, app = False)\n",
    "        except:\n",
    "            raise\n",
    "        else:\n",
    "            print(\"Writing {} records to {}\".format(len(df), fname))\n",
    "\n",
    "    write_parquet(df, fname)\n",
    "\n",
    "write_dataframe_to_parquet_on_s3(df[5:10].copy(), 'wb_lifts_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'wb_lifts_history'\n",
    "read_file = f\"s3://{BUCKET_NAME}/{fname}.parquet\"\n",
    "\n",
    "pf = ParquetFile(read_file, open_with=myopen)\n",
    "load_df = pf.to_pandas()\n",
    "load_df = set_lifts_df_datatypes(load_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "liftID                   category\n",
       "resortID                 category\n",
       "liftName                 category\n",
       "status                   category\n",
       "timeToRide                  int64\n",
       "timestamp     datetime64[ns, UTC]\n",
       "dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>liftID</th>\n",
       "      <th>resortID</th>\n",
       "      <th>liftName</th>\n",
       "      <th>status</th>\n",
       "      <th>timeToRide</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-11 00:16:41.775318+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "      <td>13</td>\n",
       "      <td>Big Red Express</td>\n",
       "      <td>X</td>\n",
       "      <td>8</td>\n",
       "      <td>2020-01-11 00:16:41.775318+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "      <td>Blackcomb Gondola Lower</td>\n",
       "      <td>X</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-01-11 00:16:41.775318+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>13</td>\n",
       "      <td>Blackcomb Gondola Upper</td>\n",
       "      <td>X</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-01-11 00:16:41.775318+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>Catskinner Express</td>\n",
       "      <td>X</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-01-11 00:16:41.775318+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>Coca-Cola Tube Park</td>\n",
       "      <td>O</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-01-11 00:16:41.775318+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>34</td>\n",
       "      <td>13</td>\n",
       "      <td>Creekside Gondola</td>\n",
       "      <td>X</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-01-11 00:16:41.775318+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>Crystal Ridge Express</td>\n",
       "      <td>X</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-01-11 00:16:41.775318+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>35</td>\n",
       "      <td>13</td>\n",
       "      <td>Emerald 6 Express</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-11 00:16:41.775318+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>Excalibur Gondola Lower</td>\n",
       "      <td>O</td>\n",
       "      <td>3</td>\n",
       "      <td>2020-01-11 00:16:41.775318+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>71</td>\n",
       "      <td>13</td>\n",
       "      <td>Excalibur Gondola Upper</td>\n",
       "      <td>X</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-01-11 00:16:41.775318+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>Excelerator Express</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-11 00:16:41.775318+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>45</td>\n",
       "      <td>13</td>\n",
       "      <td>Fitzsimmons Express</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-11 00:16:41.775318+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>44</td>\n",
       "      <td>13</td>\n",
       "      <td>Franz's Chair</td>\n",
       "      <td>X</td>\n",
       "      <td>8</td>\n",
       "      <td>2020-01-11 00:16:41.775318+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>40</td>\n",
       "      <td>13</td>\n",
       "      <td>Garbanzo Express</td>\n",
       "      <td>X</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-01-11 00:16:41.775318+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>Glacier Express</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-11 00:16:41.775318+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>37</td>\n",
       "      <td>13</td>\n",
       "      <td>Harmony 6 Express</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-11 00:16:41.775318+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>Horstman T-Bar</td>\n",
       "      <td>X</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-01-11 00:16:41.775318+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>Jersey Cream Express</td>\n",
       "      <td>X</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-01-11 00:16:41.775318+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>Magic Chair</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-11 00:16:41.775318+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   liftID resortID                 liftName status  timeToRide  \\\n",
       "0       3       13       7th Heaven Express      X           6   \n",
       "1      36       13          Big Red Express      X           8   \n",
       "2      69       13  Blackcomb Gondola Lower      X           7   \n",
       "3      70       13  Blackcomb Gondola Upper      X           7   \n",
       "4       9       13       Catskinner Express      X           4   \n",
       "5      14       13      Coca-Cola Tube Park      O           4   \n",
       "6      34       13        Creekside Gondola      X           7   \n",
       "7      10       13    Crystal Ridge Express      X           7   \n",
       "8      35       13        Emerald 6 Express      X           6   \n",
       "9       5       13  Excalibur Gondola Lower      O           3   \n",
       "10     71       13  Excalibur Gondola Upper      X           5   \n",
       "11      8       13      Excelerator Express      X           6   \n",
       "12     45       13      Fitzsimmons Express      X           6   \n",
       "13     44       13            Franz's Chair      X           8   \n",
       "14     40       13         Garbanzo Express      X           7   \n",
       "15      7       13          Glacier Express      X           6   \n",
       "16     37       13        Harmony 6 Express      X           6   \n",
       "17     11       13           Horstman T-Bar      X           4   \n",
       "18      4       13     Jersey Cream Express      X           5   \n",
       "19      6       13              Magic Chair      O           6   \n",
       "\n",
       "                          timestamp  \n",
       "0  2020-01-11 00:16:41.775318+00:00  \n",
       "1  2020-01-11 00:16:41.775318+00:00  \n",
       "2  2020-01-11 00:16:41.775318+00:00  \n",
       "3  2020-01-11 00:16:41.775318+00:00  \n",
       "4  2020-01-11 00:16:41.775318+00:00  \n",
       "5  2020-01-11 00:16:41.775318+00:00  \n",
       "6  2020-01-11 00:16:41.775318+00:00  \n",
       "7  2020-01-11 00:16:41.775318+00:00  \n",
       "8  2020-01-11 00:16:41.775318+00:00  \n",
       "9  2020-01-11 00:16:41.775318+00:00  \n",
       "10 2020-01-11 00:16:41.775318+00:00  \n",
       "11 2020-01-11 00:16:41.775318+00:00  \n",
       "12 2020-01-11 00:16:41.775318+00:00  \n",
       "13 2020-01-11 00:16:41.775318+00:00  \n",
       "14 2020-01-11 00:16:41.775318+00:00  \n",
       "15 2020-01-11 00:16:41.775318+00:00  \n",
       "16 2020-01-11 00:16:41.775318+00:00  \n",
       "17 2020-01-11 00:16:41.775318+00:00  \n",
       "18 2020-01-11 00:16:41.775318+00:00  \n",
       "19 2020-01-11 00:16:41.775318+00:00  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: change time_diff to \"duration\""
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "437px",
    "left": "869px",
    "right": "20px",
    "top": "120px",
    "width": "391px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
