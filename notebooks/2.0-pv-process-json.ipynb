{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, glob, boto3, os\n",
    "import pdb\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out json processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "BUCKET_NAME = 'snowbot-pv'\n",
    "\n",
    "# S3 Connect\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "bucket = s3.Bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\"\n",
    "MERGED_FILENAME = \"merged_file.json\"\n",
    "merged_file = DATA_DIR + MERGED_FILENAME\n",
    "\n",
    "TEST_FILENAME = \"test_file.json\"\n",
    "merged_test_file = DATA_DIR + TEST_FILENAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://alexwlchan.net/2019/07/listing-s3-keys/\n",
    "def get_matching_s3_objects(bucket, prefix=\"\", suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Generate objects in an S3 bucket.\n",
    "\n",
    "    :param bucket: Name of the S3 bucket.\n",
    "    :param prefix: Only fetch objects whose key starts with\n",
    "        this prefix (optional).\n",
    "    :param suffix: Only fetch objects whose keys end with\n",
    "        this suffix (optional).\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    kwargs = {'Bucket': bucket}\n",
    "\n",
    "    # We can pass the prefix directly to the S3 API.  If the user has passed\n",
    "    # a tuple or list of prefixes, we go through them one by one.\n",
    "    if isinstance(prefix, str):\n",
    "        prefixes = (prefix, )\n",
    "    else:\n",
    "        prefixes = prefix\n",
    "\n",
    "    for key_prefix in prefixes:\n",
    "        kwargs[\"Prefix\"] = key_prefix\n",
    "\n",
    "        for page in paginator.paginate(**kwargs):\n",
    "            try:\n",
    "                contents = page[\"Contents\"]\n",
    "            except KeyError:\n",
    "                return\n",
    "\n",
    "            for obj in contents:\n",
    "                key = obj[\"Key\"]\n",
    "                if key.endswith(suffix):\n",
    "                    yield obj\n",
    "\n",
    "\n",
    "def get_matching_s3_keys(bucket, prefix=\"\", suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Generate the keys in an S3 bucket.\n",
    "\n",
    "    :param bucket: Name of the S3 bucket.\n",
    "    :param prefix: Only fetch keys that start with this prefix (optional).\n",
    "    :param suffix: Only fetch keys that end with this suffix (optional).\n",
    "    \"\"\"\n",
    "    for obj in get_matching_s3_objects(bucket, prefix, suffix):\n",
    "        yield obj[\"Key\"]\n",
    "\n",
    "\n",
    "def merge_matching_jsons(save_file, suffix=\"\"):\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for f in get_matching_s3_keys(BUCKET_NAME, suffix=suffix):\n",
    "\n",
    "        # Write the file from S3 into a local temp file\n",
    "        with open('temp', 'wb') as tfw:\n",
    "            bucket.download_fileobj(f, tfw)\n",
    "\n",
    "        # Append the local temp file into the result list\n",
    "        with open('temp', 'rb') as tfr:\n",
    "            result.append(json.load(tfr))\n",
    "\n",
    "    os.remove(\"temp\")\n",
    "\n",
    "    # Fill the output file with the merged content\n",
    "    with open(save_file, \"w\") as outfile:\n",
    "        json.dump(result, outfile)\n",
    "\n",
    "# TBD: more efficient to go straight to df w/o saving json to file\n",
    "\n",
    "\n",
    "def jsons_to_df(jsons):\n",
    "    df = pd.DataFrame.from_dict(json_normalize(jsons, record_path='lifts', meta='timestamp'))\n",
    "    df = set_lifts_df_datatypes(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_merged_json_as_df(merged_file):\n",
    "    # load the merged json as a dataframe\n",
    "    with open(merged_file, \"r\") as f:\n",
    "        d = json.load(f)\n",
    "        df = jsons_to_df(d)\n",
    "        return df\n",
    "    \n",
    "\n",
    "def set_lifts_df_datatypes(df):\n",
    "\n",
    "    # set datatypes for lift table\n",
    "    df = df.astype({\n",
    "        \"liftID\": 'category',\n",
    "        \"resortID\": 'category',\n",
    "        \"liftName\": 'category',\n",
    "        \"status\": 'category',\n",
    "        \"timeToRide\": \"int\"\n",
    "    })\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_status_changes(df):\n",
    "    '''Returns a dataframe that only includes the times when there was a change to a new status'''\n",
    "\n",
    "    def calc_status_change(df):\n",
    "        firstrow = df.loc[df['timestamp'].idxmin()]\n",
    "        change_rows = df[df.status.ne(df.status.shift())]\n",
    "        keep_df = firstrow.to_frame().T.append(change_rows)\n",
    "\n",
    "        # Remove so that we don't need to write another column to S3 as we scrape?\n",
    "        # Just calculate it when plotting and predicting?\n",
    "        # keep_df['time_diff'] = keep_df['timestamp'].diff(1).shift(-1)\n",
    "\n",
    "        return keep_df\n",
    "\n",
    "    df = df.groupby('liftName', group_keys=False)\\\n",
    "           .apply(calc_status_change)\\\n",
    "           .reset_index(drop=True)\n",
    "\n",
    "    df = set_lifts_df_datatypes(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "# TBD: may need to convert timestamp to days (e.g. for Tableau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process lift json fies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_matching_jsons(suffix=\"lifts.json\", save_file=merged_file)\n",
    "lifts_df = load_merged_json_as_df(merged_file)\n",
    "lifts_df = set_lifts_df_datatypes(lifts_df)\n",
    "\n",
    "lifts_status_changes_df = get_status_changes(lifts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** `timeToRide` is just the time is takes to ride the lift, not the current wait time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "liftName\n",
       "7th Heaven Express                 [6]\n",
       "Big Red Express                    [8]\n",
       "Blackcomb Gondola Lower            [7]\n",
       "Blackcomb Gondola Upper            [7]\n",
       "Catskinner Express                 [4]\n",
       "Coca-Cola Tube Park                [4]\n",
       "Creekside Gondola                  [7]\n",
       "Crystal Ridge Express              [7]\n",
       "Emerald 6 Express                  [6]\n",
       "Excalibur Gondola Lower            [3]\n",
       "Excalibur Gondola Upper            [5]\n",
       "Excelerator Express                [6]\n",
       "Fitzsimmons Express                [6]\n",
       "Franz's Chair                      [8]\n",
       "Garbanzo Express                   [7]\n",
       "Glacier Express                    [6]\n",
       "Harmony 6 Express                  [6]\n",
       "Horstman T-Bar                     [4]\n",
       "Jersey Cream Express               [5]\n",
       "Magic Chair                        [6]\n",
       "Olympic Chair                      [5]\n",
       "Peak 2 Peak Gondola               [12]\n",
       "Peak Express                       [3]\n",
       "Showcase T-Bar                     [3]\n",
       "Symphony Express                   [7]\n",
       "T-Bars                             [5]\n",
       "Whistler Village Gondola Lower     [5]\n",
       "Whistler Village Gondola Upper    [11]\n",
       "Name: timeToRide, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifts_df.groupby(\"liftName\")['timeToRide'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get status durations\n",
    "# TBD: optimize if needed via # 3 under:\n",
    "# https://towardsdatascience.com/pandas-tips-and-tricks-33bcc8a40bb9\n",
    "df = lifts_status_changes_df.sort_values(by=['liftID', 'timestamp'])\n",
    "df['time_diff'] = df.groupby('liftID')['timestamp'].diff(1).shift(-1)\n",
    "\n",
    "# Fill in the durations which will be missing for the most recent status changes\n",
    "missing_time_diffs_idx = df.loc[(df['time_diff'].isnull()) & (\n",
    "    df['timestamp'] >= df['timestamp'].min()), 'timestamp'].index.values\n",
    "\n",
    "df.loc[missing_time_diffs_idx, 'time_diff'] = df['timestamp'].max(\n",
    ") - df.loc[missing_time_diffs_idx, 'timestamp']\n",
    "\n",
    "# Convert to seconds\n",
    "df['time_diff_seconds'] = df['time_diff'].dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>liftID</th>\n",
       "      <th>resortID</th>\n",
       "      <th>liftName</th>\n",
       "      <th>status</th>\n",
       "      <th>timeToRide</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time_diff</th>\n",
       "      <th>time_diff_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-03 00:19:09.631011-08:00</td>\n",
       "      <td>1 days 13:59:59.794440</td>\n",
       "      <td>136799.794440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-04 14:19:09.425451-08:00</td>\n",
       "      <td>0 days 00:29:59.297179</td>\n",
       "      <td>1799.297179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-04 14:49:08.722630-08:00</td>\n",
       "      <td>0 days 22:00:01.500183</td>\n",
       "      <td>79201.500183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-05 12:49:10.222813-08:00</td>\n",
       "      <td>0 days 01:59:58.326201</td>\n",
       "      <td>7198.326201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-05 14:49:08.549014-08:00</td>\n",
       "      <td>0 days 18:56:18.469982</td>\n",
       "      <td>68178.469982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Upper</td>\n",
       "      <td>H</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-01-31 07:45:26.739173-08:00</td>\n",
       "      <td>0 days 00:44:59.262989</td>\n",
       "      <td>2699.262989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Upper</td>\n",
       "      <td>O</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-01-31 08:30:26.002162-08:00</td>\n",
       "      <td>0 days 07:15:00.469053</td>\n",
       "      <td>26100.469053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Upper</td>\n",
       "      <td>X</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-01-31 15:45:26.471215-08:00</td>\n",
       "      <td>0 days 16:29:59.940675</td>\n",
       "      <td>59399.940675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Upper</td>\n",
       "      <td>H</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-02-01 08:15:26.411890-08:00</td>\n",
       "      <td>0 days 06:44:59.751123</td>\n",
       "      <td>24299.751123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Upper</td>\n",
       "      <td>X</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-02-01 15:00:26.163013-08:00</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1942 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     liftID resortID                        liftName status  timeToRide  \\\n",
       "0         3       13              7th Heaven Express      X           6   \n",
       "1         3       13              7th Heaven Express      O           6   \n",
       "2         3       13              7th Heaven Express      X           6   \n",
       "3         3       13              7th Heaven Express      O           6   \n",
       "4         3       13              7th Heaven Express      X           6   \n",
       "...     ...      ...                             ...    ...         ...   \n",
       "1937     72       13  Whistler Village Gondola Upper      H          11   \n",
       "1938     72       13  Whistler Village Gondola Upper      O          11   \n",
       "1939     72       13  Whistler Village Gondola Upper      X          11   \n",
       "1940     72       13  Whistler Village Gondola Upper      H          11   \n",
       "1941     72       13  Whistler Village Gondola Upper      X          11   \n",
       "\n",
       "                            timestamp              time_diff  \\\n",
       "0    2020-01-03 00:19:09.631011-08:00 1 days 13:59:59.794440   \n",
       "1    2020-01-04 14:19:09.425451-08:00 0 days 00:29:59.297179   \n",
       "2    2020-01-04 14:49:08.722630-08:00 0 days 22:00:01.500183   \n",
       "3    2020-01-05 12:49:10.222813-08:00 0 days 01:59:58.326201   \n",
       "4    2020-01-05 14:49:08.549014-08:00 0 days 18:56:18.469982   \n",
       "...                               ...                    ...   \n",
       "1937 2020-01-31 07:45:26.739173-08:00 0 days 00:44:59.262989   \n",
       "1938 2020-01-31 08:30:26.002162-08:00 0 days 07:15:00.469053   \n",
       "1939 2020-01-31 15:45:26.471215-08:00 0 days 16:29:59.940675   \n",
       "1940 2020-02-01 08:15:26.411890-08:00 0 days 06:44:59.751123   \n",
       "1941 2020-02-01 15:00:26.163013-08:00        0 days 00:00:00   \n",
       "\n",
       "      time_diff_seconds  \n",
       "0         136799.794440  \n",
       "1           1799.297179  \n",
       "2          79201.500183  \n",
       "3           7198.326201  \n",
       "4          68178.469982  \n",
       "...                 ...  \n",
       "1937        2699.262989  \n",
       "1938       26100.469053  \n",
       "1939       59399.940675  \n",
       "1940       24299.751123  \n",
       "1941           0.000000  \n",
       "\n",
       "[1942 rows x 8 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "liftID                                             category\n",
       "resortID                                           category\n",
       "liftName                                           category\n",
       "status                                             category\n",
       "timeToRide                                            int64\n",
       "timestamp            datetime64[ns, pytz.FixedOffset(-480)]\n",
       "time_diff                                   timedelta64[ns]\n",
       "time_diff_seconds                                   float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses locale date formatting, otherwise Tableau will mix up month and day\n",
    "# alternatively, can export to json:\n",
    "# lifts_status_changes_df.to_json(DATA_DIR + \"lifts_status_changes.json\", orient='table')\n",
    "df.to_csv(DATA_DIR + \"lifts_status_changes.csv\", date_format='%c')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add:\n",
    "# \n",
    "# daily: for each chair calculate most open status of the day: O > H > X\n",
    "# Days since each chair was last seen open with timestamp of most recent open time.\n",
    "# snowfall since last open\n",
    "# save data for other mountains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Storage options testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle(DATA_DIR + \"df_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fastparquet import write\n",
    "\n",
    "# parquet engines don't handle shifted timezones\n",
    "import pytz\n",
    "TZ = pytz.timezone('America/Vancouver')\n",
    "df['timestamp'] = df.timestamp.dt.tz_convert(pytz.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Note: May need snappy-python as a req to run on AWS Lambda\n",
    "df.to_parquet(DATA_DIR + \"df_test.parquet\", engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "load_df = pd.read_parquet(DATA_DIR + \"df_test.parquet\")\n",
    "load_df['timestamp'] = load_df.timestamp.dt.tz_convert(TZ) # convert back to correct timezone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#TBD convert back to correct datatypes\n",
    "load_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(DATA_DIR + \"df_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Test file size results:\n",
    "- json: 800 Kb?\n",
    "- csv: 474 Kb\n",
    "- pickle: 145 Kb\n",
    "- parquet: 15 Kb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Delta Lake Testing\n",
    "\n",
    "Requires apache spark instance.  For future use, could set one up to work with lambda using https://aws.amazon.com/emr/features/spark/?\n",
    "\n",
    "Otherwise databricks (similar to QxMD project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# json comparison and parquet to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastparquet import write, ParquetFile\n",
    "import os\n",
    "import pytz\n",
    "import s3fs\n",
    "import botocore\n",
    "\n",
    "os.chdir(\"../src/data/snowbot_AWS_lambda/\")\n",
    "from scrape import get_data\n",
    "os.chdir(\"../../../notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet engines don't handle shifted timezones\n",
    "TZ = pytz.timezone('America/Vancouver')\n",
    "\n",
    "# new version\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "myopen = fs.open\n",
    "nop = lambda *args, **kwargs: None\n",
    "\n",
    "\n",
    "fname = 'wb_lifts_history'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframe_to_parquet_on_s3(df, fname):\n",
    "    \"\"\" Write a dataframe to a Parquet file on S3.  Creates a new parquet file if one doesn't already exist.\"\"\"\n",
    "\n",
    "    def write_parquet(df, fname, app=True):\n",
    "\n",
    "        output_file = f\"s3://{BUCKET_NAME}/{fname}.parquet\"\n",
    "        write(output_file,\n",
    "              df,\n",
    "              partition_on=['timestamp'],\n",
    "              file_scheme='hive',\n",
    "              append=app,  # need to remove or catch exception to work when file doesn't exist\n",
    "              open_with=myopen,\n",
    "              mkdirs=nop)\n",
    "        print(\"Writing {} records to {}.\".format(len(df), fname))\n",
    "\n",
    "    # Unshift the timezone because parquet engines don't handle shifted timezones\n",
    "    df.loc[:, 'timestamp'] = df.loc[:, 'timestamp'].dt.tz_convert(pytz.utc)\n",
    "\n",
    "    s3_object = s3.Object(BUCKET_NAME, fname + \".parquet\")\n",
    "\n",
    "    if not list(bucket.objects.filter(Prefix=fname)):\n",
    "        print(\"File {} not found.  Creating new file.\".format(fname))\n",
    "        write_parquet(df, fname, app=False)\n",
    "\n",
    "    else:\n",
    "        print(f\"File {fname} found.\")\n",
    "        write_parquet(df, fname, app=True)\n",
    "\n",
    "\n",
    "def save_prior(json_data):\n",
    "    bucket.put_object(Key=\"lifts_prior.json\",\n",
    "                      Body=bytes(json.dumps(json_data).encode('UTF-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded prior json data from S3\n",
      "Found differences between current and prior data.\n",
      "File wb_lifts_history found.\n",
      "Writing 29 records to wb_lifts_history.\n",
      "Updated lifts_prior.json with current data.\n"
     ]
    }
   ],
   "source": [
    "# Get current lift status info json\n",
    "lifts_current = get_data()['lifts']  # String.\n",
    "lifts_current_json = json.loads(lifts_current)\n",
    "\n",
    "# Get prior lift status info json\n",
    "lifts_prior_object = s3.Object(BUCKET_NAME, 'lifts_prior.json')\n",
    "try:\n",
    "    lifts_prior_object.load()\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == \"404\":\n",
    "        print(\"Prior doesn't exist\")\n",
    "        save_prior(lifts_current_json)  # Create the prior file\n",
    "        print(\"Created PRIOR_FILENAME_TBD\")\n",
    "    else:\n",
    "        # Something else has gone wrong.\n",
    "        raise\n",
    "else:\n",
    "    # The prior exists\n",
    "    file_content = lifts_prior_object.get()['Body'].read().decode('utf-8')\n",
    "    json_content = json.loads(file_content)\n",
    "    print(\"Loaded prior json data from S3\")\n",
    "\n",
    "    # Compare jsons without their timestamps.  The timestamps on the current json will\n",
    "    # always be more recent even when none of the lift statuses have changed.\n",
    "    if json_content['lifts'] == lifts_current_json['lifts']:\n",
    "        print(\"No differences between current and prior data were found.\")\n",
    "    else:\n",
    "        # Get a df with the status chages between the prior and current json data\n",
    "        df = jsons_to_df([json_content, lifts_current_json])\n",
    "        df = get_status_changes(df)\n",
    "        print(\"Found differences between current and prior data.\")\n",
    "        write_dataframe_to_parquet_on_s3(df, fname)\n",
    "\n",
    "        # save current lift status info json as prior\n",
    "        save_prior(lifts_current_json)\n",
    "        print(\"Updated {} with current data.\".format(lifts_prior_object.key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>liftID</th>\n",
       "      <th>resortID</th>\n",
       "      <th>liftName</th>\n",
       "      <th>status</th>\n",
       "      <th>timeToRide</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-13 13:21:56.629410-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "      <td>13</td>\n",
       "      <td>Big Red Express</td>\n",
       "      <td>O</td>\n",
       "      <td>8</td>\n",
       "      <td>2020-01-13 13:21:56.629410-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "      <td>Blackcomb Gondola Lower</td>\n",
       "      <td>O</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-01-13 13:21:56.629410-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>13</td>\n",
       "      <td>Blackcomb Gondola Upper</td>\n",
       "      <td>O</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-01-13 13:21:56.629410-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>Catskinner Express</td>\n",
       "      <td>O</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-01-13 13:21:56.629410-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>42</td>\n",
       "      <td>13</td>\n",
       "      <td>Symphony Express</td>\n",
       "      <td>X</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-02-01 14:55:34.456835-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>41</td>\n",
       "      <td>13</td>\n",
       "      <td>T-Bars</td>\n",
       "      <td>X</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-02-01 14:55:34.456835-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>33</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Lower</td>\n",
       "      <td>O</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-02-01 14:55:34.456835-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Upper</td>\n",
       "      <td>X</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-02-01 14:55:34.456835-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>45</td>\n",
       "      <td>13</td>\n",
       "      <td>Fitzsimmons Express</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-02-01 15:05:15.381806-08:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>327 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    liftID resortID                        liftName status  timeToRide  \\\n",
       "0        3       13              7th Heaven Express      O           6   \n",
       "1       36       13                 Big Red Express      O           8   \n",
       "2       69       13         Blackcomb Gondola Lower      O           7   \n",
       "3       70       13         Blackcomb Gondola Upper      O           7   \n",
       "4        9       13              Catskinner Express      O           4   \n",
       "..     ...      ...                             ...    ...         ...   \n",
       "322     42       13                Symphony Express      X           7   \n",
       "323     41       13                          T-Bars      X           5   \n",
       "324     33       13  Whistler Village Gondola Lower      O           5   \n",
       "325     72       13  Whistler Village Gondola Upper      X          11   \n",
       "326     45       13             Fitzsimmons Express      X           6   \n",
       "\n",
       "                           timestamp  \n",
       "0   2020-01-13 13:21:56.629410-08:00  \n",
       "1   2020-01-13 13:21:56.629410-08:00  \n",
       "2   2020-01-13 13:21:56.629410-08:00  \n",
       "3   2020-01-13 13:21:56.629410-08:00  \n",
       "4   2020-01-13 13:21:56.629410-08:00  \n",
       "..                               ...  \n",
       "322 2020-02-01 14:55:34.456835-08:00  \n",
       "323 2020-02-01 14:55:34.456835-08:00  \n",
       "324 2020-02-01 14:55:34.456835-08:00  \n",
       "325 2020-02-01 14:55:34.456835-08:00  \n",
       "326 2020-02-01 15:05:15.381806-08:00  \n",
       "\n",
       "[327 rows x 6 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the parquet file\n",
    "\n",
    "\n",
    "def load_dataframe_from_parquet_on_s3(fname):\n",
    "    \"\"\" Load a dataframe from a Parquet on S3. \"\"\"\n",
    "    read_file = f\"s3://{BUCKET_NAME}/{fname}.parquet\"\n",
    "    pf = ParquetFile(read_file, open_with=myopen)\n",
    "    df = pf.to_pandas()\n",
    "\n",
    "    # Reshift the timezone because parquet engines don't handle shifted timezones\n",
    "    df['timestamp'].cat.set_categories(df['timestamp'].cat.categories.tz_convert(TZ), inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "load_dataframe_from_parquet_on_s3(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing timestamps for file loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file = f\"s3://{BUCKET_NAME}/{fname}.parquet\"\n",
    "pf = ParquetFile(read_file, open_with=myopen)\n",
    "test = pf.to_pandas()[\"timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed: to convert for categorical datetime to regular datetime\n",
    "df[\"timestamp\"] = pd.to_datetime(pd.Series(np.asarray(df[\"timestamp\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.dt = test.dt.tz_convert(tz= 'America/Vancouver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/Users/paul/anaconda3/lib/python3.7/site-packages/pandas/core/series.py:597: FutureWarning: Converting timezone-aware DatetimeArray to timezone-naive ndarray with 'datetime64[ns]' dtype. In the future, this will return an ndarray with 'object' dtype where each element is a 'pandas.Timestamp' with the correct 'tz'.\n",
    "\tTo accept the future behavior, pass 'dtype=object'.\n",
    "\tTo keep the old behavior, pass 'dtype=\"datetime64[ns]\"'.\n",
    "\n",
    "\n",
    "more info: https://pandas-docs.github.io/pandas-docs-travis/whatsnew/v0.24.0.html#converting-timezone-aware-series-and-index-to-numpy-arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataframe_from_parquet_on_s3(fname).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing local parquet saves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parquet(df, fname):\n",
    "    # parquet engines don't handle shifted timezones\n",
    "    df.loc[:, 'timestamp'] = df.loc[:, 'timestamp'].dt.tz_convert(pytz.utc)\n",
    "\n",
    "    # Note: May need snappy-python as a req to run on AWS Lambda\n",
    "    df.to_parquet(DATA_DIR + fname + '.parquet',\n",
    "                  engine='fastparquet',\n",
    "                  partition_on=['timestamp'],\n",
    "                  file_scheme='mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_parquet(df[0:3].copy(), 'wb_lifts_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[20:22, :].copy().to_parquet(DATA_DIR + 'wb_lifts_history' + '.parquet',\n",
    "              engine='fastparquet',\n",
    "              partition_on=['timestamp'],\n",
    "              file_scheme='mixed',\n",
    "              append=True)\n",
    "# Catch exception that is doesn't exist here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: change time_diff to \"duration\"\n",
    "# test on lambda\n",
    "# make datatype dict for and general set datatypes function\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "437px",
    "left": "869px",
    "right": "20px",
    "top": "120px",
    "width": "391px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
