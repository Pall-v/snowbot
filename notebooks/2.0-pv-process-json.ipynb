{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, glob, boto3, os\n",
    "import pdb\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out json processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "BUCKET_NAME = 'snowbot-pv'\n",
    "\n",
    "# S3 Connect\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "bucket = s3.Bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet engines don't handle shifted timezones\n",
    "TZ = pytz.timezone('America/Vancouver')\n",
    "\n",
    "DATA_DIR = \"../data/\"\n",
    "MERGED_FILENAME = \"merged_file.json\"\n",
    "merged_file = DATA_DIR + MERGED_FILENAME\n",
    "\n",
    "TEST_FILENAME = \"test_file.json\"\n",
    "merged_test_file = DATA_DIR + TEST_FILENAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://alexwlchan.net/2019/07/listing-s3-keys/\n",
    "def get_matching_s3_objects(bucket, prefix=\"\", suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Generate objects in an S3 bucket.\n",
    "\n",
    "    :param bucket: Name of the S3 bucket.\n",
    "    :param prefix: Only fetch objects whose key starts with\n",
    "        this prefix (optional).\n",
    "    :param suffix: Only fetch objects whose keys end with\n",
    "        this suffix (optional).\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    kwargs = {'Bucket': bucket}\n",
    "\n",
    "    # We can pass the prefix directly to the S3 API.  If the user has passed\n",
    "    # a tuple or list of prefixes, we go through them one by one.\n",
    "    if isinstance(prefix, str):\n",
    "        prefixes = (prefix, )\n",
    "    else:\n",
    "        prefixes = prefix\n",
    "\n",
    "    for key_prefix in prefixes:\n",
    "        kwargs[\"Prefix\"] = key_prefix\n",
    "\n",
    "        for page in paginator.paginate(**kwargs):\n",
    "            try:\n",
    "                contents = page[\"Contents\"]\n",
    "            except KeyError:\n",
    "                return\n",
    "\n",
    "            for obj in contents:\n",
    "                key = obj[\"Key\"]\n",
    "                if key.endswith(suffix):\n",
    "                    yield obj\n",
    "\n",
    "\n",
    "def get_matching_s3_keys(bucket, prefix=\"\", suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Generate the keys in an S3 bucket.\n",
    "\n",
    "    :param bucket: Name of the S3 bucket.\n",
    "    :param prefix: Only fetch keys that start with this prefix (optional).\n",
    "    :param suffix: Only fetch keys that end with this suffix (optional).\n",
    "    \"\"\"\n",
    "    for obj in get_matching_s3_objects(bucket, prefix, suffix):\n",
    "        yield obj[\"Key\"]\n",
    "\n",
    "\n",
    "def merge_matching_jsons(save_file, suffix=\"\"):\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for f in get_matching_s3_keys(BUCKET_NAME, suffix=suffix):\n",
    "\n",
    "        # Write the file from S3 into a local temp file\n",
    "        with open('temp', 'wb') as tfw:\n",
    "            bucket.download_fileobj(f, tfw)\n",
    "\n",
    "        # Append the local temp file into the result list\n",
    "        with open('temp', 'rb') as tfr:\n",
    "            result.append(json.load(tfr))\n",
    "\n",
    "    os.remove(\"temp\")\n",
    "\n",
    "    # Fill the output file with the merged content\n",
    "    with open(save_file, \"w\") as outfile:\n",
    "        json.dump(result, outfile)\n",
    "\n",
    "# TBD: more efficient to go straight to df w/o saving json to file\n",
    "\n",
    "\n",
    "def set_lifts_df_datatypes(df):\n",
    "\n",
    "    # Important to set categories because when writing incrementally to parquet, some increments\n",
    "    # may not include all statuses.  Manually setting the categories avoids errors due to\n",
    "    # different catergory indexing between increments.\n",
    "    status_cat_dtype = pd.api.types.CategoricalDtype(\n",
    "        categories=['X', 'H', 'O'], ordered=True)\n",
    "\n",
    "    # set datatypes for lift table\n",
    "    df = df.astype({\n",
    "        \"liftID\": 'category',\n",
    "        \"resortID\": 'category',\n",
    "        \"liftName\": 'category',\n",
    "        \"status\": status_cat_dtype,\n",
    "        \"timeToRide\": \"int\"\n",
    "    })\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def jsons_to_df(jsons, record_path):\n",
    "    df = pd.DataFrame.from_dict(pd.json_normalize(\n",
    "        jsons, record_path=record_path, meta='timestamp'))\n",
    "    df = set_lifts_df_datatypes(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_merged_json_as_df(merged_file, record_path):\n",
    "    # load the merged json as a dataframe\n",
    "    with open(merged_file, \"r\") as f:\n",
    "        d = json.load(f)\n",
    "        df = jsons_to_df(d, record_path)\n",
    "        return df\n",
    "\n",
    "\n",
    "def get_status_changes(df, keep_oldest=False):\n",
    "    \"\"\"\n",
    "    Filter out rows that do not represent a status change.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Includes 'status' and timestamp columns.  Lists status of every lift for each timestamp.\n",
    "    keep_oldest : boolean\n",
    "        Indicates if the returned DataFrame should keep the oldest status for each lift even if\n",
    "        a lift has no status changes.  This is so that the earliest status for each lift is not\n",
    "        lost, and all lifts are listed the returned DataFrame even if their status has not\n",
    "        changed.  Use `False` when there is just one DataFrame to process.  Use `True` is cases\n",
    "        where the status chages will be appended to an existing dataframe that already has at\n",
    "        least one row for each lift.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Only includes the rows from the original dataframe where there was a change to a new\n",
    "        status.\n",
    "    \"\"\"\n",
    "\n",
    "    def calc_status_change(df, keep_oldest=keep_oldest):\n",
    "        change_rows = df[df.status.ne(df.status.shift())]\n",
    "\n",
    "        if keep_oldest:\n",
    "            firstrow = df.loc[df['timestamp'].idxmin()]\n",
    "            keep_df = firstrow.to_frame().T.append(change_rows)\n",
    "        else:\n",
    "            keep_df = change_rows\n",
    "\n",
    "        # Remove so that we don't need to write another column to S3 as we scrape?\n",
    "        # Just calculate it when plotting and predicting?\n",
    "        # keep_df['time_diff'] = keep_df['timestamp'].diff(1).shift(-1)\n",
    "\n",
    "        return keep_df\n",
    "\n",
    "    df = df.groupby('liftName', group_keys=False)\\\n",
    "           .apply(calc_status_change)\\\n",
    "           .reset_index(drop=True)\n",
    "\n",
    "    df = set_lifts_df_datatypes(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "# TBD: may need to convert timestamp to days (e.g. for Tableau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = load_merged_json_as_df(merged_test_file, 'lifts')\n",
    "test_df.sort_values(by=['liftID', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>liftID</th>\n",
       "      <th>resortID</th>\n",
       "      <th>liftName</th>\n",
       "      <th>status</th>\n",
       "      <th>timeToRide</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "      <td>Blackcomb Gondola Lower</td>\n",
       "      <td>O</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-01-03 00:19:09.631011-08:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  liftID resortID                 liftName status  timeToRide  \\\n",
       "0     69       13  Blackcomb Gondola Lower      O           7   \n",
       "\n",
       "                         timestamp  \n",
       "0 2020-01-03 00:19:09.631011-08:00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TBD: test that len is 1 when keep_oldest=False and larger otherwise\n",
    "get_status_changes(test_df, keep_oldest=False).sort_values(by=['liftID', 'timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process lift json fies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_matching_jsons(suffix=\"lifts.json\", save_file=merged_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifts_df = load_merged_json_as_df(merged_file, 'lifts')\n",
    "\n",
    "lifts_status_changes_df = get_status_changes(lifts_df, keep_oldest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** `timeToRide` is just the time is takes to ride the lift, not the current wait time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "liftName\n",
       "7th Heaven Express                 [6]\n",
       "Big Red Express                    [8]\n",
       "Blackcomb Gondola Lower            [7]\n",
       "Blackcomb Gondola Upper            [7]\n",
       "Catskinner Express                 [4]\n",
       "Coca-Cola Tube Park                [4]\n",
       "Creekside Gondola                  [7]\n",
       "Crystal Ridge Express              [7]\n",
       "Emerald 6 Express                  [6]\n",
       "Excalibur Gondola Lower            [3]\n",
       "Excalibur Gondola Upper            [5]\n",
       "Excelerator Express                [6]\n",
       "Fitzsimmons Express                [6]\n",
       "Franz's Chair                      [8]\n",
       "Garbanzo Express                   [7]\n",
       "Glacier Express                    [6]\n",
       "Harmony 6 Express                  [6]\n",
       "Horstman T-Bar                     [4]\n",
       "Jersey Cream Express               [5]\n",
       "Magic Chair                        [6]\n",
       "Olympic Chair                      [5]\n",
       "Peak 2 Peak Gondola               [12]\n",
       "Peak Express                       [3]\n",
       "Showcase T-Bar                     [3]\n",
       "Symphony Express                   [7]\n",
       "T-Bars                             [5]\n",
       "Whistler Village Gondola Lower     [5]\n",
       "Whistler Village Gondola Upper    [11]\n",
       "Name: timeToRide, dtype: object"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifts_df.groupby(\"liftName\")['timeToRide'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>liftID</th>\n",
       "      <th>resortID</th>\n",
       "      <th>liftName</th>\n",
       "      <th>status</th>\n",
       "      <th>timeToRide</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "      <td>Blackcomb Gondola Lower</td>\n",
       "      <td>X</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-01-03 00:19:09.631011-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70</td>\n",
       "      <td>13</td>\n",
       "      <td>Blackcomb Gondola Upper</td>\n",
       "      <td>X</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-01-03 00:19:09.631011-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>Excalibur Gondola Lower</td>\n",
       "      <td>X</td>\n",
       "      <td>3</td>\n",
       "      <td>2020-01-03 00:19:09.631011-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71</td>\n",
       "      <td>13</td>\n",
       "      <td>Excalibur Gondola Upper</td>\n",
       "      <td>X</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-01-03 00:19:09.631011-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>Excelerator Express</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-03 00:19:09.631011-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84559</th>\n",
       "      <td>22</td>\n",
       "      <td>13</td>\n",
       "      <td>Peak 2 Peak Gondola</td>\n",
       "      <td>X</td>\n",
       "      <td>12</td>\n",
       "      <td>2020-02-03 10:30:26.557898-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84560</th>\n",
       "      <td>44</td>\n",
       "      <td>13</td>\n",
       "      <td>Franz's Chair</td>\n",
       "      <td>X</td>\n",
       "      <td>8</td>\n",
       "      <td>2020-02-03 10:30:26.557898-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84561</th>\n",
       "      <td>43</td>\n",
       "      <td>13</td>\n",
       "      <td>Peak Express</td>\n",
       "      <td>X</td>\n",
       "      <td>3</td>\n",
       "      <td>2020-02-03 10:30:26.557898-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84562</th>\n",
       "      <td>42</td>\n",
       "      <td>13</td>\n",
       "      <td>Symphony Express</td>\n",
       "      <td>X</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-02-03 10:30:26.557898-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84563</th>\n",
       "      <td>41</td>\n",
       "      <td>13</td>\n",
       "      <td>T-Bars</td>\n",
       "      <td>X</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-02-03 10:30:26.557898-08:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84564 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      liftID resortID                 liftName status  timeToRide  \\\n",
       "0         69       13  Blackcomb Gondola Lower      X           7   \n",
       "1         70       13  Blackcomb Gondola Upper      X           7   \n",
       "2          5       13  Excalibur Gondola Lower      X           3   \n",
       "3         71       13  Excalibur Gondola Upper      X           5   \n",
       "4          8       13      Excelerator Express      X           6   \n",
       "...      ...      ...                      ...    ...         ...   \n",
       "84559     22       13      Peak 2 Peak Gondola      X          12   \n",
       "84560     44       13            Franz's Chair      X           8   \n",
       "84561     43       13             Peak Express      X           3   \n",
       "84562     42       13         Symphony Express      X           7   \n",
       "84563     41       13                   T-Bars      X           5   \n",
       "\n",
       "                             timestamp  \n",
       "0     2020-01-03 00:19:09.631011-08:00  \n",
       "1     2020-01-03 00:19:09.631011-08:00  \n",
       "2     2020-01-03 00:19:09.631011-08:00  \n",
       "3     2020-01-03 00:19:09.631011-08:00  \n",
       "4     2020-01-03 00:19:09.631011-08:00  \n",
       "...                                ...  \n",
       "84559 2020-02-03 10:30:26.557898-08:00  \n",
       "84560 2020-02-03 10:30:26.557898-08:00  \n",
       "84561 2020-02-03 10:30:26.557898-08:00  \n",
       "84562 2020-02-03 10:30:26.557898-08:00  \n",
       "84563 2020-02-03 10:30:26.557898-08:00  \n",
       "\n",
       "[84564 rows x 6 columns]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_status_durations(lifts_df):\n",
    "    '''Calculate values and add columns for the time difference between the\n",
    "    timestamp for the current status and the timestamp for the next status\n",
    "    for each lift:\n",
    "    `time_diff` column: Gives the duration that the lift was in the status indicated in the `status` column.\n",
    "    `time_diff_seconds` column: `time_diff` converted to seconds.\n",
    "    \n",
    "    lifts_status_changes_df should be TBD\n",
    "    '''\n",
    "    # TBD: optimize if needed via # 3 under:\n",
    "    # https://towardsdatascience.com/pandas-tips-and-tricks-33bcc8a40bb9\n",
    "    df = lifts_df.sort_values(by=['liftID', 'timestamp'])\n",
    "    df['time_diff'] = df.groupby('liftID')['timestamp'].diff(1).shift(-1)\n",
    "\n",
    "    # Fill in the durations which will be missing for the most recent status changes\n",
    "    missing_time_diffs_idx = df.loc[(df['time_diff'].isnull()) & (\n",
    "        df['timestamp'] >= df['timestamp'].min()), 'timestamp'].index.values\n",
    "\n",
    "    df.loc[missing_time_diffs_idx, 'time_diff'] = df['timestamp'].max(\n",
    "    ) - df.loc[missing_time_diffs_idx, 'timestamp']\n",
    "\n",
    "    # Convert to seconds\n",
    "    df['time_diff_seconds'] = df['time_diff'].dt.total_seconds()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_status_durations(lifts_status_changes_df)\n",
    "\n",
    "# Uses local date formatting, otherwise Tableau will mix up month and day\n",
    "# alternatively, can export to json:\n",
    "# lifts_status_changes_df.to_json(DATA_DIR + \"lifts_status_changes.json\", orient='table')\n",
    "df.to_csv(DATA_DIR + \"lifts_status_changes.csv\", date_format='%c')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add:\n",
    "# \n",
    "# daily: for each chair calculate most open status of the day: O > H > X\n",
    "# Days since each chair was last seen open with timestamp of most recent open time.\n",
    "# snowfall since last open\n",
    "# save data for other mountains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Storage options testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle(DATA_DIR + \"df_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fastparquet import write\n",
    "\n",
    "# parquet engines don't handle shifted timezones\n",
    "import pytz\n",
    "TZ = pytz.timezone('America/Vancouver')\n",
    "df['timestamp'] = df.timestamp.dt.tz_convert(pytz.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Note: May need snappy-python as a req to run on AWS Lambda\n",
    "df.to_parquet(DATA_DIR + \"df_test.parquet\", engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "load_df = pd.read_parquet(DATA_DIR + \"df_test.parquet\")\n",
    "load_df['timestamp'] = load_df.timestamp.dt.tz_convert(TZ) # convert back to correct timezone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#TBD convert back to correct datatypes\n",
    "load_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(DATA_DIR + \"df_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Test file size results:\n",
    "- json: 800 Kb?\n",
    "- csv: 474 Kb\n",
    "- pickle: 145 Kb\n",
    "- parquet: 15 Kb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Delta Lake Testing\n",
    "\n",
    "Requires apache spark instance.  For future use, could set one up to work with lambda using https://aws.amazon.com/emr/features/spark/?\n",
    "\n",
    "Otherwise databricks (similar to QxMD project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# json comparison and parquet to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15 µs, sys: 254 µs, total: 269 µs\n",
      "Wall time: 696 µs\n"
     ]
    }
   ],
   "source": [
    "from fastparquet import write, ParquetFile\n",
    "import os\n",
    "import pytz\n",
    "import s3fs\n",
    "import botocore\n",
    "\n",
    "os.chdir(\"../src/data/snowbot_AWS_lambda/\")\n",
    "from scrape import get_data\n",
    "os.chdir(\"../../../notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem()\n",
    "myopen = fs.open\n",
    "nop = lambda *args, **kwargs: None\n",
    "\n",
    "HISTORY_FNAME = 'wb_lifts_history.parquet'\n",
    "PRIOR_STATUS_FNAME = 'lifts_prior.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframe_to_parquet_on_s3(df, fname):\n",
    "    \"\"\" Write a dataframe to a Parquet file on S3.  Creates a new parquet file if one\n",
    "    doesn't already exist.\n",
    "    \"\"\"\n",
    "\n",
    "    def write_parquet(df, fname, app=True):\n",
    "\n",
    "        output_file = f\"s3://{BUCKET_NAME}/{fname}\"\n",
    "        write(output_file,\n",
    "              df,\n",
    "              # partition_on=['timestamp'],\n",
    "              file_scheme='hive',\n",
    "              append=app,  # need to remove or catch exception to work when file doesn't exist\n",
    "              open_with=myopen,\n",
    "              mkdirs=nop)\n",
    "        print(f\"Writing {len(df)} records to {fname}.\")\n",
    "\n",
    "    # Unshift the timezone because parquet engines don't handle shifted timezones\n",
    "    df.loc[:, 'timestamp'] = df.loc[:, 'timestamp'].dt.tz_convert(pytz.utc)\n",
    "\n",
    "    s3_object = s3.Object(BUCKET_NAME, fname)\n",
    "\n",
    "    if not list(bucket.objects.filter(Prefix=fname)):\n",
    "        print(f\"File {fname} not found.  Creating new file.\")\n",
    "        # Keep oldest status for each lift because creating new file\n",
    "        df = get_status_changes(df, keep_oldest=True)\n",
    "        write_parquet(df, fname, app=False)\n",
    "\n",
    "    else:\n",
    "        print(f\"File {fname} found.\")\n",
    "        df = get_status_changes(df, keep_oldest=False)\n",
    "        write_parquet(df, fname, app=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized version\n",
    "For all data from the EpicMix API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "resortID=1\n",
    "if resortID:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "# Filter for a specific resort\n",
    "def filter_resort(data, resortID=None):\n",
    "    if resortID:\n",
    "        return data[\"resortID\"] == resortID\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "def get_data_all_TEST():\n",
    "    \"\"\"new version: Defaults to all resorts.  Otion to filter\"\"\"\n",
    "    API_URL = 'http://www.epicmix.com/vailresorts/sites/epicmix/api/mobile/'\n",
    "    DATA_LIST = {'lifts': 'lifts', 'weather': 'snowconditions', 'terrain': 'terrains'}  # keys are used in the requests, the values and used in the response\n",
    "    json_data = dict()\n",
    "\n",
    "    for d, name in DATA_LIST.items():\n",
    "        res = requests.get(API_URL + d + '.ashx')\n",
    "        res.raise_for_status()\n",
    "        data = json.loads(res.text)[name]\n",
    "        data = list(filter(filter_resort, data))\n",
    "        json_data[d] = json.dumps({'timestamp': str(datetime.now(TZ)), d: data})\n",
    "\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTORY_SUFFIX = '_history_DEV.parquet'\n",
    "PRIOR_SUFFIX = '_prior_DEV.json'\n",
    "\n",
    "\n",
    "class ParquetWriter():\n",
    "    \"\"\"Identifies new data and writes it to Parquet file on S3.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Get current data\n",
    "        self.data_current_all = get_data_all_TEST()  # String.\n",
    "\n",
    "    def write_new_data_all(self):\n",
    "        \"\"\"Writes new data for each type (i.e. 'lifts', 'weather', 'terrian')\n",
    "        of data returned by the API.\n",
    "        \"\"\"\n",
    "        # self.table is the type of data\n",
    "        for self.table in self.data_current_all:\n",
    "            self.current_json = json.loads(self.data_current_all[self.table])\n",
    "            self.prior_fname = self.table + PRIOR_SUFFIX\n",
    "            self.prior_object = s3.Object(BUCKET_NAME, self.prior_fname)\n",
    "            self.write_new_data()\n",
    "\n",
    "    def write_new_data(self):\n",
    "        \"\"\"If new data since the last update of Parquet file is found, add it to the Parquet\n",
    "        file.  Save the current data as json to serve as the prior in the next comparison.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get prior data json\n",
    "        try:\n",
    "            self.prior_object.load()\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] == \"404\":\n",
    "                print(\"Prior doesn't exist\")\n",
    "                # Create the prior file\n",
    "                self.save_prior_data()\n",
    "                print(f\"Created {self.prior_fname}\")\n",
    "            else:\n",
    "                # Something else has gone wrong.\n",
    "                raise\n",
    "        else:\n",
    "            # The prior data file exists\n",
    "            self.get_prior_data()\n",
    "            if self.data_changed():\n",
    "\n",
    "                # Get a df with the status chages between the prior and current json data\n",
    "                df = jsons_to_df([self.prior_json, self.current_json], record_path=self.table)\n",
    "                write_dataframe_to_parquet_on_s3(df, self.table + HISTORY_SUFFIX)\n",
    "\n",
    "                # save current data json as prior\n",
    "                self.save_prior_data()\n",
    "                print(\n",
    "                    f\"Replaced data in {self.prior_object.key} with current data.\")\n",
    "\n",
    "    def get_prior_data(self):\n",
    "        prior = self.prior_object.get()[\n",
    "            'Body'].read().decode('utf-8')\n",
    "        self.prior_json = json.loads(prior)\n",
    "        print(f\"Loaded prior {self.table} json data from S3\")\n",
    "\n",
    "    def data_changed(self):\n",
    "        \"\"\"Compare current data json with prior data json without their timestamps.  The timestamps\n",
    "        on the current json will always be more recent even when none of the other data has changed.\n",
    "        \"\"\"\n",
    "        if self.prior_json[self.table] == self.current_json[self.table]:\n",
    "            print(\n",
    "                f\"No differences between current and prior {self.table} data were found.\")\n",
    "            return False\n",
    "        else:\n",
    "            print(\n",
    "                f\"Found differences between current and prior {self.table} data.\")\n",
    "            return True\n",
    "\n",
    "    def save_prior_data(self):\n",
    "        \"\"\"Save the current data as prior data on S3.\"\"\"\n",
    "        bucket.put_object(Key=self.prior_fname,\n",
    "                          Body=bytes(json.dumps(self.current_json).encode('UTF-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded prior lifts json data from S3\n",
      "Found differences between current and prior lifts data.\n",
      "File lifts_history_DEV.parquet found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul/anaconda3/lib/python3.7/site-packages/fastparquet/writer.py:655: FutureWarning: RangeIndex._start is deprecated and will be removed in a future version. Use RangeIndex.start instead\n",
      "  index_cols = [{'name': index_cols.name, 'start': index_cols._start,\n",
      "/Users/paul/anaconda3/lib/python3.7/site-packages/fastparquet/writer.py:656: FutureWarning: RangeIndex._stop is deprecated and will be removed in a future version. Use RangeIndex.stop instead\n",
      "  'stop': index_cols._stop, 'step': index_cols._step,\n",
      "/Users/paul/anaconda3/lib/python3.7/site-packages/fastparquet/writer.py:656: FutureWarning: RangeIndex._step is deprecated and will be removed in a future version. Use RangeIndex.step instead\n",
      "  'stop': index_cols._stop, 'step': index_cols._step,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 22 records to lifts_history_DEV.parquet.\n",
      "Replaced data in lifts_prior_DEV.json with current data.\n",
      "Loaded prior weather json data from S3\n",
      "Found differences between current and prior weather data.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Only a column name can be used for the key in a dtype mappings argument.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-236-c1603ddf4ca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParquetWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_new_data_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-235-12fadb1372f1>\u001b[0m in \u001b[0;36mwrite_new_data_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior_fname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mPRIOR_SUFFIX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUCKET_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_new_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite_new_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-235-12fadb1372f1>\u001b[0m in \u001b[0;36mwrite_new_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;31m# Get a df with the status chages between the prior and current json data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsons_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_json\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0mwrite_dataframe_to_parquet_on_s3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mHISTORY_SUFFIX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-230-2bd6b0097649>\u001b[0m in \u001b[0;36mjsons_to_df\u001b[0;34m(jsons, record_path)\u001b[0m\n\u001b[1;32m     96\u001b[0m     df = pd.DataFrame.from_dict(pd.json_normalize(\n\u001b[1;32m     97\u001b[0m         jsons, record_path=record_path, meta='timestamp'))\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_lifts_df_datatypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-230-2bd6b0097649>\u001b[0m in \u001b[0;36mset_lifts_df_datatypes\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;34m\"liftName\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'category'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;34m\"status\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstatus_cat_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;34m\"timeToRide\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"int\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     })\n\u001b[1;32m     90\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   5673\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcol_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5674\u001b[0m                     raise KeyError(\n\u001b[0;32m-> 5675\u001b[0;31m                         \u001b[0;34m\"Only a column name can be used for the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5676\u001b[0m                         \u001b[0;34m\"key in a dtype mappings argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5677\u001b[0m                     )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Only a column name can be used for the key in a dtype mappings argument.'"
     ]
    }
   ],
   "source": [
    "pr = ParquetWriter()\n",
    "pr.write_new_data_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>liftID</th>\n",
       "      <th>resortID</th>\n",
       "      <th>liftName</th>\n",
       "      <th>status</th>\n",
       "      <th>timeToRide</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time_diff</th>\n",
       "      <th>time_diff_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>Jersey Cream Express</td>\n",
       "      <td>O</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>Excalibur Gondola Lower</td>\n",
       "      <td>O</td>\n",
       "      <td>3</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>Magic Chair</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>Glacier Express</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>Excelerator Express</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>Catskinner Express</td>\n",
       "      <td>O</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>Crystal Ridge Express</td>\n",
       "      <td>O</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>Horstman T-Bar</td>\n",
       "      <td>X</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>Showcase T-Bar</td>\n",
       "      <td>O</td>\n",
       "      <td>3</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>Showcase T-Bar</td>\n",
       "      <td>X</td>\n",
       "      <td>3</td>\n",
       "      <td>2020-02-04 14:48:45.404567-08:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>Coca-Cola Tube Park</td>\n",
       "      <td>O</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>13</td>\n",
       "      <td>Peak 2 Peak Gondola</td>\n",
       "      <td>O</td>\n",
       "      <td>12</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>33</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Lower</td>\n",
       "      <td>O</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>34</td>\n",
       "      <td>13</td>\n",
       "      <td>Creekside Gondola</td>\n",
       "      <td>O</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>35</td>\n",
       "      <td>13</td>\n",
       "      <td>Emerald 6 Express</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "      <td>13</td>\n",
       "      <td>Big Red Express</td>\n",
       "      <td>O</td>\n",
       "      <td>8</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>37</td>\n",
       "      <td>13</td>\n",
       "      <td>Harmony 6 Express</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "      <td>Olympic Chair</td>\n",
       "      <td>O</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>40</td>\n",
       "      <td>13</td>\n",
       "      <td>Garbanzo Express</td>\n",
       "      <td>O</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>41</td>\n",
       "      <td>13</td>\n",
       "      <td>T-Bars</td>\n",
       "      <td>O</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>42</td>\n",
       "      <td>13</td>\n",
       "      <td>Symphony Express</td>\n",
       "      <td>X</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>43</td>\n",
       "      <td>13</td>\n",
       "      <td>Peak Express</td>\n",
       "      <td>O</td>\n",
       "      <td>3</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>44</td>\n",
       "      <td>13</td>\n",
       "      <td>Franz's Chair</td>\n",
       "      <td>X</td>\n",
       "      <td>8</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>45</td>\n",
       "      <td>13</td>\n",
       "      <td>Fitzsimmons Express</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "      <td>Blackcomb Gondola Lower</td>\n",
       "      <td>O</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>13</td>\n",
       "      <td>Blackcomb Gondola Upper</td>\n",
       "      <td>O</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>71</td>\n",
       "      <td>13</td>\n",
       "      <td>Excalibur Gondola Upper</td>\n",
       "      <td>O</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Upper</td>\n",
       "      <td>O</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:09:56.128553</td>\n",
       "      <td>596.128553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   liftID resortID                        liftName status  timeToRide  \\\n",
       "0       3       13              7th Heaven Express      O           6   \n",
       "18      4       13            Jersey Cream Express      O           5   \n",
       "9       5       13         Excalibur Gondola Lower      O           3   \n",
       "19      6       13                     Magic Chair      O           6   \n",
       "15      7       13                 Glacier Express      O           6   \n",
       "11      8       13             Excelerator Express      O           6   \n",
       "4       9       13              Catskinner Express      O           4   \n",
       "7      10       13           Crystal Ridge Express      O           7   \n",
       "17     11       13                  Horstman T-Bar      X           4   \n",
       "23     12       13                  Showcase T-Bar      O           3   \n",
       "24     12       13                  Showcase T-Bar      X           3   \n",
       "5      14       13             Coca-Cola Tube Park      O           4   \n",
       "21     22       13             Peak 2 Peak Gondola      O          12   \n",
       "27     33       13  Whistler Village Gondola Lower      O           5   \n",
       "6      34       13               Creekside Gondola      O           7   \n",
       "8      35       13               Emerald 6 Express      O           6   \n",
       "1      36       13                 Big Red Express      O           8   \n",
       "16     37       13               Harmony 6 Express      O           6   \n",
       "20     39       13                   Olympic Chair      O           5   \n",
       "14     40       13                Garbanzo Express      O           7   \n",
       "26     41       13                          T-Bars      O           5   \n",
       "25     42       13                Symphony Express      X           7   \n",
       "22     43       13                    Peak Express      O           3   \n",
       "13     44       13                   Franz's Chair      X           8   \n",
       "12     45       13             Fitzsimmons Express      X           6   \n",
       "2      69       13         Blackcomb Gondola Lower      O           7   \n",
       "3      70       13         Blackcomb Gondola Upper      O           7   \n",
       "10     71       13         Excalibur Gondola Upper      O           5   \n",
       "28     72       13  Whistler Village Gondola Upper      O          11   \n",
       "\n",
       "                          timestamp       time_diff  time_diff_seconds  \n",
       "0  2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "18 2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "9  2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "19 2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "15 2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "11 2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "4  2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "7  2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "17 2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "23 2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "24 2020-02-04 14:48:45.404567-08:00        00:00:00           0.000000  \n",
       "5  2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "21 2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "27 2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "6  2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "8  2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "1  2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "16 2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "20 2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "14 2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "26 2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "25 2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "22 2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "13 2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "12 2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "2  2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "3  2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "10 2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  \n",
       "28 2020-02-04 14:38:49.276014-08:00 00:09:56.128553         596.128553  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parq_df = load_dataframe_from_parquet_on_s3('lifts' + HISTORY_SUFFIX)\n",
    "\n",
    "lifts_status_changes_parq_df = get_status_durations(parq_df)\n",
    "lifts_status_changes_parq_df.to_csv(DATA_DIR + \"lifts_status_changes_parq.csv\", date_format='%c')\n",
    "lifts_status_changes_parq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "liftID                                 category\n",
       "resortID                               category\n",
       "liftName                               category\n",
       "status                                 category\n",
       "timeToRide                                int64\n",
       "timestamp     datetime64[ns, America/Vancouver]\n",
       "dtype: object"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parq_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '776FA24CA4BB91C5',\n",
       "  'HostId': 'AoNqwBsoH++T97P4H/+75E0tIrsQ22yNWJAt0mrCHf2pNROay9afsm5ZB6jToAzx4YOtebd06yc=',\n",
       "  'HTTPStatusCode': 204,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'AoNqwBsoH++T97P4H/+75E0tIrsQ22yNWJAt0mrCHf2pNROay9afsm5ZB6jToAzx4YOtebd06yc=',\n",
       "   'x-amz-request-id': '776FA24CA4BB91C5',\n",
       "   'date': 'Tue, 04 Feb 2020 22:10:42 GMT',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.Object(BUCKET_NAME, 'lifts' + PRIOR_SUFFIX).delete()\n",
    "s3.Object(BUCKET_NAME, 'terrain' + PRIOR_SUFFIX).delete()\n",
    "s3.Object(BUCKET_NAME, 'weather' + PRIOR_SUFFIX).delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Original Version\n",
    "Handles lifts only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ParquetWriter():\n",
    "    \"\"\"Identifies new data and writes it to Parquet file on S3.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Get current lift status info json\n",
    "        lifts_current = get_data()['lifts']  # String.\n",
    "        self.lifts_current_json = json.loads(lifts_current)\n",
    "\n",
    "        self.lifts_prior_object = s3.Object(BUCKET_NAME, PRIOR_STATUS_FNAME)\n",
    "\n",
    "    def write_new_data(self):\n",
    "        \"\"\"If new data since the last update of Parquet file is found, add it to the Parquet\n",
    "        file.  Save the current data as json to serve as the prior in the next comparison.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get prior lift status info json\n",
    "        try:\n",
    "            self.lifts_prior_object.load()\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] == \"404\":\n",
    "                print(\"Prior doesn't exist\")\n",
    "                # Create the prior file\n",
    "                self.save_prior_data()\n",
    "                print(f\"Created {PRIOR_STATUS_FNAME}\")\n",
    "            else:\n",
    "                # Something else has gone wrong.\n",
    "                raise\n",
    "        else:\n",
    "            # The prior exists\n",
    "            self.get_prior_data()\n",
    "            if self.data_changed():\n",
    "\n",
    "                # Get a df with the status chages between the prior and current json data\n",
    "                df = jsons_to_df([self.lifts_prior_json, self.lifts_current_json])\n",
    "                write_dataframe_to_parquet_on_s3(df, HISTORY_FNAME)\n",
    "\n",
    "                # save current lift status info json as prior\n",
    "                self.save_prior_data()\n",
    "                print(\n",
    "                    f\"Replaced data in {self.lifts_prior_object.key} with current data.\")\n",
    "\n",
    "    def get_prior_data(self):\n",
    "        lifts_prior = self.lifts_prior_object.get()[\n",
    "            'Body'].read().decode('utf-8')\n",
    "        self.lifts_prior_json = json.loads(lifts_prior)\n",
    "        print(\"Loaded prior json data from S3\")\n",
    "\n",
    "    def data_changed(self):\n",
    "        \"\"\"Compare current data json with prior data json without their timestamps.  The timestamps\n",
    "        on the current json will always be more recent even when none of the lift statuses have changed.\n",
    "        \"\"\"\n",
    "        if self.lifts_prior_json['lifts'] == self.lifts_current_json['lifts']:\n",
    "            print(\"No differences between current and prior data were found.\")\n",
    "            return False\n",
    "        else:\n",
    "            print(\"Found differences between current and prior data.\")\n",
    "            return True\n",
    "\n",
    "    def save_prior_data(self):\n",
    "        \"\"\"Save the current data as prior data on S3.\"\"\"\n",
    "        bucket.put_object(Key=PRIOR_STATUS_FNAME,\n",
    "                          Body=bytes(json.dumps(self.lifts_current_json).encode('UTF-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded prior json data from S3\n",
      "Found differences between current and prior data.\n",
      "File wb_lifts_history.parquet found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul/anaconda3/lib/python3.7/site-packages/fastparquet/writer.py:655: FutureWarning: RangeIndex._start is deprecated and will be removed in a future version. Use RangeIndex.start instead\n",
      "  index_cols = [{'name': index_cols.name, 'start': index_cols._start,\n",
      "/Users/paul/anaconda3/lib/python3.7/site-packages/fastparquet/writer.py:656: FutureWarning: RangeIndex._stop is deprecated and will be removed in a future version. Use RangeIndex.stop instead\n",
      "  'stop': index_cols._stop, 'step': index_cols._step,\n",
      "/Users/paul/anaconda3/lib/python3.7/site-packages/fastparquet/writer.py:656: FutureWarning: RangeIndex._step is deprecated and will be removed in a future version. Use RangeIndex.step instead\n",
      "  'stop': index_cols._stop, 'step': index_cols._step,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 14 records to wb_lifts_history.parquet.\n",
      "Replaced data in lifts_prior.json with current data.\n"
     ]
    }
   ],
   "source": [
    "ParquetWriter().write_new_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Warnings**\n",
    "\n",
    "See https://github.com/dask/fastparquet/issues/477 for fastparquet warnings about `RangeIndex._start, RangeIndex._stop, RangeIndex._step`\n",
    "\n",
    "\n",
    "    /Users/paul/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:90: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
    "    /Users/paul/anaconda3/lib/python3.7/site-packages/fastparquet/writer.py:655: FutureWarning: RangeIndex._start is deprecated and will be removed in a future version. Use RangeIndex.start instead\n",
    "      index_cols = [{'name': index_cols.name, 'start': index_cols._start,\n",
    "    /Users/paul/anaconda3/lib/python3.7/site-packages/fastparquet/writer.py:656: FutureWarning: RangeIndex._stop is deprecated and will be removed in a future version. Use RangeIndex.stop instead\n",
    "      'stop': index_cols._stop, 'step': index_cols._step,\n",
    "    /Users/paul/anaconda3/lib/python3.7/site-packages/fastparquet/writer.py:656: FutureWarning: RangeIndex._step is deprecated and will be removed in a future version. Use RangeIndex.step instead\n",
    "      'stop': index_cols._stop, 'step': index_cols._step,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def s3_object_exists(fname):\n",
    "    \"\"\"Check if an s3 object exists.  Returns `True` if the object exists.\"\"\"\n",
    "    try:\n",
    "        s3.Object(BUCKET_NAME, fname)\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            print(f\"{fname} doesn't exist\")\n",
    "        else:\n",
    "            raise\n",
    "    return True\n",
    "\n",
    "\n",
    "def load_dataframe_from_parquet_on_s3(fname):\n",
    "    \"\"\" Load a dataframe from a Parquet file on S3. \"\"\"\n",
    "    if s3_object_exists(fname):\n",
    "        read_file = f\"s3://{BUCKET_NAME}/{fname}\"\n",
    "        pf = ParquetFile(read_file, open_with=myopen)\n",
    "        df = pf.to_pandas()\n",
    "\n",
    "        # Reshift the timezone because parquet engines don't handle shifted timezones\n",
    "        df.loc[:, 'timestamp'] = df.loc[:, 'timestamp'].dt.tz_convert(TZ)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "parq_df = load_dataframe_from_parquet_on_s3(HISTORY_FNAME)\n",
    "\n",
    "lifts_status_changes_parq_df = get_status_durations(parq_df)\n",
    "lifts_status_changes_parq_df.to_csv(DATA_DIR + \"lifts_status_changes_parq.csv\", date_format='%c')\n",
    "lifts_status_changes_parq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>liftID</th>\n",
       "      <th>resortID</th>\n",
       "      <th>liftName</th>\n",
       "      <th>status</th>\n",
       "      <th>timeToRide</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time_diff</th>\n",
       "      <th>time_diff_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-02-03 14:21:57.064742-08:00</td>\n",
       "      <td>00:47:48.055160</td>\n",
       "      <td>2868.055160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>Showcase T-Bar</td>\n",
       "      <td>O</td>\n",
       "      <td>3</td>\n",
       "      <td>2020-02-03 14:21:57.064742-08:00</td>\n",
       "      <td>00:47:48.055160</td>\n",
       "      <td>2868.055160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>41</td>\n",
       "      <td>13</td>\n",
       "      <td>T-Bars</td>\n",
       "      <td>O</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-02-03 14:21:57.064742-08:00</td>\n",
       "      <td>00:47:48.055160</td>\n",
       "      <td>2868.055160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "      <td>Blackcomb Gondola Lower</td>\n",
       "      <td>O</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-02-03 14:21:57.064742-08:00</td>\n",
       "      <td>01:15:33.492083</td>\n",
       "      <td>4533.492083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>13</td>\n",
       "      <td>Peak 2 Peak Gondola</td>\n",
       "      <td>X</td>\n",
       "      <td>12</td>\n",
       "      <td>2020-02-03 14:21:57.064742-08:00</td>\n",
       "      <td>18:49:17.304856</td>\n",
       "      <td>67757.304856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>42</td>\n",
       "      <td>13</td>\n",
       "      <td>Symphony Express</td>\n",
       "      <td>O</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-02-04 10:51:07.675750-08:00</td>\n",
       "      <td>03:47:41.600264</td>\n",
       "      <td>13661.600264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>45</td>\n",
       "      <td>13</td>\n",
       "      <td>Fitzsimmons Express</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-02-04 11:11:03.627551-08:00</td>\n",
       "      <td>03:30:00.349858</td>\n",
       "      <td>12600.349858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>Coca-Cola Tube Park</td>\n",
       "      <td>O</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-02-04 12:41:10.947905-08:00</td>\n",
       "      <td>01:59:53.029504</td>\n",
       "      <td>7193.029504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>42</td>\n",
       "      <td>13</td>\n",
       "      <td>Symphony Express</td>\n",
       "      <td>X</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-02-04 14:38:49.276014-08:00</td>\n",
       "      <td>00:02:14.701395</td>\n",
       "      <td>134.701395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>42</td>\n",
       "      <td>13</td>\n",
       "      <td>Symphony Express</td>\n",
       "      <td>X</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-02-04 14:41:03.977409-08:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   liftID resortID                 liftName status  timeToRide  \\\n",
       "0       3       13       7th Heaven Express      O           6   \n",
       "23     12       13           Showcase T-Bar      O           3   \n",
       "26     41       13                   T-Bars      O           5   \n",
       "2      69       13  Blackcomb Gondola Lower      O           7   \n",
       "21     22       13      Peak 2 Peak Gondola      X          12   \n",
       "..    ...      ...                      ...    ...         ...   \n",
       "73     42       13         Symphony Express      O           7   \n",
       "74     45       13      Fitzsimmons Express      X           6   \n",
       "75     14       13      Coca-Cola Tube Park      O           4   \n",
       "76     42       13         Symphony Express      X           7   \n",
       "77     42       13         Symphony Express      X           7   \n",
       "\n",
       "                          timestamp       time_diff  time_diff_seconds  \n",
       "0  2020-02-03 14:21:57.064742-08:00 00:47:48.055160        2868.055160  \n",
       "23 2020-02-03 14:21:57.064742-08:00 00:47:48.055160        2868.055160  \n",
       "26 2020-02-03 14:21:57.064742-08:00 00:47:48.055160        2868.055160  \n",
       "2  2020-02-03 14:21:57.064742-08:00 01:15:33.492083        4533.492083  \n",
       "21 2020-02-03 14:21:57.064742-08:00 18:49:17.304856       67757.304856  \n",
       "..                              ...             ...                ...  \n",
       "73 2020-02-04 10:51:07.675750-08:00 03:47:41.600264       13661.600264  \n",
       "74 2020-02-04 11:11:03.627551-08:00 03:30:00.349858       12600.349858  \n",
       "75 2020-02-04 12:41:10.947905-08:00 01:59:53.029504        7193.029504  \n",
       "76 2020-02-04 14:38:49.276014-08:00 00:02:14.701395         134.701395  \n",
       "77 2020-02-04 14:41:03.977409-08:00        00:00:00           0.000000  \n",
       "\n",
       "[78 rows x 8 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifts_status_changes_parq_df.sort_values('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X', 'H', 'O'], dtype='object')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parq_df.status.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O O O O O O X X O O O X O O O X O O O X O O O X X X X X X X X X X X X X X X X X X X X O O O O O O O O O O O O O O O O O O O O O O O O O X O\n"
     ]
    }
   ],
   "source": [
    "print(*parq_df.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>liftID</th>\n",
       "      <th>resortID</th>\n",
       "      <th>liftName</th>\n",
       "      <th>status</th>\n",
       "      <th>timeToRide</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-02-03 14:21:57.064742-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-02-03 15:09:45.119902-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-02-04 09:21:14.155051-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "      <td>13</td>\n",
       "      <td>Big Red Express</td>\n",
       "      <td>O</td>\n",
       "      <td>8</td>\n",
       "      <td>2020-02-03 14:21:57.064742-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>13</td>\n",
       "      <td>Big Red Express</td>\n",
       "      <td>X</td>\n",
       "      <td>8</td>\n",
       "      <td>2020-02-03 15:37:30.556825-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>33</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Lower</td>\n",
       "      <td>X</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-02-03 15:37:30.556825-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>33</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Lower</td>\n",
       "      <td>O</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-02-04 09:11:14.369598-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Upper</td>\n",
       "      <td>O</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-02-03 14:21:57.064742-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Upper</td>\n",
       "      <td>X</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-02-03 15:37:30.556825-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Upper</td>\n",
       "      <td>O</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-02-04 09:11:14.369598-08:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   liftID resortID                        liftName status  timeToRide  \\\n",
       "0       3       13              7th Heaven Express      O           6   \n",
       "29      3       13              7th Heaven Express      X           6   \n",
       "67      3       13              7th Heaven Express      O           6   \n",
       "1      36       13                 Big Red Express      O           8   \n",
       "35     36       13                 Big Red Express      X           8   \n",
       "..    ...      ...                             ...    ...         ...   \n",
       "47     33       13  Whistler Village Gondola Lower      X           5   \n",
       "65     33       13  Whistler Village Gondola Lower      O           5   \n",
       "28     72       13  Whistler Village Gondola Upper      O          11   \n",
       "48     72       13  Whistler Village Gondola Upper      X          11   \n",
       "66     72       13  Whistler Village Gondola Upper      O          11   \n",
       "\n",
       "                          timestamp  \n",
       "0  2020-02-03 14:21:57.064742-08:00  \n",
       "29 2020-02-03 15:09:45.119902-08:00  \n",
       "67 2020-02-04 09:21:14.155051-08:00  \n",
       "1  2020-02-03 14:21:57.064742-08:00  \n",
       "35 2020-02-03 15:37:30.556825-08:00  \n",
       "..                              ...  \n",
       "47 2020-02-03 15:37:30.556825-08:00  \n",
       "65 2020-02-04 09:11:14.369598-08:00  \n",
       "28 2020-02-03 14:21:57.064742-08:00  \n",
       "48 2020-02-03 15:37:30.556825-08:00  \n",
       "66 2020-02-04 09:11:14.369598-08:00  \n",
       "\n",
       "[76 rows x 6 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parq_df.sort_values([\"liftName\", \"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Issue when running get_status_durations(parq_df)\n",
    "Resulting in error:\n",
    "\n",
    "    ~/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype)\n",
    "        705 \n",
    "        706         if len(codes) and (codes.max() >= len(dtype.categories) or codes.min() < -1):\n",
    "    --> 707             raise ValueError(\"codes need to be between -1 and \" \"len(categories)-1\")\n",
    "        708 \n",
    "        709         return cls(codes, dtype=dtype, fastpath=True)\n",
    "\n",
    "    ValueError: codes need to be between -1 and len(categories)-1\n",
    "\n",
    "\n",
    "Same error seen when running `parq_df[['status']].sort_values(by=['status'])`\n",
    "\n",
    "This was caused by missing categories (`H`) in the `status` column (and maybe others)\n",
    "\n",
    "#### Code to inspect issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   status\n",
       "0       H\n",
       "59      H\n",
       "58      H\n",
       "57      H\n",
       "56      H\n",
       "..    ...\n",
       "38      O\n",
       "39      O\n",
       "40      O\n",
       "42      O\n",
       "65      O\n",
       "\n",
       "[66 rows x 1 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test for issue\n",
    "parq_df[['status']].sort_values(by=['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X', 'H', 'O'], dtype='object')"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parq_df.status.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 2 2 2 0 2 2 2 2 2 2 2 2 0 2 2 2 0 2 2 2 0 2 0 2 0 2 2 2\n"
     ]
    }
   ],
   "source": [
    "print(*parq_df.status.cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parq_df.status.cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should be false\n",
    "parq_df.status.cat.codes.max() >= len(parq_df.status.dtype.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should be false\n",
    "parq_df.liftName.cat.codes.min() < -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for c in parq_df.columns:\n",
    "    print(parq_df[c].cat.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "liftID                   category\n",
       "resortID                 category\n",
       "liftName                 category\n",
       "status                   category\n",
       "timeToRide                  int64\n",
       "timestamp     datetime64[ns, UTC]\n",
       "dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parq_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "liftID                                      category\n",
       "resortID                                    category\n",
       "liftName                                    category\n",
       "status                                      category\n",
       "timeToRide                                     int64\n",
       "timestamp     datetime64[ns, pytz.FixedOffset(-480)]\n",
       "dtype: object"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifts_status_changes_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "parq_df[\"timestamp\"] = pd.to_datetime(pd.Series(np.asarray(parq_df[\"timestamp\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': array(['H', 'O', 'X'], dtype=object)}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_file = f\"s3://{BUCKET_NAME}/{fname}.parquet\"\n",
    "pf = ParquetFile(read_file, open_with=myopen)\n",
    "\n",
    "# Check the categories for a specific row group\n",
    "pf.grab_cats(columns='status', row_group_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If partitioning by column, gives known values for each column\n",
    "pf.cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Possible solutions\n",
    "1. Remove partitioning by date column when writing to parquet\n",
    "2. Set status categories manually via `set_categories`. (and any other columns with the same issue.  See https://github.com/dask/dask/issues/2944\n",
    "3. Leave problem columns as text-based when writing and loading from parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing timestamps for file loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file = f\"s3://{BUCKET_NAME}/{fname}.parquet\"\n",
    "pf = ParquetFile(read_file, open_with=myopen)\n",
    "test = pf.to_pandas()[\"timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Converting timezone-aware DatetimeArray to timezone-naive ndarray with 'datetime64[ns]' dtype. In the future, this will return an ndarray with 'object' dtype where each element is a 'pandas.Timestamp' with the correct 'tz'.\n",
      "\tTo accept the future behavior, pass 'dtype=object'.\n",
      "\tTo keep the old behavior, pass 'dtype=\"datetime64[ns]\"'.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# If needed: to convert for categorical datetime to regular datetime\n",
    "df[\"timestamp\"] = pd.to_datetime(pd.Series(np.asarray(df[\"timestamp\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.dt = test.dt.tz_convert(tz= 'America/Vancouver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/Users/paul/anaconda3/lib/python3.7/site-packages/pandas/core/series.py:597: FutureWarning: Converting timezone-aware DatetimeArray to timezone-naive ndarray with 'datetime64[ns]' dtype. In the future, this will return an ndarray with 'object' dtype where each element is a 'pandas.Timestamp' with the correct 'tz'.\n",
    "\tTo accept the future behavior, pass 'dtype=object'.\n",
    "\tTo keep the old behavior, pass 'dtype=\"datetime64[ns]\"'.\n",
    "\n",
    "\n",
    "more info: https://pandas-docs.github.io/pandas-docs-travis/whatsnew/v0.24.0.html#converting-timezone-aware-series-and-index-to-numpy-arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "liftID        category\n",
       "resortID      category\n",
       "liftName      category\n",
       "status        category\n",
       "timeToRide       int64\n",
       "timestamp     category\n",
       "dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataframe_from_parquet_on_s3(fname).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing local parquet saves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parquet(df, fname):\n",
    "    # parquet engines don't handle shifted timezones\n",
    "    df.loc[:, 'timestamp'] = df.loc[:, 'timestamp'].dt.tz_convert(pytz.utc)\n",
    "\n",
    "    # Note: May need snappy-python as a req to run on AWS Lambda\n",
    "    df.to_parquet(DATA_DIR + fname + '.parquet',\n",
    "                  engine='fastparquet',\n",
    "                  partition_on=['timestamp'],\n",
    "                  file_scheme='mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_parquet(df[0:3].copy(), 'wb_lifts_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[20:22, :].copy().to_parquet(DATA_DIR + 'wb_lifts_history' + '.parquet',\n",
    "              engine='fastparquet',\n",
    "              partition_on=['timestamp'],\n",
    "              file_scheme='mixed',\n",
    "              append=True)\n",
    "# Catch exception that is doesn't exist here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: change time_diff to \"duration\"\n",
    "# test on lambda\n",
    "# make datatype dict for and general set datatypes function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_lifts_history():\n",
    "    s3.Object(BUCKET_NAME, HISTORY_FNAME).delete()\n",
    "\n",
    "def del_lifts_prior():\n",
    "    s3.Object(BUCKET_NAME, PRIOR_STATUS_FNAME).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_lifts_history()\n",
    "del_lifts_prior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "437px",
    "left": "869px",
    "right": "20px",
    "top": "120px",
    "width": "391px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
