{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul/anaconda3/lib/python3.7/site-packages/fastparquet/dataframe.py:5: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n",
      "  from pandas.core.index import CategoricalIndex, RangeIndex, Index, MultiIndex\n"
     ]
    }
   ],
   "source": [
    "import json, glob, boto3, os\n",
    "import pdb\n",
    "import pandas as pd\n",
    "from fastparquet import write, ParquetFile\n",
    "import pytz\n",
    "import s3fs\n",
    "import botocore\n",
    "from collections import Iterable\n",
    "from typing import List, Union\n",
    "from copy import deepcopy\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "BUCKET_NAME = 'snowbot-pv'\n",
    "\n",
    "# S3 Connect\n",
    "s3 = session.resource('s3')\n",
    "bucket = s3.Bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet engines don't handle shifted timezones\n",
    "TZ = pytz.timezone('America/Vancouver')\n",
    "\n",
    "DATA_DIR = \"../data/\"\n",
    "MERGED_JSON_FILENAME = \"merged_file.json\"\n",
    "merged_json_file = DATA_DIR + MERGED_JSON_FILENAME\n",
    "\n",
    "# Used for weather data in jsons_to_df()\n",
    "weather_meta_fields = [\n",
    "    'newSnow', 'last24Hours', 'last48Hours', 'last7Days', 'midMountainBase',\n",
    "    'resortID'\n",
    "]\n",
    "weather_record_path = ['weather', 'weatherForecast']\n",
    "weather_meta = [['weather', i] for i in weather_meta_fields]\n",
    "weather_meta.append('timestamp')\n",
    "\n",
    "# Used for lift and terrain status in jsons_to_df()\n",
    "# Important to set categories because when writing incrementally to parquet, some increments\n",
    "# may not include all statuses.  Manually setting the categories avoids errors due to\n",
    "# different catergory indexing between increments.\n",
    "status_cat_dtype = pd.api.types.CategoricalDtype(categories=['X', 'H', 'O'],\n",
    "                                                 ordered=True)\n",
    "groomed_cat_dtype = pd.api.types.CategoricalDtype(categories=['No', 'Yes'],\n",
    "                                                  ordered=True)\n",
    "\n",
    "# Column dtypes that are to be set for each dataframe\n",
    "df_dtypes = {\n",
    "    \"lifts\": {\n",
    "        'liftID': 'category',\n",
    "        'resortID': 'category',\n",
    "        'liftName': 'category',\n",
    "        'status': status_cat_dtype,\n",
    "        'timeToRide': 'object'\n",
    "    },\n",
    "    'terrain': {\n",
    "        'runID': 'category',\n",
    "        'resortID': 'category',\n",
    "        'groomed': groomed_cat_dtype,\n",
    "        'runName': 'category',\n",
    "        'runType': 'category',\n",
    "        'status': status_cat_dtype,\n",
    "        'terrainName': 'category'\n",
    "    },\n",
    "    'weather': {\n",
    "        'resortID': 'category',\n",
    "        'forecast.dayDescription': 'object',\n",
    "        'forecast.daycode': 'category',\n",
    "        'forecast.forecastString': 'object',\n",
    "        'forecast.iconName': 'object',\n",
    "        'forecast.summaryDescription': 'object',\n",
    "        'forecast.temperatureHigh': 'object',\n",
    "        'forecast.temperatureLow': 'object',\n",
    "        'weather.last24Hours': 'object',\n",
    "        'weather.last48Hours': 'object',\n",
    "        'weather.last7Days': 'object',\n",
    "        'weather.midMountainBase': 'object',\n",
    "        'weather.newSnow': 'object'\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def flatten(items):\n",
    "    \"\"\"Yield items from any nested iterable\"\"\"\n",
    "    for x in items:\n",
    "        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n",
    "            for sub_x in flatten(x):\n",
    "                yield sub_x\n",
    "        else:\n",
    "            yield x\n",
    "\n",
    "\n",
    "# The columns that serve to identify records for each topic\n",
    "topic_ID_col_names = {\n",
    "    'lifts': ['resortID', 'liftName'],\n",
    "    'terrain': ['resortID', 'runID', 'terrainName'],\n",
    "    'weather': 'resortID',\n",
    "    'all_topics': 'timestamp'\n",
    "}\n",
    "# All of the column names that serve to identify records in at least one of the topics\n",
    "all_ID_col_names = set(flatten(topic_ID_col_names.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://alexwlchan.net/2019/07/listing-s3-keys/\n",
    "def get_matching_s3_objects(bucket, prefix=\"\", suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Generate objects in an S3 bucket.\n",
    "\n",
    "    :param bucket: Name of the S3 bucket.\n",
    "    :param prefix: Only fetch objects whose key starts with\n",
    "        this prefix (optional).\n",
    "    :param suffix: Only fetch objects whose keys end with\n",
    "        this suffix (optional).\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    kwargs = {'Bucket': bucket}\n",
    "\n",
    "    # We can pass the prefix directly to the S3 API.  If the user has passed\n",
    "    # a tuple or list of prefixes, we go through them one by one.\n",
    "    if isinstance(prefix, str):\n",
    "        prefixes = (prefix, )\n",
    "    else:\n",
    "        prefixes = prefix\n",
    "\n",
    "    for key_prefix in prefixes:\n",
    "        kwargs[\"Prefix\"] = key_prefix\n",
    "\n",
    "        for page in paginator.paginate(**kwargs):\n",
    "            try:\n",
    "                contents = page[\"Contents\"]\n",
    "            except KeyError:\n",
    "                return\n",
    "\n",
    "            for obj in contents:\n",
    "                key = obj[\"Key\"]\n",
    "                if key.endswith(suffix):\n",
    "                    yield obj\n",
    "\n",
    "\n",
    "def get_matching_s3_keys(bucket, prefix=\"\", suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Generate the keys in an S3 bucket.\n",
    "\n",
    "    :param bucket: Name of the S3 bucket.\n",
    "    :param prefix: Only fetch keys that start with this prefix (optional).\n",
    "    :param suffix: Only fetch keys that end with this suffix (optional).\n",
    "    \"\"\"\n",
    "    for obj in get_matching_s3_objects(bucket, prefix, suffix):\n",
    "        yield obj[\"Key\"]\n",
    "\n",
    "\n",
    "def merge_matching_jsons_on_s3(save_file, prefix=\"\", suffix=\"\"):\n",
    "    \"\"\"Merges json files on S3 that match the suffix into a new json and save it\n",
    "    as the save_file on S3.\"\"\"\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for f in get_matching_s3_keys(BUCKET_NAME, prefix=prefix, suffix=suffix):\n",
    "\n",
    "        # Write the file from S3 into a local temp file\n",
    "        with open('temp', 'wb') as tfw:\n",
    "            bucket.download_fileobj(f, tfw)\n",
    "\n",
    "        # Append the local temp file into the result list\n",
    "        with open('temp', 'rb') as tfr:\n",
    "            result.append(json.load(tfr))\n",
    "\n",
    "    os.remove(\"temp\")\n",
    "\n",
    "    # Fill the output file with the merged content\n",
    "    with open(save_file, \"w\") as outfile:\n",
    "        json.dump(result, outfile)\n",
    "\n",
    "# TBD: more efficient to go straight to df w/o saving json to file\n",
    "\n",
    "\n",
    "def set_df_datatypes(df, topic):\n",
    "    \"\"\"Set the datatypes for a df according to the topic that\n",
    "    it represents.\"\"\"\n",
    "    df = df.astype(df_dtypes[topic])\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def jsons_to_df(jsons, record_path, meta='timestamp'):\n",
    "    \"\"\"Convert a json containing one or more timestamps to a dataframe.\"\"\"\n",
    "    if record_path == 'weather':\n",
    "        # Deal with the nested object that the weather data uses to store the weather forecast\n",
    "        df = pd.json_normalize(jsons, record_path=weather_record_path,\n",
    "                               meta=weather_meta, record_prefix='forecast.')\n",
    "        df.rename(columns={\"weather.resortID\": \"resortID\"}, inplace=True)\n",
    "    else:\n",
    "        df = pd.json_normalize(jsons, record_path=record_path,\n",
    "                               meta=meta)\n",
    "\n",
    "    df = set_df_datatypes(df, record_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_json_as_df(merged_json_file, record_path):\n",
    "    \"\"\"Load json file containing one or more timestamps as a dataframe.\"\"\"\n",
    "    with open(merged_json_file, \"r\") as f:\n",
    "        d = json.load(f)\n",
    "        df = jsons_to_df(d, record_path)\n",
    "        return df\n",
    "\n",
    "\n",
    "def get_data_changes(df, topic, keep_oldest=False):\n",
    "    \"\"\"\n",
    "    Filter out rows that do not represent changed data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Includes 'timestamp' identifying and data columns.  Lists data for each timestamp.\n",
    "    keep_oldest : boolean\n",
    "        Indicates if the returned DataFrame should keep the oldest record for each entity (i.e.\n",
    "        lift, resort, tor terrain) even if an entity has no data changes.  This is so that the\n",
    "        earliest data for each entity is not lost, and all entities are listed the returned DataFrame\n",
    "        even if their data has not changed.  Use `False` when there is just one DataFrame to process.\n",
    "        Use `True` is cases where the data changes will be appended to an existing dataframe that\n",
    "        already has at least one row for each entity.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Only includes the rows from the original dataframe where there was a change to new values\n",
    "        in the data columns.\n",
    "    \"\"\"\n",
    "    ID_columns = topic_ID_col_names[topic]\n",
    "    data_columns = [c for c in df.columns if c not in all_ID_col_names]\n",
    "\n",
    "    def filter_for_data_changes(df, keep_oldest=keep_oldest):\n",
    "        \"\"\"Filter out rows where data is unchanged for adjacent timestamps.\n",
    "        Required to handle cases when there are > 2 rows per entity.\n",
    "        \"\"\"\n",
    "        # TBD: make more efficient via slice_shift(), which doesn't copy data,\n",
    "        # instead of shift()?\n",
    "        keep_idx = df[data_columns].ne(df[data_columns].shift()).any(\n",
    "            axis=1).values[1:]  # True for rows with data changes\n",
    "        changed_rows = df.reset_index(drop=True).drop(index=0)[keep_idx]\n",
    "\n",
    "        if keep_oldest:\n",
    "            firstrow = df.loc[df['timestamp'].idxmin()]\n",
    "            keep_df = firstrow.to_frame().T.append(changed_rows)\n",
    "        else:\n",
    "            keep_df = changed_rows\n",
    "\n",
    "        return keep_df\n",
    "\n",
    "    # Drop any rows that are complete duplicates so that conditional evaluation will\n",
    "    # work.  This is required for Peak 2 Peak Gondola because it is duplicated in the\n",
    "    # lifts data.  Maybe others as well.\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # 1 means that there were up to 2 rows found per group\n",
    "    if df.groupby(ID_columns, group_keys=False).cumcount().max() < 2:\n",
    "        # Most efficient method.  Only works if there are 2 or less rows per entity.\n",
    "        subset = df.columns.drop('timestamp')\n",
    "        df = df.sort_values('timestamp')\n",
    "\n",
    "        if keep_oldest:\n",
    "            df = df.drop_duplicates(subset=subset, keep='first')\n",
    "        else:\n",
    "            df = df.drop_duplicates(subset=subset, keep=False)\n",
    "            df = df.drop_duplicates(subset=ID_columns, keep='last')\n",
    "\n",
    "    else:\n",
    "        # Less efficient method.  Required if there are > 2 rows per entity.\n",
    "        df = df.sort_values('timestamp').groupby(ID_columns, group_keys=False)\\\n",
    "               .apply(filter_for_data_changes)\\\n",
    "               .reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    records_are_unique(df, include_timestamp_in_colnames(ID_columns))\n",
    "    df = set_df_datatypes(df, topic)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def records_are_unique(df: pd.DataFrame, record_id_cols: List[str]) -> bool:\n",
    "    \"\"\"Check if records in df can be uniquely identified using record_id_cols\n",
    "    and raise warning if they are not.\"\"\"\n",
    "    are_unique = df.set_index(record_id_cols).index.is_unique\n",
    "    if not are_unique:\n",
    "        warnings.warn(f\"Records in dataframe are not uniquely identified by {record_id_cols}\")\n",
    "    return are_unique\n",
    "\n",
    "\n",
    "def include_timestamp_in_colnames(col_names: Union[List[str], str]) -> List[str]:\n",
    "    \"\"\"Returns a list of strings which includes 'timestamp' in addition to the list\n",
    "    or sting given for `col_names`.\n",
    "    \n",
    "    >>> include_timestamp_in_colnames(topic_ID_col_names['terrain'])\n",
    "    ['resortID', 'runID', 'terrainName', 'timestamp']\n",
    "    \"\"\"\n",
    "    col_names = deepcopy(col_names)\n",
    "    if type(col_names) == str : col_names = [col_names]\n",
    "    col_names.extend(['timestamp'])\n",
    "    return col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process lift json fies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_status_durations(lifts_df):\n",
    "    '''Calculate values and add columns for the time difference between the\n",
    "    timestamp for the current status and the timestamp for the next status\n",
    "    for each lift:\n",
    "    `time_diff` column: Gives the duration that the lift was in the status indicated in the `status` column.\n",
    "    `time_diff_seconds` column: `time_diff` converted to seconds.\n",
    "    \n",
    "    lifts_status_changes_df should be TBD\n",
    "    '''\n",
    "    # TBD: optimize if needed via # 3 under:\n",
    "    # https://towardsdatascience.com/pandas-tips-and-tricks-33bcc8a40bb9\n",
    "    record_id_cols = include_timestamp_in_colnames(topic_ID_col_names['lifts'])\n",
    "    df = lifts_df.sort_values(by=record_id_cols)\n",
    "    df['time_diff'] = df.groupby(['resortID', 'liftID'])['timestamp'].diff(1).shift(-1)\n",
    "\n",
    "    # Fill in the durations which will be missing for the most recent status changes\n",
    "    missing_time_diffs_idx = df.loc[(df['time_diff'].isnull()) & (\n",
    "        df['timestamp'] >= df['timestamp'].min()), 'timestamp'].index.values\n",
    "\n",
    "    df.loc[missing_time_diffs_idx, 'time_diff'] = df['timestamp'].max(\n",
    "    ) - df.loc[missing_time_diffs_idx, 'timestamp']\n",
    "\n",
    "    # Convert to seconds\n",
    "    df['time_diff_seconds'] = df['time_diff'].dt.total_seconds()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whistler Lifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_matching_jsons_on_s3(suffix=\"lifts.json\", save_file=merged_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whis_lifts_df = load_json_as_df(merged_json_file, 'lifts')\n",
    "\n",
    "whis_lifts_status_changes_df = get_data_changes(whis_lifts_df, 'lifts', keep_oldest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifts_status_changes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** `timeToRide` is just the time is takes to ride the lift, not the current wait time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whis_lifts_df.groupby(\"liftName\")['timeToRide'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whis_lifts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whis_lifts_df = get_status_durations(whis_lifts_status_changes_df)\n",
    "\n",
    "# Uses local date formatting, otherwise Tableau will mix up month and day\n",
    "# alternatively, can export to json:\n",
    "# lifts_status_changes_df.to_json(DATA_DIR + \"lifts_status_changes.json\", orient='table')\n",
    "whis_lifts_df.to_csv(DATA_DIR + \"whis_lifts_status_changes.csv\", date_format='%c')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add:\n",
    "# \n",
    "# daily: for each chair calculate most open status of the day: O > H > X\n",
    "# Days since each chair was last seen open with timestamp of most recent open time.\n",
    "# snowfall since last open\n",
    "# save data for other mountains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through JSON files for all topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in ['lifts', 'terrain', 'weather']:\n",
    "    merge_matching_jsons_on_s3(suffix=topic + \".json\", save_file=merged_json_file)\n",
    "    df = load_json_as_df(merged_json_file, topic)\n",
    "    status_changes_df = get_data_changes(df, topic, keep_oldest=True)\n",
    "    \n",
    "    if topic == 'lifts':\n",
    "        get_status_durations(status_changes_df).to_csv(DATA_DIR + 'whis_lifts_status_changes.csv', date_format='%c')\n",
    "    else:\n",
    "        status_changes_df.to_csv(DATA_DIR + 'whis_' + topic + '_status_changes.csv', date_format='%c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Storage options testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle(DATA_DIR + \"df_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fastparquet import write\n",
    "\n",
    "# parquet engines don't handle shifted timezones\n",
    "import pytz\n",
    "TZ = pytz.timezone('America/Vancouver')\n",
    "df['timestamp'] = df.timestamp.dt.tz_convert(pytz.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Note: May need snappy-python as a req to run on AWS Lambda\n",
    "df.to_parquet(DATA_DIR + \"df_test.parquet\", engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "load_df = pd.read_parquet(DATA_DIR + \"df_test.parquet\")\n",
    "load_df['timestamp'] = load_df.timestamp.dt.tz_convert(TZ) # convert back to correct timezone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#TBD convert back to correct datatypes\n",
    "load_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(DATA_DIR + \"df_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Test file size results:\n",
    "- json: 800 Kb?\n",
    "- csv: 474 Kb\n",
    "- pickle: 145 Kb\n",
    "- parquet: 15 Kb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Delta Lake Notes\n",
    "\n",
    "Requires apache spark instance.  For future use, could set one up to work with lambda using https://aws.amazon.com/emr/features/spark/?\n",
    "\n",
    "Otherwise databricks (similar to QxMD project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet on S3\n",
    "\n",
    "For all topics from the EpicMix API.  Compare most recent topic data from json on S3 and if the data has changes, append the changes to parquet file on S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "fs = s3fs.S3FileSystem()\n",
    "myopen = fs.open\n",
    "nop = lambda *args, **kwargs: None\n",
    "\n",
    "HISTORY_SUFFIX = '_history_DEV.parquet'\n",
    "PRIOR_SUFFIX = '_prior_DEV.json'\n",
    "\n",
    "\n",
    "def write_dataframe_to_parquet_on_s3(df, topic, fname):\n",
    "    \"\"\" Write a dataframe to a Parquet file on S3.  Creates a new parquet file if one\n",
    "    doesn't already exist.\n",
    "    \"\"\"\n",
    "\n",
    "    def write_parquet(df, fname, app=True):\n",
    "\n",
    "        output_file = f\"s3://{BUCKET_NAME}/{fname}\"\n",
    "        write(output_file,\n",
    "              df,\n",
    "              # partition_on=['timestamp'],\n",
    "              file_scheme='hive',\n",
    "              append=app,  # need to remove or catch exception to work when file doesn't exist\n",
    "              open_with=myopen,\n",
    "              mkdirs=nop)\n",
    "        print(f\"Writing {len(df)} records to {fname}.\")\n",
    "\n",
    "    # Unshift the timezone because parquet engines don't handle shifted timezones\n",
    "    df.loc[:, 'timestamp'] = df.loc[:, 'timestamp'].dt.tz_convert(pytz.utc)\n",
    "\n",
    "    s3_object = bucket.Object(fname)\n",
    "\n",
    "    if not list(bucket.objects.filter(Prefix=fname)):\n",
    "        print(f\"File {fname} not found.  Creating new file.\")\n",
    "        # Keep oldest record for each entity because creating new file\n",
    "        df = get_data_changes(df, topic=topic, keep_oldest=True)\n",
    "        write_parquet(df, fname, app=False)\n",
    "\n",
    "    else:\n",
    "        print(f\"File {fname} found on S3.\")\n",
    "        df = get_data_changes(df, topic=topic, keep_oldest=False)\n",
    "        write_parquet(df, fname, app=True)\n",
    "\n",
    "\n",
    "def filter_resort(data, resortID: int = None) -> dict:\n",
    "    \"\"\"Filter for a specific resort.\"\"\"\n",
    "    if resortID:\n",
    "        return data[\"resortID\"] == resortID\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "\n",
    "def get_data(filter_topic: Union[str, List] = None, filter_resortID: int = None) -> dict:\n",
    "    \"\"\"Get data from EpicMix API. Defaults to all resorts.  Option to filter for a\n",
    "    specific resort or topic.\n",
    "    \"\"\"\n",
    "    API_URL = 'http://www.epicmix.com/vailresorts/sites/epicmix/api/mobile/'\n",
    "    # keys are used in the requests, the values and used in the response\n",
    "    DATA_LIST = {'lifts': 'lifts',\n",
    "                 'weather': 'snowconditions', 'terrain': 'terrains'}\n",
    "    json_data = dict()\n",
    "\n",
    "    # Create lists to filter by topic\n",
    "    if filter_topic is not None:\n",
    "        filtered_data_list = {k: v for k,\n",
    "                              v in DATA_LIST.items() if k in filter_topic}\n",
    "    else:\n",
    "        filtered_data_list = DATA_LIST\n",
    "\n",
    "    for d, name in filtered_data_list.items():\n",
    "        res = requests.get(API_URL + d + '.ashx')\n",
    "        res.raise_for_status()\n",
    "        data = json.loads(res.text)[name]\n",
    "        data = list(filter(lambda x: filter_resort(x, filter_resortID), data))\n",
    "        json_data[d] = json.dumps(\n",
    "            {'timestamp': str(datetime.now(TZ)), d: data})\n",
    "\n",
    "    return json_data\n",
    "\n",
    "\n",
    "def s3_object_exists(fname):\n",
    "    \"\"\"Check if an s3 object exists.  Returns `True` if the object exists.\"\"\"\n",
    "    try:\n",
    "        bucket.Object(fname)\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            print(f\"{fname} doesn't exist\")\n",
    "        else:\n",
    "            raise\n",
    "    return True\n",
    "\n",
    "\n",
    "def load_dataframe_from_parquet_on_s3(fname):\n",
    "    \"\"\" Load a dataframe from a Parquet file on S3. \"\"\"\n",
    "    if s3_object_exists(fname):\n",
    "        read_file = f\"s3://{BUCKET_NAME}/{fname}\"\n",
    "        pf = ParquetFile(read_file, open_with=myopen)\n",
    "        df = pf.to_pandas()\n",
    "\n",
    "        # Reshift the timezone because parquet engines don't handle shifted timezones\n",
    "        df.loc[:, 'timestamp'] = df.loc[:, 'timestamp'].dt.tz_convert(TZ)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "class api_data():\n",
    "    def __init__(self, topic: str, current_json: str):\n",
    "        self.topic = topic\n",
    "        self.current_json = current_json\n",
    "        # May not exist yet\n",
    "        self.prior_fname = topic + PRIOR_SUFFIX\n",
    "        self.prior_object = bucket.Object(self.prior_fname)\n",
    "        self.check_prior_object()\n",
    "\n",
    "    def check_prior_object(self):\n",
    "        \"\"\"Get prior data json\"\"\"\n",
    "        try:\n",
    "            self.prior_object.load()\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] == \"404\":\n",
    "                print(f\"Prior json for {self.topic} doesn't exist\")\n",
    "                self.prior_exists = False\n",
    "            else:\n",
    "                # Something else has gone wrong.\n",
    "                raise\n",
    "        else:\n",
    "            self.prior_exists = True\n",
    "        return self.prior_exists\n",
    "\n",
    "    def get_prior_data_json(self):\n",
    "        \"\"\"Get prior data json from S3.\"\"\"\n",
    "        if self.prior_exists == True:\n",
    "            prior = self.prior_object.get()['Body'].read().decode('utf-8')\n",
    "            self.prior_json = json.loads(prior)\n",
    "            print(f\"Loaded prior {self.topic} json data from S3\")\n",
    "            return self.prior_json\n",
    "        else:\n",
    "            print(f\"Prior json for {self.topic} doesn't exist\")\n",
    "\n",
    "    def data_changed(self):\n",
    "        \"\"\"Compare current data json with prior data json without their timestamps.  The timestamps\n",
    "        on the current json will always be more recent even when none of the other data has changed.\n",
    "        \"\"\"\n",
    "        if self.prior_json[self.topic] == self.current_json[self.topic]:\n",
    "            print(\n",
    "                f\"No differences between current and prior {self.topic} data were found.\")\n",
    "            return False\n",
    "        else:\n",
    "            print(\n",
    "                f\"Found differences between current and prior {self.topic} data.\")\n",
    "            return True\n",
    "\n",
    "    def save_prior_data(self):\n",
    "        \"\"\"Save the current data as prior data on S3.\"\"\"\n",
    "        bucket.put_object(Key=self.prior_fname,\n",
    "                          Body=bytes(json.dumps(self.current_json).encode('UTF-8')))\n",
    "\n",
    "\n",
    "class ParquetWriter():\n",
    "    \"\"\"Identifies new data and writes it to Parquet file on S3.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Get current data\n",
    "        self.data_current_all = get_data()  # String.\n",
    "\n",
    "    def write_new_data_all(self):\n",
    "        \"\"\"Writes new data for each type (i.e. 'lift', 'weather', 'terrian')\n",
    "        of data returned by the API.\n",
    "        \"\"\"\n",
    "        for topic in self.data_current_all:\n",
    "            current_json = json.loads(self.data_current_all[topic])\n",
    "            data = api_data(topic, current_json)\n",
    "            self.write_new_data(data)\n",
    "\n",
    "    def write_new_data(self, api_data):\n",
    "        \"\"\"If current data has changed since the last update of Parquet file is, add it\n",
    "        to the Parquet file.  Save the current data as json to serve as the prior for\n",
    "        the next comparison.\n",
    "        \"\"\"\n",
    "\n",
    "        if api_data.prior_exists:\n",
    "            api_data.get_prior_data_json()\n",
    "            if api_data.data_changed():\n",
    "                # Get a df with the chages between the prior and current json data\n",
    "                df = jsons_to_df(\n",
    "                    [api_data.prior_json, api_data.current_json], record_path=api_data.topic)\n",
    "                write_dataframe_to_parquet_on_s3(\n",
    "                    df, api_data.topic, api_data.topic + HISTORY_SUFFIX)\n",
    "\n",
    "                # save current data json as prior\n",
    "                api_data.save_prior_data()\n",
    "                print(\n",
    "                    f\"Replaced data in {api_data.prior_object.key} with current data.\")\n",
    "        else:\n",
    "            print(f\"Prior json for {api_data.topic} doesn't exist\")\n",
    "            # Create the prior file\n",
    "            api_data.save_prior_data()\n",
    "            print(f\"Created {api_data.prior_fname}\")\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded prior lifts json data from S3\n",
      "No differences between current and prior lifts data were found.\n",
      "\n",
      "\n",
      "Loaded prior weather json data from S3\n",
      "Found differences between current and prior weather data.\n",
      "File weather_history_DEV.parquet found on S3.\n",
      "Writing 8 records to weather_history_DEV.parquet.\n",
      "Replaced data in weather_prior_DEV.json with current data.\n",
      "\n",
      "\n",
      "Loaded prior terrain json data from S3\n",
      "Found differences between current and prior terrain data.\n",
      "File terrain_history_DEV.parquet found on S3.\n",
      "Writing 9 records to terrain_history_DEV.parquet.\n",
      "Replaced data in terrain_prior_DEV.json with current data.\n",
      "\n",
      "\n",
      "CPU times: user 1.06 s, sys: 168 ms, total: 1.23 s\n",
      "Wall time: 5.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pr = ParquetWriter()\n",
    "pr.write_new_data_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "while True:\n",
    "    print('\\n\\n\\n' + time.ctime() + ':\\n---------------------')\n",
    "    pr = ParquetWriter()\n",
    "    pr.write_new_data_all()\n",
    "    time.sleep(3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warnings**\n",
    "\n",
    "See https://github.com/dask/fastparquet/issues/477 for fastparquet warnings about `RangeIndex._start, RangeIndex._stop, RangeIndex._step`\n",
    "\n",
    "\n",
    "    /Users/paul/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:90: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
    "    /Users/paul/anaconda3/lib/python3.7/site-packages/fastparquet/writer.py:655: FutureWarning: RangeIndex._start is deprecated and will be removed in a future version. Use RangeIndex.start instead\n",
    "      index_cols = [{'name': index_cols.name, 'start': index_cols._start,\n",
    "    /Users/paul/anaconda3/lib/python3.7/site-packages/fastparquet/writer.py:656: FutureWarning: RangeIndex._stop is deprecated and will be removed in a future version. Use RangeIndex.stop instead\n",
    "      'stop': index_cols._stop, 'step': index_cols._step,\n",
    "    /Users/paul/anaconda3/lib/python3.7/site-packages/fastparquet/writer.py:656: FutureWarning: RangeIndex._step is deprecated and will be removed in a future version. Use RangeIndex.step instead\n",
    "      'stop': index_cols._stop, 'step': index_cols._step,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parquet and save as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "codes need to be between -1 and len(categories)-1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d83b4f4eda32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mparq_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataframe_from_parquet_on_s3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lifts'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mHISTORY_SUFFIX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlifts_status_changes_parq_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_status_durations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparq_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m lifts_status_changes_parq_df.to_csv(\n\u001b[1;32m      4\u001b[0m     DATA_DIR + \"lifts_status_changes_parq.csv\", date_format='%c')\n",
      "\u001b[0;32m<ipython-input-16-9ff47d7104ec>\u001b[0m in \u001b[0;36mget_status_durations\u001b[0;34m(lifts_df)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# https://towardsdatascience.com/pandas-tips-and-tricks-33bcc8a40bb9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mrecord_id_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minclude_timestamp_in_colnames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_ID_col_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lifts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlifts_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecord_id_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time_diff'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'resortID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'liftID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36msort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index)\u001b[0m\n\u001b[1;32m   4932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4933\u001b[0m         new_data = self._data.take(\n\u001b[0;32m-> 4934\u001b[0;31m             \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4935\u001b[0m         )\n\u001b[1;32m   4936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m         return self.reindex_indexer(\n\u001b[0;32m-> 1394\u001b[0;31m             \u001b[0mnew_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1395\u001b[0m         )\n\u001b[1;32m   1396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[1;32m   1265\u001b[0m                     ),\n\u001b[1;32m   1266\u001b[0m                 )\n\u001b[0;32m-> 1267\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m             ]\n\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1265\u001b[0m                     ),\n\u001b[1;32m   1266\u001b[0m                 )\n\u001b[0;32m-> 1267\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m             ]\n\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_tuple)\u001b[0m\n\u001b[1;32m   1833\u001b[0m         \u001b[0;31m# but are passed the axis depending on the calling routing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m         \u001b[0;31m# if its REALLY axis 0, then this will be a reindex and not a take\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1835\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m         \u001b[0;31m# Called from three places in managers, all of which satisfy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, allow_fill, fill_value)\u001b[0m\n\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m         \u001b[0mcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1856\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_codes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1857\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36mfrom_codes\u001b[0;34m(cls, codes, categories, ordered, dtype)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"codes need to be between -1 and len(categories)-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: codes need to be between -1 and len(categories)-1"
     ]
    }
   ],
   "source": [
    "parq_df = load_dataframe_from_parquet_on_s3('lifts' + HISTORY_SUFFIX)\n",
    "lifts_status_changes_parq_df = get_status_durations(parq_df)\n",
    "lifts_status_changes_parq_df.to_csv(\n",
    "    DATA_DIR + \"lifts_status_changes_parq.csv\", date_format='%c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_DIR = \"../data/test/\"\n",
    "TEST_VALIDATION_DATA_DIR = TEST_DATA_DIR + \"valid/\"\n",
    "lifts_json_test_file = TEST_DATA_DIR + \"lifts_test.json\"\n",
    "terrain_json_test_file = TEST_DATA_DIR + \"terrain_test.json\"\n",
    "weather_json_test_file = TEST_DATA_DIR + \"weather_test.json\"\n",
    "merged_lifts_json_test_file = TEST_DATA_DIR + \"merged_lifts_test.json\"\n",
    "merged_terrain_json_test_file = TEST_DATA_DIR + \"merged_terrain_test.json\"\n",
    "merged_weather_json_test_file = TEST_DATA_DIR + \"merged_weather_test.json\"\n",
    "merged_whis_lifts_json_test_file = TEST_DATA_DIR + \"merged_whis_lifts_test.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find column combinations to identify entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Move to EDA notebook\n",
    "# Do we really need to use all the terrain columns in topic_ID_col_names to uniquely identify each run?\n",
    "ID_cols = ['resortID', 'runID', 'runName', 'terrainName']\n",
    "df = load_json_as_df(terrain_json_test_file, 'terrain')\n",
    "\n",
    "for combo in combinations(ID_cols, len(ID_cols)-1):\n",
    "    print(f\"Combo: {combo}\\tDuplicates: {df.duplicated(combo).sum()}\")\n",
    "    \n",
    "print(f\"Combo: {ID_cols}\\tDuplicates: {df.duplicated(ID_cols).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will always get one duplicate lift entry because the Peak 2 Peak Gondola is returned twice in the API data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_cols = ['resortID', 'liftName', 'liftID']\n",
    "df = load_json_as_df(lifts_json_test_file, 'lifts')\n",
    "\n",
    "for combo in combinations(ID_cols, len(ID_cols)-1):\n",
    "    print(f\"Combo: {combo}\\tDuplicates: {df.duplicated(combo).sum()}\")\n",
    "\n",
    "print(f\"Combo: {ID_cols}\\tDuplicates: {df.duplicated(ID_cols).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new\n",
    "\n",
    "# TBD add test descriptions?\n",
    "\n",
    "\n",
    "class TestNotebook(unittest.TestCase):\n",
    "\n",
    "    topic_names = ['lifts', 'terrain', 'weather']\n",
    "\n",
    "    def test_get_data(self):\n",
    "        self.assertEqual(\n",
    "            list(get_data(filter_topic=['lifts']).keys()), ['lifts'],\n",
    "            'Returned dictionary was not filtered for the right topic'\n",
    "        )\n",
    "        self.assertEqual(\n",
    "            list(get_data(filter_topic=['lifts', 'terrain']).keys()),\n",
    "            ['lifts', 'terrain'],\n",
    "            'Returned dictionary was not filtered for the right topics'\n",
    "        )\n",
    "        self.assertEqual(\n",
    "            set(get_data().keys()), {'lifts', 'weather', 'terrain'},\n",
    "            'Returned dictionary was not filtered for all topics'\n",
    "        )\n",
    "\n",
    "    def test_get_data_changes(self):\n",
    "\n",
    "        # 'lifts' needs to be listed twice under 'topic' because there are two tests\n",
    "        # run on lift data.  The test using merged_whis_lifts_json_test_file has more than\n",
    "        # 2 timepoints in order to test the special code that is used to handle that case.\n",
    "        tests_df = pd.DataFrame({\n",
    "            'test_file': [merged_whis_lifts_json_test_file,\n",
    "                          merged_lifts_json_test_file,\n",
    "                          merged_terrain_json_test_file,\n",
    "                          merged_weather_json_test_file],\n",
    "            'topic': ['lifts', 'lifts', 'terrain', 'weather'],\n",
    "            'validation_fname_prefix': ['get_data_changes_merged_whis_lifts',\n",
    "                                        'get_data_changes_merged_lifts',\n",
    "                                        'get_data_changes_merged_terrain',\n",
    "                                        'get_data_changes_merged_weather']\n",
    "        })\n",
    "\n",
    "        for row in tests_df.iterrows():\n",
    "\n",
    "            test_file = row[1]['test_file']\n",
    "            df = load_json_as_df(test_file, row[1]['topic'])\n",
    "            df = df.sample(frac=1)  # Shuffle the data\n",
    "\n",
    "            for keep_oldest in [True, False]:\n",
    "                if keep_oldest == True:\n",
    "                    validation_fname_suffix = '_keep_oldest_valid.json'\n",
    "                else:\n",
    "                    validation_fname_suffix = '_drop_oldest_valid.json'\n",
    "\n",
    "                tested_df = get_data_changes(\n",
    "                    df, row[1]['topic'], keep_oldest=keep_oldest)\n",
    "\n",
    "                valid_file = TEST_VALIDATION_DATA_DIR + \\\n",
    "                    row[1]['validation_fname_prefix'] + validation_fname_suffix\n",
    "                valid_df = pd.read_pickle(valid_file)\n",
    "\n",
    "                #  Sort and reindex before comparison because we are not testing the indexes\n",
    "                # or row orders that the functions return.\n",
    "                tested_df.sort_values(\n",
    "                    tested_df.columns.to_list(), ignore_index=True, inplace=True)\n",
    "                valid_df.sort_values(\n",
    "                    valid_df.columns.to_list(), ignore_index=True, inplace=True)\n",
    "\n",
    "                pd.testing.assert_frame_equal(\n",
    "                    tested_df, valid_df,\n",
    "                    f\"Result from {test_file} did not match validation dataframe {valid_file}.\"\n",
    "                )\n",
    "\n",
    "    def test_ID_col_names(self):\n",
    "        \"\"\"Make sure that records can be uniquely identified by using ID columns for each topic\n",
    "        (in combination with timestamp)\"\"\"\n",
    "        files = [merged_lifts_json_test_file,\n",
    "                 merged_terrain_json_test_file, merged_weather_json_test_file]\n",
    "\n",
    "        for file, topic in zip(files, self.topic_names):\n",
    "\n",
    "            df = load_json_as_df(file, topic)\n",
    "\n",
    "            # Drop any rows that are complete duplicates. This is required for the Peak 2 Peak\n",
    "            # Gondola because it is duplicated in the lifts data.  Maybe others as well.\n",
    "            df.drop_duplicates(inplace=True)\n",
    "            \n",
    "            record_id_cols = include_timestamp_in_colnames(topic_ID_col_names[topic])\n",
    "            self.assertTrue(records_are_unique(df, record_id_cols),\n",
    "                            f\"{record_id_cols} are not sufficient to uniquely identify the {topic} records.\")\n",
    "\n",
    "    \n",
    "# To add:    \n",
    "# test records_are_unique() raises warning\n",
    "    \n",
    "    \n",
    "    # Assert df has no NaN or NaTs:\n",
    "    # assert parq_df.isnull().sum().sum() == 0\n",
    "\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD: test dtypes if not covered by type hint testing\n",
    "test_json = load_prior_json_from_s3('lifts')\n",
    "jsons_to_df(test_json, 'lifts')#.dtypes\n",
    "\n",
    "# TBD Make sure categories are complete... by sorting columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'terrain'\n",
    "\n",
    "# test that datatypes are correct if not covered by hint testing?\n",
    "parq_df = load_dataframe_from_parquet_on_s3(topic + HISTORY_SUFFIX)\n",
    "parq_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that the status categories are complete for lifts and terrain\n",
    "# maybe like this or direct access the index\n",
    "test_terrain = load_dataframe_from_parquet_on_s3(topic + HISTORY_SUFFIX)\n",
    "#test_terrain.status.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parq_df = set_df_datatypes(parq_df, topic)\n",
    "parq_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort by timestamp and ID columns\n",
    "record_id_cols = include_timestamp_in_colnames(topic_ID_col_names[topic])\n",
    "print(f\"sorted by {record_id_cols}\")\n",
    "parq_df.sort_values(record_id_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example code: Show data for whistler\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(parq_df.sort_values(sort).query('resortID == 13'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example code: Filter dataframe by date\n",
    "parq_df = parq_df[parq_df['timestamp'] > '2020-02-08']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a entities that has the same runName and resortID, but different terrainNames:\n",
    "parq_df.query('runName == \"Bear Paw\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can uniquely identify each run via a combination of: `resortID`, `runID` and `terrainName`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't need to include `runName`:\n",
    "assert(\n",
    "    parq_df.drop_duplicates(subset=['resortID', 'runID', 'terrainName', 'timestamp'], keep=False).equals(\n",
    "    parq_df.drop_duplicates(subset=['resortID', 'runID', 'runName', 'terrainName', 'timestamp'], keep=False))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD: testing\n",
    "- no duplicate rows\n",
    "- no duplicate information in adjacent rows by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load live whistler parquet data\n",
    "\n",
    "%time parq_df = load_dataframe_from_parquet_on_s3(HISTORY_FNAME)\n",
    "\n",
    "lifts_status_changes_parq_df = get_status_durations(parq_df)\n",
    "lifts_status_changes_parq_df.to_csv(DATA_DIR + \"lifts_status_changes_parq.csv\", date_format='%c')\n",
    "lifts_status_changes_parq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parq_df.status.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*parq_df.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parq_df.sort_values([\"liftName\", \"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue when running get_status_durations(parq_df)\n",
    "Resulting in error:\n",
    "\n",
    "    ~/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype)\n",
    "        705 \n",
    "        706         if len(codes) and (codes.max() >= len(dtype.categories) or codes.min() < -1):\n",
    "    --> 707             raise ValueError(\"codes need to be between -1 and \" \"len(categories)-1\")\n",
    "        708 \n",
    "        709         return cls(codes, dtype=dtype, fastpath=True)\n",
    "\n",
    "    ValueError: codes need to be between -1 and len(categories)-1\n",
    "\n",
    "\n",
    "Same error seen when running `parq_df[['status']].sort_values(by=['status'])`\n",
    "\n",
    "This was caused by missing categories (`H`) in the `status` column (and maybe others)\n",
    "\n",
    "#### Code to inspect issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for issue by sorting:\n",
    "# will raise `ValueError: codes need to be between -1 and len(categories)-1`\n",
    "parq_df[['status']].sort_values(by=['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parq_df.status.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the category codes present in the column\n",
    "print(*parq_df.status.cat.codes.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(parq_df.status.cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be false\n",
    "parq_df.status.cat.codes.max() >= len(parq_df.status.dtype.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be false\n",
    "parq_df.liftName.cat.codes.min() < -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in parq_df.columns:\n",
    "    print(parq_df[c].cat.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "parq_df[\"timestamp\"] = pd.to_datetime(pd.Series(np.asarray(parq_df[\"timestamp\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working on the issue for other categorical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "parq_fname = '../data/test/lifts_history_diff_category_vales.parquet'\n",
    "pf = ParquetFile(read_file, open_with=myopen)\n",
    "parq_df = pf.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error will be encountered when sorting by a column that has the issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorting for liftID...\n",
      "sorting for resortID...\n",
      "sorting for liftName...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "codes need to be between -1 and len(categories)-1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-173-15ebdb639a5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mparq_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"sorting for {c}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mparq_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m#print(parq_df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36msort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index)\u001b[0m\n\u001b[1;32m   4932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4933\u001b[0m         new_data = self._data.take(\n\u001b[0;32m-> 4934\u001b[0;31m             \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4935\u001b[0m         )\n\u001b[1;32m   4936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m         return self.reindex_indexer(\n\u001b[0;32m-> 1394\u001b[0;31m             \u001b[0mnew_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1395\u001b[0m         )\n\u001b[1;32m   1396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[1;32m   1265\u001b[0m                     ),\n\u001b[1;32m   1266\u001b[0m                 )\n\u001b[0;32m-> 1267\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m             ]\n\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1265\u001b[0m                     ),\n\u001b[1;32m   1266\u001b[0m                 )\n\u001b[0;32m-> 1267\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m             ]\n\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_tuple)\u001b[0m\n\u001b[1;32m   1833\u001b[0m         \u001b[0;31m# but are passed the axis depending on the calling routing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m         \u001b[0;31m# if its REALLY axis 0, then this will be a reindex and not a take\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1835\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m         \u001b[0;31m# Called from three places in managers, all of which satisfy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, allow_fill, fill_value)\u001b[0m\n\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m         \u001b[0mcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1856\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_codes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1857\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36mfrom_codes\u001b[0;34m(cls, codes, categories, ordered, dtype)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"codes need to be between -1 and len(categories)-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: codes need to be between -1 and len(categories)-1"
     ]
    }
   ],
   "source": [
    "for c in ['liftID', 'resortID', 'liftName', 'status', 'timeToRide', 'timestamp']:\n",
    "    parq_df = pf.to_pandas([c])\n",
    "    print(f\"sorting for {c}...\")\n",
    "    parq_df.sort_values(c)\n",
    "    #print(parq_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest category code in the dataframe column should be greater than or equal to the number of category codes used in the column. \n",
    "\n",
    "`pd.Categorical.from_codes()` expects codes as sequential numbers starting from 0 up to `len(categories) - 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The highest category code in the dataframe column\n",
    "parq_df.liftName.cat.codes.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of category codes used in the dataframe column\n",
    "len(parq_df.liftName.cat.codes.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of category values used in the dataframe column\n",
    "len(parq_df.liftName.dtype.categories.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that Dask, **fastparquet**, pyarrow, and **pandas** don't currently have a way to specify the categorical dtype of a column split across many files. Each file (parition) is treated independently. This results in categorials with unknown categories in the Pandas DataFrame. If we know that the categories are all the same, we're able to read in the first files categories and assign those to the entire DataFrame. But this is a bit fragile, as it relies on an assumption not necessarily guaranteed by the file structure.  If, for example, a new lifName is added, then the new partitions will contain a new category for the lift name but the old partitions will not.  https://tomaugspurger.github.io/sklearn-dask-tabular.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'liftID': 50, 'resortID': 15, 'liftName': 320, 'status': 3}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns marked as categorical in the extra metadata\n",
    "# (meaning the data must have come from pandas).\n",
    "pf.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- schema: \n",
      "| - liftID: INT64, OPTIONAL\n",
      "| - resortID: INT64, OPTIONAL\n",
      "| - liftName: BYTE_ARRAY, UTF8, OPTIONAL\n",
      "| - status: BYTE_ARRAY, UTF8, OPTIONAL\n",
      "| - timeToRide: BYTE_ARRAY, UTF8, OPTIONAL\n",
      "  - timestamp: INT64, TIMESTAMP_MICROS, OPTIONAL\n"
     ]
    }
   ],
   "source": [
    "print(pf.schema.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of categories in the ParquetFile\n",
    "import numpy as np\n",
    "pf_cats = np.unique(pf.grab_cats('liftName')['liftName'])\n",
    "len(pf_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of categories in the dataframe made from the ParquetFile\n",
    "df_cats = np.unique(parq_df.liftName.dtype.categories.values)\n",
    "len(df_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#12 Magic Carpet 1',\n",
       " '#8 Red Cliffs Tow',\n",
       " 'Golden Peak T-Bar #16',\n",
       " 'Quicksilver Gondola - North',\n",
       " 'Quicksilver Gondola - South',\n",
       " 'Train Rider Lift'}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which categories were only in the ParquetFile?\n",
    "set(pf_cats).difference(set(df_cats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#12 Magic Carpet 1 (Ski School)',\n",
       " 'Chairlift #10',\n",
       " 'Chairlift #6',\n",
       " 'Quicksilver Gondola',\n",
       " 'Trail Rider'}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which categories were only in the dataframe made from the ParquetFile?\n",
    "set(df_cats).difference(set(pf_cats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row group 1's categories are equal to row group 0's\n",
      "Row group 2's categories are equal to row group 1's\n",
      "Row group 3's categories are equal to row group 2's\n",
      "Row group 4's categories are equal to row group 3's\n",
      "Row group 5's categories are equal to row group 4's\n",
      "Row group 6's categories are equal to row group 5's\n",
      "Row group 7's categories are equal to row group 6's\n",
      "Row group 8's categories are equal to row group 7's\n",
      "Row group 9's categories are equal to row group 8's\n",
      "Row group 10's categories are equal to row group 9's\n",
      "Row group 11's categories are equal to row group 10's\n",
      "Row group 12's categories are equal to row group 11's\n",
      "\n",
      "Row group 13's categories are NOT equal to row group 12's\n",
      "Unique values in row group 13 that are not in row group 12:\n",
      "\t['#12 Magic Carpet 1 (Ski School)']\n",
      "\n",
      "\n",
      "Row group 14's categories are NOT equal to row group 13's\n",
      "Unique values in row group 14 that are not in row group 13:\n",
      "\t[]\n",
      "\n",
      "Row group 15's categories are equal to row group 14's\n",
      "Row group 16's categories are equal to row group 15's\n",
      "Row group 17's categories are equal to row group 16's\n",
      "Row group 18's categories are equal to row group 17's\n",
      "\n",
      "Row group 19's categories are NOT equal to row group 18's\n",
      "Unique values in row group 19 that are not in row group 18:\n",
      "\t['Quicksilver Gondola' 'Trail Rider']\n",
      "\n",
      "\n",
      "Row group 20's categories are NOT equal to row group 19's\n",
      "Unique values in row group 20 that are not in row group 19:\n",
      "\t[]\n",
      "\n",
      "\n",
      "Row group 21's categories are NOT equal to row group 20's\n",
      "Unique values in row group 21 that are not in row group 20:\n",
      "\t['Chairlift #10' 'Chairlift #6']\n",
      "\n",
      "Row group 22's categories are equal to row group 21's\n",
      "Row group 23's categories are equal to row group 22's\n",
      "Row group 24's categories are equal to row group 23's\n",
      "Row group 25's categories are equal to row group 24's\n",
      "Row group 26's categories are equal to row group 25's\n",
      "Row group 27's categories are equal to row group 26's\n",
      "Row group 28's categories are equal to row group 27's\n",
      "Row group 29's categories are equal to row group 28's\n",
      "Row group 30's categories are equal to row group 29's\n",
      "Row group 31's categories are equal to row group 30's\n",
      "Row group 32's categories are equal to row group 31's\n",
      "Row group 33's categories are equal to row group 32's\n",
      "Row group 34's categories are equal to row group 33's\n",
      "Row group 35's categories are equal to row group 34's\n"
     ]
    }
   ],
   "source": [
    "# Check to see if the same categories are used across all the ParquetFile partitions\n",
    "prev_cats = None\n",
    "\n",
    "for rg_idx in range(len(pf.row_groups) - 1):\n",
    "    cats = pf.grab_cats('liftName', rg_idx)['liftName']\n",
    "    if prev_cats is not None:\n",
    "        if np.array_equal(cats, prev_cats):\n",
    "            print(f\"Row group {rg_idx}'s categories are equal to row group {rg_idx-1}'s\")\n",
    "        else:\n",
    "            print(f\"\\nRow group {rg_idx}'s categories are NOT equal to row group {rg_idx-1}'s\")\n",
    "            print(f\"Unique values in row group {rg_idx} that are not in row group {rg_idx-1}:\")\n",
    "            print(f\"\\t{np.setdiff1d(cats, prev_cats)}\\n\")\n",
    "    prev_cats = cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things tried that crashed the kernel\n",
    "for i in parq_df['liftName'].items():\n",
    "    print(i)\n",
    "\n",
    "parq_df['liftName'].to_string()\n",
    "parq_df['liftName'].tolist()\n",
    "parq_df['liftName'].astype('str')\n",
    "set(parq_df.liftName.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.pre_allocate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Possible solutions\n",
    "1. Remove partitioning by date column when writing to parquet\n",
    "2. **Set status categories manually via `set_categories`. (and any other columns with the same issue.  See https://github.com/dask/dask/issues/2944**\n",
    "3. Leave problem columns as text-based when writing and loading from parquet\n",
    "4. Manually specify number of categories when converting to df.  E.g. `parq_df = pf.to_pandas(['liftName'], categories={'liftName': 319})` (didn't work)\n",
    "5. Catch exception and load column as string instead?\n",
    "6. repartition and reload?... probably can't be done via fastparquet because the same issue will be encountered when trying to save the loaded dataframe to a new parquet file.\n",
    "7. Process each row group to rebuild the list of category values from the values that appear in the rows?  Using ParquetFile.iter_row_groups() convert the col dtype to object for each row-group dataframe, then join them into one dataframe and convert the column dtype to categorical.\n",
    "8. Maintain a list of category values for each column.  Before adding any data to history parquet file, check to make sure that there are not any new category values for the column, and if there are, add them to the categorical dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing timestamps for file loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file = f\"s3://{BUCKET_NAME}/{fname}.parquet\"\n",
    "pf = ParquetFile(read_file, open_with=myopen)\n",
    "test = pf.to_pandas()[\"timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed: to convert for categorical datetime to regular datetime\n",
    "df[\"timestamp\"] = pd.to_datetime(pd.Series(np.asarray(df[\"timestamp\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.dt = test.dt.tz_convert(tz= 'America/Vancouver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parq_df.liftName.dtype.categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/Users/paul/anaconda3/lib/python3.7/site-packages/pandas/core/series.py:597: FutureWarning: Converting timezone-aware DatetimeArray to timezone-naive ndarray with 'datetime64[ns]' dtype. In the future, this will return an ndarray with 'object' dtype where each element is a 'pandas.Timestamp' with the correct 'tz'.\n",
    "\tTo accept the future behavior, pass 'dtype=object'.\n",
    "\tTo keep the old behavior, pass 'dtype=\"datetime64[ns]\"'.\n",
    "\n",
    "\n",
    "more info: https://pandas-docs.github.io/pandas-docs-travis/whatsnew/v0.24.0.html#converting-timezone-aware-series-and-index-to-numpy-arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "len(parq_df.liftName.cat.codes.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataframe_from_parquet_on_s3(fname).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing local parquet saves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parquet(df, fname):\n",
    "    # parquet engines don't handle shifted timezones\n",
    "    df.loc[:, 'timestamp'] = df.loc[:, 'timestamp'].dt.tz_convert(pytz.utc)\n",
    "\n",
    "    # Note: May need snappy-python as a req to run on AWS Lambda\n",
    "    df.to_parquet(DATA_DIR + fname + '.parquet',\n",
    "                  engine='fastparquet',\n",
    "                  partition_on=['timestamp'],\n",
    "                  file_scheme='mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_parquet(df[0:3].copy(), 'wb_lifts_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[20:22, :].copy().to_parquet(DATA_DIR + 'wb_lifts_history' + '.parquet',\n",
    "              engine='fastparquet',\n",
    "              partition_on=['timestamp'],\n",
    "              file_scheme='mixed',\n",
    "              append=True)\n",
    "# Catch exception that is doesn't exist here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: change time_diff to \"duration\"\n",
    "# test on lambda\n",
    "# make datatype dict for and general set datatypes function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prior_json_from_s3(topic: str) -> dict:\n",
    "    \"\"\"E.g. load_prior_json_from_s3('weather')\"\"\"\n",
    "    prior_object = bucket.Object(topic + PRIOR_SUFFIX)\n",
    "    prior = prior_object.get()['Body'].read().decode('utf-8')\n",
    "    return json.loads(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_difference(df1, df2, which=None):\n",
    "    \"\"\"Find rows which are different between two DataFrames.\"\"\"\n",
    "    comparison_df = df1.merge(df2,\n",
    "                              indicator=True,\n",
    "                              how='outer')\n",
    "    if which is None:\n",
    "        diff_df = comparison_df[comparison_df['_merge'] != 'both']\n",
    "    else:\n",
    "        diff_df = comparison_df[comparison_df['_merge'] == which]\n",
    "    return diff_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD: Add to scraping \n",
    "# check that weather_prior_json['weather'][0].keys() matches list of expected columns (in case new ones are in use)\n",
    "# Add as exception handling for the other topics as well\n",
    "load_prior_json_from_s3('weather')['weather'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a json from S3\n",
    "terrain_prior_json = load_prior_json_from_s3('terrain')\n",
    "\n",
    "# Convert json to a dataframe normally wer can use jsons_to_df()\n",
    "terrain_prior_df = pd.json_normalize(\n",
    "    data=terrain_prior_json,\n",
    "    record_path=['terrain'],\n",
    "    meta='timestamp'\n",
    ")\n",
    "\n",
    "terrain_prior_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_lifts_history():\n",
    "    bucket.Object(HISTORY_FNAME).delete()\n",
    "\n",
    "def del_lifts_prior():\n",
    "    bucket.Object(PRIOR_STATUS_FNAME).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_lifts_history()\n",
    "del_lifts_prior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 Object Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete prior jsons\n",
    "bucket.Object('lifts' + PRIOR_SUFFIX).delete()\n",
    "bucket.Object('terrain' + PRIOR_SUFFIX).delete()\n",
    "bucket.Object('weather' + PRIOR_SUFFIX).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete parquet history files\n",
    "bucket.objects.filter(Prefix='lifts' + HISTORY_SUFFIX + '/').delete()\n",
    "bucket.objects.filter(Prefix='terrain' + HISTORY_SUFFIX + '/').delete()\n",
    "bucket.objects.filter(Prefix='weather' + HISTORY_SUFFIX + '/').delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a parquet file from S3\n",
    "for f in get_matching_s3_keys(BUCKET_NAME, prefix='lifts_history_DEV'):\n",
    "    print(f)\n",
    "    s3.meta.client.download_file(BUCKET_NAME, f, '../data/test/' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parquet file from S3 into dataframe\n",
    "read_file = f\"s3://{BUCKET_NAME}/{'lifts' + HISTORY_SUFFIX}\"\n",
    "pf = ParquetFile(read_file, open_with=myopen)\n",
    "parq_df = pf.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- Terrain data: runs need to be identified via combination of `resortID`, `runName` and `runID`\n",
    "- There are run IDs that repeat for the same resort (e.g. for Vail resortID == 1, runID == 10\n",
    "- TBD: Are the combination of `resortID`, `runID`, and `runType` always unique?\n",
    "\n",
    "## To do\n",
    "- return data object with filter_by_resort() method\n",
    "- live_data class?  Subclass for each subject?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "255.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "437px",
    "left": "869px",
    "right": "20px",
    "top": "121px",
    "width": "391px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
