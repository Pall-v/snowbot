{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, glob, boto3, os\n",
    "import pdb\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out json processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://alexwlchan.net/2019/07/listing-s3-keys/\n",
    "def get_matching_s3_objects(bucket, prefix=\"\", suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Generate objects in an S3 bucket.\n",
    "\n",
    "    :param bucket: Name of the S3 bucket.\n",
    "    :param prefix: Only fetch objects whose key starts with\n",
    "        this prefix (optional).\n",
    "    :param suffix: Only fetch objects whose keys end with\n",
    "        this suffix (optional).\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    kwargs = {'Bucket': bucket}\n",
    "\n",
    "    # We can pass the prefix directly to the S3 API.  If the user has passed\n",
    "    # a tuple or list of prefixes, we go through them one by one.\n",
    "    if isinstance(prefix, str):\n",
    "        prefixes = (prefix, )\n",
    "    else:\n",
    "        prefixes = prefix\n",
    "\n",
    "    for key_prefix in prefixes:\n",
    "        kwargs[\"Prefix\"] = key_prefix\n",
    "\n",
    "        for page in paginator.paginate(**kwargs):\n",
    "            try:\n",
    "                contents = page[\"Contents\"]\n",
    "            except KeyError:\n",
    "                return\n",
    "\n",
    "            for obj in contents:\n",
    "                key = obj[\"Key\"]\n",
    "                if key.endswith(suffix):\n",
    "                    yield obj\n",
    "\n",
    "\n",
    "def get_matching_s3_keys(bucket, prefix=\"\", suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Generate the keys in an S3 bucket.\n",
    "\n",
    "    :param bucket: Name of the S3 bucket.\n",
    "    :param prefix: Only fetch keys that start with this prefix (optional).\n",
    "    :param suffix: Only fetch keys that end with this suffix (optional).\n",
    "    \"\"\"\n",
    "    for obj in get_matching_s3_objects(bucket, prefix, suffix):\n",
    "        yield obj[\"Key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "BUCKET_NAME = 'snowbot-pv'\n",
    "\n",
    "# S3 Connect\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "bucket = s3.Bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\"\n",
    "MERGED_FILENAME = \"merged_file.json\"\n",
    "merged_file = DATA_DIR + MERGED_FILENAME\n",
    "\n",
    "TEST_FILENAME = \"test_file.json\"\n",
    "merged_test_file = DATA_DIR + TEST_FILENAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_matching_jsons(save_file, suffix=\"\"):\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    for f in get_matching_s3_keys(BUCKET_NAME, suffix=suffix):\n",
    "\n",
    "        # Write the file from S3 into a local temp file\n",
    "        with open('temp', 'wb') as tfw:\n",
    "            bucket.download_fileobj(f, tfw)\n",
    "\n",
    "        # Append the local temp file into the result list\n",
    "        with open('temp', 'rb') as tfr:          \n",
    "            result.append(json.load(tfr))\n",
    "\n",
    "    os.remove(\"temp\")\n",
    "\n",
    "    # Fill the output file with the merged content\n",
    "    with open(save_file, \"w\") as outfile:\n",
    "         json.dump(result, outfile)\n",
    "            \n",
    "# TBD: more efficient to go straight to df w/o saving json to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_merged_json_as_df(merged_file):\n",
    "    #load the merged json as a dataframe\n",
    "    with open(merged_file, \"r\") as f:\n",
    "        d = json.load(f)\n",
    "        df = pd.DataFrame.from_dict(json_normalize(d, record_path='lifts', meta='timestamp'))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_lifts_df_datatypes(df):\n",
    "\n",
    "    # set datatypes for lift table\n",
    "    df = df.astype({\n",
    "        \"liftID\": 'category',\n",
    "        \"resortID\": 'category',\n",
    "        \"liftName\": 'category',\n",
    "        \"status\": 'category',\n",
    "        \"timeToRide\": \"int\"\n",
    "    })\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_status_changes(df):\n",
    "    '''Returns a dataframe that only includes the times when there was a change to a new status'''\n",
    "    \n",
    "    def calc_status_change(df):\n",
    "        firstrow = df.loc[df['timestamp'].idxmin()]\n",
    "        change_rows = df[df.status.ne(df.status.shift())]\n",
    "        keep_df = firstrow.to_frame().T.append(change_rows)\n",
    "        \n",
    "        # Remove so that we don't need to write another column to S3 as we scrape?\n",
    "        # Just calculate it when plotting and predicting?\n",
    "        # keep_df['time_diff'] = keep_df['timestamp'].diff(1).shift(-1)\n",
    "        \n",
    "        return keep_df\n",
    "        \n",
    "    \n",
    "    df = df.groupby('liftName', group_keys=False)\\\n",
    "           .apply(calc_status_change)\\\n",
    "           .reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# TBD: may need to convert timestamp to days (e.g. for Tableau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process lift json fies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_matching_jsons(suffix=\"lifts.json\", save_file=merged_file)\n",
    "lifts_df = load_merged_json_as_df(merged_file)\n",
    "lifts_df = set_lifts_df_datatypes(lifts_df)\n",
    "\n",
    "lifts_status_changes_df = get_status_changes(lifts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** `timeToRide` is just the time is takes to ride the lift, not the current wait time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "liftName\n",
       "7th Heaven Express                 [6]\n",
       "Big Red Express                    [8]\n",
       "Blackcomb Gondola Lower            [7]\n",
       "Blackcomb Gondola Upper            [7]\n",
       "Catskinner Express                 [4]\n",
       "Coca-Cola Tube Park                [4]\n",
       "Creekside Gondola                  [7]\n",
       "Crystal Ridge Express              [7]\n",
       "Emerald 6 Express                  [6]\n",
       "Excalibur Gondola Lower            [3]\n",
       "Excalibur Gondola Upper            [5]\n",
       "Excelerator Express                [6]\n",
       "Fitzsimmons Express                [6]\n",
       "Franz's Chair                      [8]\n",
       "Garbanzo Express                   [7]\n",
       "Glacier Express                    [6]\n",
       "Harmony 6 Express                  [6]\n",
       "Horstman T-Bar                     [4]\n",
       "Jersey Cream Express               [5]\n",
       "Magic Chair                        [6]\n",
       "Olympic Chair                      [5]\n",
       "Peak 2 Peak Gondola               [12]\n",
       "Peak Express                       [3]\n",
       "Showcase T-Bar                     [3]\n",
       "Symphony Express                   [7]\n",
       "T-Bars                             [5]\n",
       "Whistler Village Gondola Lower     [5]\n",
       "Whistler Village Gondola Upper    [11]\n",
       "Name: timeToRide, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifts_df.groupby(\"liftName\")['timeToRide'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get status durations\n",
    "# TBD: optimize if needed via # 3 under:\n",
    "# https://towardsdatascience.com/pandas-tips-and-tricks-33bcc8a40bb9\n",
    "df = lifts_status_changes_df.sort_values(by=['liftID', 'timestamp'])\n",
    "df['time_diff'] = df.groupby('liftID')['timestamp'].diff(1).shift(-1)\n",
    "\n",
    "# Fill in the durations which will be missing for the most recent status changes\n",
    "missing_time_diffs_idx = df.loc[(df['time_diff'].isnull()) & (\n",
    "    df['timestamp'] >= df['timestamp'].min()), 'timestamp'].index.values\n",
    "\n",
    "df.loc[missing_time_diffs_idx, 'time_diff'] = df['timestamp'].max(\n",
    ") - df.loc[missing_time_diffs_idx, 'timestamp']\n",
    "\n",
    "# Convert to seconds\n",
    "df['time_diff_seconds'] = df['time_diff'].dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>liftID</th>\n",
       "      <th>resortID</th>\n",
       "      <th>liftName</th>\n",
       "      <th>status</th>\n",
       "      <th>timeToRide</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time_diff</th>\n",
       "      <th>time_diff_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-03 00:19:09.631011-08:00</td>\n",
       "      <td>1 days 13:59:59.794440</td>\n",
       "      <td>136799.794440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-04 14:19:09.425451-08:00</td>\n",
       "      <td>0 days 00:29:59.297179</td>\n",
       "      <td>1799.297179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-04 14:49:08.722630-08:00</td>\n",
       "      <td>0 days 22:00:01.500183</td>\n",
       "      <td>79201.500183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-05 12:49:10.222813-08:00</td>\n",
       "      <td>0 days 01:59:58.326201</td>\n",
       "      <td>7198.326201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-05 14:49:08.549014-08:00</td>\n",
       "      <td>0 days 18:56:18.469982</td>\n",
       "      <td>68178.469982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Upper</td>\n",
       "      <td>H</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-01-29 08:00:28.210238-08:00</td>\n",
       "      <td>0 days 00:29:58.285712</td>\n",
       "      <td>1798.285712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Upper</td>\n",
       "      <td>O</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-01-29 08:30:26.495950-08:00</td>\n",
       "      <td>0 days 07:14:59.875339</td>\n",
       "      <td>26099.875339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Upper</td>\n",
       "      <td>X</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-01-29 15:45:26.371289-08:00</td>\n",
       "      <td>0 days 15:59:59.554913</td>\n",
       "      <td>57599.554913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Upper</td>\n",
       "      <td>H</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-01-30 07:45:25.926202-08:00</td>\n",
       "      <td>0 days 00:45:00.787765</td>\n",
       "      <td>2700.787765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Upper</td>\n",
       "      <td>O</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-01-30 08:30:26.713967-08:00</td>\n",
       "      <td>0 days 06:29:59.768409</td>\n",
       "      <td>23399.768409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1832 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     liftID resortID                        liftName status timeToRide  \\\n",
       "0         3       13              7th Heaven Express      X          6   \n",
       "1         3       13              7th Heaven Express      O          6   \n",
       "2         3       13              7th Heaven Express      X          6   \n",
       "3         3       13              7th Heaven Express      O          6   \n",
       "4         3       13              7th Heaven Express      X          6   \n",
       "...     ...      ...                             ...    ...        ...   \n",
       "1827     72       13  Whistler Village Gondola Upper      H         11   \n",
       "1828     72       13  Whistler Village Gondola Upper      O         11   \n",
       "1829     72       13  Whistler Village Gondola Upper      X         11   \n",
       "1830     72       13  Whistler Village Gondola Upper      H         11   \n",
       "1831     72       13  Whistler Village Gondola Upper      O         11   \n",
       "\n",
       "                            timestamp              time_diff  \\\n",
       "0    2020-01-03 00:19:09.631011-08:00 1 days 13:59:59.794440   \n",
       "1    2020-01-04 14:19:09.425451-08:00 0 days 00:29:59.297179   \n",
       "2    2020-01-04 14:49:08.722630-08:00 0 days 22:00:01.500183   \n",
       "3    2020-01-05 12:49:10.222813-08:00 0 days 01:59:58.326201   \n",
       "4    2020-01-05 14:49:08.549014-08:00 0 days 18:56:18.469982   \n",
       "...                               ...                    ...   \n",
       "1827 2020-01-29 08:00:28.210238-08:00 0 days 00:29:58.285712   \n",
       "1828 2020-01-29 08:30:26.495950-08:00 0 days 07:14:59.875339   \n",
       "1829 2020-01-29 15:45:26.371289-08:00 0 days 15:59:59.554913   \n",
       "1830 2020-01-30 07:45:25.926202-08:00 0 days 00:45:00.787765   \n",
       "1831 2020-01-30 08:30:26.713967-08:00 0 days 06:29:59.768409   \n",
       "\n",
       "      time_diff_seconds  \n",
       "0         136799.794440  \n",
       "1           1799.297179  \n",
       "2          79201.500183  \n",
       "3           7198.326201  \n",
       "4          68178.469982  \n",
       "...                 ...  \n",
       "1827        1798.285712  \n",
       "1828       26099.875339  \n",
       "1829       57599.554913  \n",
       "1830        2700.787765  \n",
       "1831       23399.768409  \n",
       "\n",
       "[1832 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "liftID                                               object\n",
       "resortID                                             object\n",
       "liftName                                             object\n",
       "status                                               object\n",
       "timeToRide                                           object\n",
       "timestamp            datetime64[ns, pytz.FixedOffset(-480)]\n",
       "time_diff                                   timedelta64[ns]\n",
       "time_diff_seconds                                   float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses locale date formatting, otherwise Tableau will mix up month and day\n",
    "# alternatively, can export to json:\n",
    "# lifts_status_changes_df.to_json(DATA_DIR + \"lifts_status_changes.json\", orient='table')\n",
    "df.to_csv(DATA_DIR + \"lifts_status_changes.csv\", date_format='%c')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add:\n",
    "# \n",
    "# daily: for each chair calculate most open status of the day: O > H > X\n",
    "# Days since each chair was last seen open with timestamp of most recent open time.\n",
    "# snowfall since last open\n",
    "# save data for other mountains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage options testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(DATA_DIR + \"df_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastparquet import write\n",
    "\n",
    "# parquet engines don't handle shifted timezones\n",
    "import pytz\n",
    "TZ = pytz.timezone('America/Vancouver')\n",
    "df['timestamp'] = df.timestamp.dt.tz_convert(pytz.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: May need snappy-python as a req to run on AWS Lambda\n",
    "df.to_parquet(DATA_DIR + \"df_test.parquet\", engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_df = pd.read_parquet(DATA_DIR + \"df_test.parquet\")\n",
    "load_df['timestamp'] = load_df.timestamp.dt.tz_convert(TZ) # convert back to correct timezone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TBD convert back to correct datatypes\n",
    "load_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(DATA_DIR + \"df_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test file size results:\n",
    "- json: 800 Kb?\n",
    "- csv: 474 Kb\n",
    "- pickle: 145 Kb\n",
    "- parquet: 15 Kb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Lake Testing\n",
    "\n",
    "Requires apache spark instance.  For future use, could set one up to work with lambda using https://aws.amazon.com/emr/features/spark/?\n",
    "\n",
    "Otherwise databricks (similar to QxMD project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# json comparison and parquet to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastparquet import write, ParquetFile\n",
    "import os\n",
    "import pytz\n",
    "import s3fs\n",
    "import botocore\n",
    "\n",
    "os.chdir(\"../src/data/snowbot_AWS_lambda/\")\n",
    "from scrape import get_data\n",
    "os.chdir(\"..\")\n",
    "\n",
    "# parquet engines don't handle shifted timezones\n",
    "\n",
    "TZ = pytz.timezone('America/Vancouver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new version\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "myopen = fs.open\n",
    "nop = lambda *args, **kwargs: None\n",
    "\n",
    "\n",
    "fname = 'wb_lifts_history'\n",
    "\n",
    "\n",
    "def write_dataframe_to_parquet_on_s3(df, fname):\n",
    "    \"\"\" Write a dataframe to a Parquet on S3.  Creates a new parquet file if one doesn't already exist.\"\"\"\n",
    "    \n",
    "    # Unshift the timezone because parquet engines don't handle shifted timezones\n",
    "    df.loc[:, 'timestamp'] = df.loc[:, 'timestamp'].dt.tz_convert(pytz.utc)\n",
    "\n",
    "    s3_object = s3.Object(BUCKET_NAME, fname + \".parquet\")\n",
    "\n",
    "    def write_parquet(df, fname, app=True):\n",
    "\n",
    "        output_file = f\"s3://{BUCKET_NAME}/{fname}.parquet\"\n",
    "        write(output_file,\n",
    "              df,\n",
    "              partition_on=['timestamp'],\n",
    "              file_scheme='hive',\n",
    "              append=app,  # need to remove or catch exception to work when file doesn't exist\n",
    "              open_with=myopen,\n",
    "              mkdirs=nop)\n",
    "        print(\"Writing {} records to {}\".format(len(df), fname))\n",
    "\n",
    "    if not list(bucket.objects.filter(Prefix=fname)):\n",
    "        print(\"File {} not found.  Creating file.\".format(fname))\n",
    "        write_parquet(df, fname, app=False)\n",
    "\n",
    "    else:\n",
    "        print(\"file found.  adding new stuff\")\n",
    "        write_parquet(df, fname, app=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsons_to_df(jsons):\n",
    "    # repeats. functionize first occurace?\n",
    "    return pd.DataFrame.from_dict(json_normalize(jsons, record_path='lifts', meta='timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded prior json data from S3\n",
      "Found differences\n",
      "file found.  adding new stuff\n",
      "Writing 43 records to wb_lifts_history\n",
      "Updated lifts_prior.json with current data.\n"
     ]
    }
   ],
   "source": [
    "# Get current lift status info json\n",
    "lifts_current = get_data()['lifts'] # String.  \n",
    "lifts_current_json = json.loads(lifts_current)\n",
    "\n",
    "# retrieve prior lift status info json\n",
    "\n",
    "def save_prior(json_data):\n",
    "    bucket.put_object(Key=\"lifts_prior.json\",\n",
    "                      Body=bytes(json.dumps(json_data).encode('UTF-8')))\n",
    "\n",
    "\n",
    "lifts_prior_object = s3.Object(BUCKET_NAME, 'lifts_prior.json')\n",
    "\n",
    "try:\n",
    "    lifts_prior_object.load()\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == \"404\":\n",
    "        print(\"Prior doesn't exist\")\n",
    "        save_prior(lifts_current_json)  # Create the prior file\n",
    "        print(\"Created PRIOR_FILENAME_TBD\")\n",
    "    else:\n",
    "        # Something else has gone wrong.\n",
    "        raise\n",
    "else:\n",
    "    # The prior exists\n",
    "    file_content = lifts_prior_object.get()['Body'].read().decode('utf-8')\n",
    "    json_content = json.loads(file_content)\n",
    "    print(\"loaded prior json data from S3\")\n",
    "\n",
    "    # compare jsons without their timestamps\n",
    "    if json_content['lifts'] == lifts_current_json['lifts']:\n",
    "        print(\"No differences between current and prior data were found.\")\n",
    "    else:\n",
    "        # Get a df with the status chages between the prior and current json data\n",
    "        df = jsons_to_df([json_content, lifts_current_json])\n",
    "        df = set_lifts_df_datatypes(df)  # TBD add to jsons_to_df\n",
    "        df = get_status_changes(df)\n",
    "        df = set_lifts_df_datatypes(df)  # TBD add to jsons_to_df\n",
    "        print(\"Found differences\")\n",
    "        write_dataframe_to_parquet_on_s3(df, fname)\n",
    "\n",
    "        # save current lift status info json as prior\n",
    "        save_prior(lifts_current_json)\n",
    "        print(\"Updated {} with current data.\".format(lifts_prior_object.key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>liftID</th>\n",
       "      <th>resortID</th>\n",
       "      <th>liftName</th>\n",
       "      <th>status</th>\n",
       "      <th>timeToRide</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7th Heaven Express</td>\n",
       "      <td>H</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-13 13:21:56.629410-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "      <td>13</td>\n",
       "      <td>Big Red Express</td>\n",
       "      <td>H</td>\n",
       "      <td>8</td>\n",
       "      <td>2020-01-13 13:21:56.629410-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "      <td>Blackcomb Gondola Lower</td>\n",
       "      <td>H</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-01-13 13:21:56.629410-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>13</td>\n",
       "      <td>Blackcomb Gondola Upper</td>\n",
       "      <td>H</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-01-13 13:21:56.629410-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>Catskinner Express</td>\n",
       "      <td>H</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-01-13 13:21:56.629410-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>40</td>\n",
       "      <td>13</td>\n",
       "      <td>Garbanzo Express</td>\n",
       "      <td>O</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-01-30 15:31:26.798102-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>Magic Chair</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-30 15:31:26.798102-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "      <td>Olympic Chair</td>\n",
       "      <td>O</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-01-30 15:31:26.798102-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>33</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Lower</td>\n",
       "      <td>O</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-01-30 15:31:26.798102-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>Whistler Village Gondola Upper</td>\n",
       "      <td>O</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-01-30 15:31:26.798102-08:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    liftID resortID                        liftName status  timeToRide  \\\n",
       "0        3       13              7th Heaven Express      H           6   \n",
       "1       36       13                 Big Red Express      H           8   \n",
       "2       69       13         Blackcomb Gondola Lower      H           7   \n",
       "3       70       13         Blackcomb Gondola Upper      H           7   \n",
       "4        9       13              Catskinner Express      H           4   \n",
       "..     ...      ...                             ...    ...         ...   \n",
       "173     40       13                Garbanzo Express      O           7   \n",
       "174      6       13                     Magic Chair      O           6   \n",
       "175     39       13                   Olympic Chair      O           5   \n",
       "176     33       13  Whistler Village Gondola Lower      O           5   \n",
       "177     72       13  Whistler Village Gondola Upper      O          11   \n",
       "\n",
       "                           timestamp  \n",
       "0   2020-01-13 13:21:56.629410-08:00  \n",
       "1   2020-01-13 13:21:56.629410-08:00  \n",
       "2   2020-01-13 13:21:56.629410-08:00  \n",
       "3   2020-01-13 13:21:56.629410-08:00  \n",
       "4   2020-01-13 13:21:56.629410-08:00  \n",
       "..                               ...  \n",
       "173 2020-01-30 15:31:26.798102-08:00  \n",
       "174 2020-01-30 15:31:26.798102-08:00  \n",
       "175 2020-01-30 15:31:26.798102-08:00  \n",
       "176 2020-01-30 15:31:26.798102-08:00  \n",
       "177 2020-01-30 15:31:26.798102-08:00  \n",
       "\n",
       "[178 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the parquet file\n",
    "\n",
    "\n",
    "def load_dataframe_from_parquet_on_s3(fname):\n",
    "    \"\"\" Load a dataframe from a Parquet on S3. \"\"\"\n",
    "    read_file = f\"s3://{BUCKET_NAME}/{fname}.parquet\"\n",
    "    pf = ParquetFile(read_file, open_with=myopen)\n",
    "    df = pf.to_pandas()\n",
    "\n",
    "    # Reshift the timezone because parquet engines don't handle shifted timezones\n",
    "    df['timestamp'].cat.set_categories(df['timestamp'].cat.categories.tz_convert(TZ), inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "load_dataframe_from_parquet_on_s3(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing timestamps for file loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file = f\"s3://{BUCKET_NAME}/{fname}.parquet\"\n",
    "pf = ParquetFile(read_file, open_with=myopen)\n",
    "test = pf.to_pandas()[\"timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed: to convert for categorical datetime to regular datetime\n",
    "df[\"timestamp\"] = pd.to_datetime(pd.Series(np.asarray(df[\"timestamp\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.dt = test.dt.tz_convert(tz= 'America/Vancouver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/Users/paul/anaconda3/lib/python3.7/site-packages/pandas/core/series.py:597: FutureWarning: Converting timezone-aware DatetimeArray to timezone-naive ndarray with 'datetime64[ns]' dtype. In the future, this will return an ndarray with 'object' dtype where each element is a 'pandas.Timestamp' with the correct 'tz'.\n",
    "\tTo accept the future behavior, pass 'dtype=object'.\n",
    "\tTo keep the old behavior, pass 'dtype=\"datetime64[ns]\"'.\n",
    "\n",
    "\n",
    "more info: https://pandas-docs.github.io/pandas-docs-travis/whatsnew/v0.24.0.html#converting-timezone-aware-series-and-index-to-numpy-arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataframe_from_parquet_on_s3(fname).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing local parquet saves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parquet(df, fname):\n",
    "    # parquet engines don't handle shifted timezones\n",
    "    df.loc[:, 'timestamp'] = df.loc[:, 'timestamp'].dt.tz_convert(pytz.utc)\n",
    "\n",
    "    # Note: May need snappy-python as a req to run on AWS Lambda\n",
    "    df.to_parquet(DATA_DIR + fname + '.parquet',\n",
    "                  engine='fastparquet',\n",
    "                  partition_on=['timestamp'],\n",
    "                  file_scheme='mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_parquet(df[0:3].copy(), 'wb_lifts_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[20:22, :].copy().to_parquet(DATA_DIR + 'wb_lifts_history' + '.parquet',\n",
    "              engine='fastparquet',\n",
    "              partition_on=['timestamp'],\n",
    "              file_scheme='mixed',\n",
    "              append=True)\n",
    "# Catch exception that is doesn't exist here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: change time_diff to \"duration\"\n",
    "# test on lambda\n",
    "# make datatype dict for and general set datatypes function\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "437px",
    "left": "869px",
    "right": "20px",
    "top": "120px",
    "width": "391px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
