{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, glob, boto3, os\n",
    "import pdb\n",
    "import pandas as pd\n",
    "from fastparquet import write, ParquetFile\n",
    "import pytz\n",
    "import s3fs\n",
    "import botocore\n",
    "from collections import Iterable\n",
    "from typing import List, Union\n",
    "from copy import deepcopy\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "BUCKET_NAME = 'snowbot-pv'\n",
    "\n",
    "# S3 Connect\n",
    "s3 = session.resource('s3')\n",
    "bucket = s3.Bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet engines don't handle shifted timezones\n",
    "TZ = pytz.timezone('America/Vancouver')\n",
    "\n",
    "DATA_DIR = \"../data/\"\n",
    "MERGED_JSON_FILENAME = \"merged_file.json\"\n",
    "merged_json_file = DATA_DIR + MERGED_JSON_FILENAME\n",
    "\n",
    "# Used for weather data in jsons_to_df()\n",
    "weather_meta_fields = [\n",
    "    'newSnow', 'last24Hours', 'last48Hours', 'last7Days', 'midMountainBase',\n",
    "    'resortID'\n",
    "]\n",
    "weather_record_path = ['weather', 'weatherForecast']\n",
    "weather_meta = [['weather', i] for i in weather_meta_fields]\n",
    "weather_meta.append('timestamp')\n",
    "\n",
    "# Used for lift and terrain status in jsons_to_df()\n",
    "# Important to set categories because when writing incrementally to parquet, some increments\n",
    "# may not include all statuses.  Manually setting the categories avoids errors due to\n",
    "# different catergory indexing between increments.\n",
    "status_cat_dtype = pd.api.types.CategoricalDtype(categories=['X', 'H', 'O'],\n",
    "                                                 ordered=True)\n",
    "groomed_cat_dtype = pd.api.types.CategoricalDtype(categories=['No', 'Yes'],\n",
    "                                                  ordered=True)\n",
    "\n",
    "# Column dtypes that are to be set for each dataframe\n",
    "df_dtypes = {\n",
    "    \"lifts\": {\n",
    "        'liftID': 'category',\n",
    "        'resortID': 'category',\n",
    "        'liftName': 'category',\n",
    "        'status': status_cat_dtype,\n",
    "        'timeToRide': 'object'\n",
    "    },\n",
    "    'terrain': {\n",
    "        'runID': 'category',\n",
    "        'resortID': 'category',\n",
    "        'groomed': groomed_cat_dtype,\n",
    "        'runName': 'category',\n",
    "        'runType': 'category',\n",
    "        'status': status_cat_dtype,\n",
    "        'terrainName': 'category'\n",
    "    },\n",
    "    'weather': {\n",
    "        'resortID': 'category',\n",
    "        'forecast.dayDescription': 'object',\n",
    "        'forecast.daycode': 'category',\n",
    "        'forecast.forecastString': 'object',\n",
    "        'forecast.iconName': 'object',\n",
    "        'forecast.summaryDescription': 'object',\n",
    "        'forecast.temperatureHigh': 'object',\n",
    "        'forecast.temperatureLow': 'object',\n",
    "        'weather.last24Hours': 'object',\n",
    "        'weather.last48Hours': 'object',\n",
    "        'weather.last7Days': 'object',\n",
    "        'weather.midMountainBase': 'object',\n",
    "        'weather.newSnow': 'object'\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def flatten(items):\n",
    "    \"\"\"Yield items from any nested iterable\"\"\"\n",
    "    for x in items:\n",
    "        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n",
    "            for sub_x in flatten(x):\n",
    "                yield sub_x\n",
    "        else:\n",
    "            yield x\n",
    "\n",
    "\n",
    "# The columns that serve to identify records for each topic\n",
    "topic_ID_col_names = {\n",
    "    'lifts': ['resortID', 'liftName'],\n",
    "    'terrain': ['resortID', 'runID', 'terrainName'],\n",
    "    'weather': 'resortID',\n",
    "    'all_topics': 'timestamp'\n",
    "}\n",
    "# All of the column names that serve to identify records in at least one of the topics\n",
    "all_ID_col_names = set(flatten(topic_ID_col_names.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://alexwlchan.net/2019/07/listing-s3-keys/\n",
    "def get_matching_s3_objects(bucket, prefix=\"\", suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Generate objects in an S3 bucket.\n",
    "\n",
    "    :param bucket: Name of the S3 bucket.\n",
    "    :param prefix: Only fetch objects whose key starts with\n",
    "        this prefix (optional).\n",
    "    :param suffix: Only fetch objects whose keys end with\n",
    "        this suffix (optional).\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    kwargs = {'Bucket': bucket}\n",
    "\n",
    "    # We can pass the prefix directly to the S3 API.  If the user has passed\n",
    "    # a tuple or list of prefixes, we go through them one by one.\n",
    "    if isinstance(prefix, str):\n",
    "        prefixes = (prefix, )\n",
    "    else:\n",
    "        prefixes = prefix\n",
    "\n",
    "    for key_prefix in prefixes:\n",
    "        kwargs[\"Prefix\"] = key_prefix\n",
    "\n",
    "        for page in paginator.paginate(**kwargs):\n",
    "            try:\n",
    "                contents = page[\"Contents\"]\n",
    "            except KeyError:\n",
    "                return\n",
    "\n",
    "            for obj in contents:\n",
    "                key = obj[\"Key\"]\n",
    "                if key.endswith(suffix):\n",
    "                    yield obj\n",
    "\n",
    "\n",
    "def get_matching_s3_keys(bucket, prefix=\"\", suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Generate the keys in an S3 bucket.\n",
    "\n",
    "    :param bucket: Name of the S3 bucket.\n",
    "    :param prefix: Only fetch keys that start with this prefix (optional).\n",
    "    :param suffix: Only fetch keys that end with this suffix (optional).\n",
    "    \"\"\"\n",
    "    for obj in get_matching_s3_objects(bucket, prefix, suffix):\n",
    "        yield obj[\"Key\"]\n",
    "\n",
    "\n",
    "def merge_matching_jsons_on_s3(save_file, prefix=\"\", suffix=\"\"):\n",
    "    \"\"\"Merges json files on S3 that match the suffix into a new json and save it\n",
    "    as the save_file on S3.\"\"\"\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for f in get_matching_s3_keys(BUCKET_NAME, prefix=prefix, suffix=suffix):\n",
    "\n",
    "        # Write the file from S3 into a local temp file\n",
    "        with open('temp', 'wb') as tfw:\n",
    "            bucket.download_fileobj(f, tfw)\n",
    "\n",
    "        # Append the local temp file into the result list\n",
    "        with open('temp', 'rb') as tfr:\n",
    "            result.append(json.load(tfr))\n",
    "\n",
    "    os.remove(\"temp\")\n",
    "\n",
    "    # Fill the output file with the merged content\n",
    "    with open(save_file, \"w\") as outfile:\n",
    "        json.dump(result, outfile)\n",
    "\n",
    "# TBD: more efficient to go straight to df w/o saving json to file\n",
    "\n",
    "\n",
    "def set_df_datatypes(df, topic):\n",
    "    \"\"\"Set the datatypes for a df according to the topic that\n",
    "    it represents.\"\"\"\n",
    "    df = df.astype(df_dtypes[topic])\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def jsons_to_df(jsons, record_path, meta='timestamp'):\n",
    "    \"\"\"Convert a json containing one or more timestamps to a dataframe.\"\"\"\n",
    "    if record_path == 'weather':\n",
    "        # Deal with the nested object that the weather data uses to store the weather forecast\n",
    "        df = pd.json_normalize(jsons, record_path=weather_record_path,\n",
    "                               meta=weather_meta, record_prefix='forecast.')\n",
    "        df.rename(columns={\"weather.resortID\": \"resortID\"}, inplace=True)\n",
    "    else:\n",
    "        df = pd.json_normalize(jsons, record_path=record_path,\n",
    "                               meta=meta)\n",
    "\n",
    "    df = set_df_datatypes(df, record_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_json_as_df(merged_json_file, record_path):\n",
    "    \"\"\"Load json file containing one or more timestamps as a dataframe.\"\"\"\n",
    "    with open(merged_json_file, \"r\") as f:\n",
    "        d = json.load(f)\n",
    "        df = jsons_to_df(d, record_path)\n",
    "        return df\n",
    "\n",
    "\n",
    "def get_data_changes(df, topic, keep_oldest=False):\n",
    "    \"\"\"\n",
    "    Filter out rows that do not represent changed data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Includes 'timestamp' identifying and data columns.  Lists data for each timestamp.\n",
    "    keep_oldest : boolean\n",
    "        Indicates if the returned DataFrame should keep the oldest record for each entity (i.e.\n",
    "        lift, resort, tor terrain) even if an entity has no data changes.  This is so that the\n",
    "        earliest data for each entity is not lost, and all entities are listed the returned DataFrame\n",
    "        even if their data has not changed.  Use `False` when there is just one DataFrame to process.\n",
    "        Use `True` is cases where the data changes will be appended to an existing dataframe that\n",
    "        already has at least one row for each entity.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Only includes the rows from the original dataframe where there was a change to new values\n",
    "        in the data columns.\n",
    "    \"\"\"\n",
    "    ID_columns = topic_ID_col_names[topic]\n",
    "    data_columns = [c for c in df.columns if c not in all_ID_col_names]\n",
    "\n",
    "    def filter_for_data_changes(df, keep_oldest=keep_oldest):\n",
    "        \"\"\"Filter out rows where data is unchanged for adjacent timestamps.\n",
    "        Required to handle cases when there are > 2 rows per entity.\n",
    "        \"\"\"\n",
    "        keep_idx = df[data_columns].ne(df[data_columns].shift()).any(\n",
    "            axis=1).values[1:]  # True for rows with data changes\n",
    "        changed_rows = df.reset_index(drop=True).drop(index=0)[keep_idx]\n",
    "\n",
    "        if keep_oldest:\n",
    "            firstrow = df.loc[df['timestamp'].idxmin()]\n",
    "            keep_df = firstrow.to_frame().T.append(changed_rows)\n",
    "        else:\n",
    "            keep_df = changed_rows\n",
    "\n",
    "        return keep_df\n",
    "\n",
    "    # Drop any rows that are complete duplicates so that conditional evaluation will\n",
    "    # work.  This is required for Peak 2 Peak Gondola because it is duplicated in the\n",
    "    # lifts data.  Maybe others as well.\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # 1 means that there were up to 2 rows fond per group\n",
    "    if df.groupby(ID_columns, group_keys=False).cumcount().max() < 2:\n",
    "        # Most efficient method.  Only works if there are 2 or less rows per entity.\n",
    "        subset = df.columns.drop('timestamp')\n",
    "        df = df.sort_values('timestamp')\n",
    "\n",
    "        if keep_oldest:\n",
    "            df = df.drop_duplicates(subset=subset, keep='first')\n",
    "        else:\n",
    "            df = df.drop_duplicates(subset=subset, keep=False)\n",
    "            df = df.drop_duplicates(subset=ID_columns, keep='last')\n",
    "\n",
    "    else:\n",
    "        # Less efficient method.  Required if there are > 2 rows per entity.\n",
    "        df = df.sort_values('timestamp').groupby(ID_columns, group_keys=False)\\\n",
    "               .apply(filter_for_data_changes)\\\n",
    "               .reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    records_are_unique(df, include_timestamp_in_colnames(ID_columns))\n",
    "    df = set_df_datatypes(df, topic)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def records_are_unique(df: pd.DataFrame, record_id_cols: List[str]) -> bool:\n",
    "    \"\"\"Check if records in df can be uniquely identified using record_id_cols\n",
    "    and raise warning if they are not.\"\"\"\n",
    "    are_unique = df.set_index(record_id_cols).index.is_unique\n",
    "    if not are_unique:\n",
    "        warnings.warn(f\"Records in dataframe are not uniquely identified by {record_id_cols}\")\n",
    "    return are_unique\n",
    "\n",
    "\n",
    "def include_timestamp_in_colnames(col_names: Union[List[str], str]) -> List[str]:\n",
    "    \"\"\"Returns a list of strings which includes 'timestamp' in addition to the list\n",
    "    or sting given for `col_names`.\n",
    "    \n",
    "    >>> include_timestamp_in_colnames(topic_ID_col_names['terrain'])\n",
    "    ['resortID', 'runID', 'terrainName', 'timestamp']\n",
    "    \"\"\"\n",
    "    col_names = deepcopy(col_names)\n",
    "    if type(col_names) == str : col_names = [col_names]\n",
    "    col_names.extend(['timestamp'])\n",
    "    return col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process lift json fies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_status_durations(lifts_df):\n",
    "    '''Calculate values and add columns for the time difference between the\n",
    "    timestamp for the current status and the timestamp for the next status\n",
    "    for each lift:\n",
    "    `time_diff` column: Gives the duration that the lift was in the status indicated in the `status` column.\n",
    "    `time_diff_seconds` column: `time_diff` converted to seconds.\n",
    "    \n",
    "    lifts_status_changes_df should be TBD\n",
    "    '''\n",
    "    # TBD: optimize if needed via # 3 under:\n",
    "    # https://towardsdatascience.com/pandas-tips-and-tricks-33bcc8a40bb9\n",
    "    df = lifts_df.sort_values(by=['resortID', 'liftID', 'timestamp'])\n",
    "    df['time_diff'] = df.groupby(['resortID', 'liftID'])['timestamp'].diff(1).shift(-1)\n",
    "\n",
    "    # Fill in the durations which will be missing for the most recent status changes\n",
    "    missing_time_diffs_idx = df.loc[(df['time_diff'].isnull()) & (\n",
    "        df['timestamp'] >= df['timestamp'].min()), 'timestamp'].index.values\n",
    "\n",
    "    df.loc[missing_time_diffs_idx, 'time_diff'] = df['timestamp'].max(\n",
    "    ) - df.loc[missing_time_diffs_idx, 'timestamp']\n",
    "\n",
    "    # Convert to seconds\n",
    "    df['time_diff_seconds'] = df['time_diff'].dt.total_seconds()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whistler Lifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_matching_jsons_on_s3(suffix=\"lifts.json\", save_file=merged_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whis_lifts_df = load_json_as_df(merged_json_file, 'lifts')\n",
    "\n",
    "whis_lifts_status_changes_df = get_data_changes(whis_lifts_df, 'lifts', keep_oldest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifts_status_changes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** `timeToRide` is just the time is takes to ride the lift, not the current wait time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whis_lifts_df.groupby(\"liftName\")['timeToRide'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whis_lifts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whis_lifts_df = get_status_durations(whis_lifts_status_changes_df)\n",
    "\n",
    "# Uses local date formatting, otherwise Tableau will mix up month and day\n",
    "# alternatively, can export to json:\n",
    "# lifts_status_changes_df.to_json(DATA_DIR + \"lifts_status_changes.json\", orient='table')\n",
    "whis_lifts_df.to_csv(DATA_DIR + \"whis_lifts_status_changes.csv\", date_format='%c')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add:\n",
    "# \n",
    "# daily: for each chair calculate most open status of the day: O > H > X\n",
    "# Days since each chair was last seen open with timestamp of most recent open time.\n",
    "# snowfall since last open\n",
    "# save data for other mountains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through JSON files for all topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in ['lifts', 'terrain', 'weather']:\n",
    "    merge_matching_jsons_on_s3(suffix=topic + \".json\", save_file=merged_json_file)\n",
    "    df = load_json_as_df(merged_json_file, topic)\n",
    "    status_changes_df = get_data_changes(df, topic, keep_oldest=True)\n",
    "    \n",
    "    if topic == 'lifts':\n",
    "        get_status_durations(status_changes_df).to_csv(DATA_DIR + 'whis_lifts_status_changes.csv', date_format='%c')\n",
    "    else:\n",
    "        status_changes_df.to_csv(DATA_DIR + 'whis_' + topic + '_status_changes.csv', date_format='%c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Storage options testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle(DATA_DIR + \"df_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fastparquet import write\n",
    "\n",
    "# parquet engines don't handle shifted timezones\n",
    "import pytz\n",
    "TZ = pytz.timezone('America/Vancouver')\n",
    "df['timestamp'] = df.timestamp.dt.tz_convert(pytz.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Note: May need snappy-python as a req to run on AWS Lambda\n",
    "df.to_parquet(DATA_DIR + \"df_test.parquet\", engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "load_df = pd.read_parquet(DATA_DIR + \"df_test.parquet\")\n",
    "load_df['timestamp'] = load_df.timestamp.dt.tz_convert(TZ) # convert back to correct timezone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#TBD convert back to correct datatypes\n",
    "load_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(DATA_DIR + \"df_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Test file size results:\n",
    "- json: 800 Kb?\n",
    "- csv: 474 Kb\n",
    "- pickle: 145 Kb\n",
    "- parquet: 15 Kb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Delta Lake Notes\n",
    "\n",
    "Requires apache spark instance.  For future use, could set one up to work with lambda using https://aws.amazon.com/emr/features/spark/?\n",
    "\n",
    "Otherwise databricks (similar to QxMD project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet on S3\n",
    "\n",
    "For all topics from the EpicMix API.  Compare most recent topic data from json on S3 and if the data has changes, append the changes to parquet file on S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "fs = s3fs.S3FileSystem()\n",
    "myopen = fs.open\n",
    "nop = lambda *args, **kwargs: None\n",
    "\n",
    "HISTORY_SUFFIX = '_history_DEV.parquet'\n",
    "PRIOR_SUFFIX = '_prior_DEV.json'\n",
    "\n",
    "\n",
    "def write_dataframe_to_parquet_on_s3(df, topic, fname):\n",
    "    \"\"\" Write a dataframe to a Parquet file on S3.  Creates a new parquet file if one\n",
    "    doesn't already exist.\n",
    "    \"\"\"\n",
    "\n",
    "    def write_parquet(df, fname, app=True):\n",
    "\n",
    "        output_file = f\"s3://{BUCKET_NAME}/{fname}\"\n",
    "        write(output_file,\n",
    "              df,\n",
    "              # partition_on=['timestamp'],\n",
    "              file_scheme='hive',\n",
    "              append=app,  # need to remove or catch exception to work when file doesn't exist\n",
    "              open_with=myopen,\n",
    "              mkdirs=nop)\n",
    "        print(f\"Writing {len(df)} records to {fname}.\")\n",
    "\n",
    "    # Unshift the timezone because parquet engines don't handle shifted timezones\n",
    "    df.loc[:, 'timestamp'] = df.loc[:, 'timestamp'].dt.tz_convert(pytz.utc)\n",
    "\n",
    "    s3_object = bucket.Object(fname)\n",
    "\n",
    "    if not list(bucket.objects.filter(Prefix=fname)):\n",
    "        print(f\"File {fname} not found.  Creating new file.\")\n",
    "        # Keep oldest record for each entity because creating new file\n",
    "        df = get_data_changes(df, topic=topic, keep_oldest=True)\n",
    "        write_parquet(df, fname, app=False)\n",
    "\n",
    "    else:\n",
    "        print(f\"File {fname} found on S3.\")\n",
    "        df = get_data_changes(df, topic=topic, keep_oldest=False)\n",
    "        write_parquet(df, fname, app=True)\n",
    "\n",
    "\n",
    "def filter_resort(data, resortID: int = None) -> dict:\n",
    "    \"\"\"Filter for a specific resort.\"\"\"\n",
    "    if resortID:\n",
    "        return data[\"resortID\"] == resortID\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "\n",
    "def get_data(filter_topic: Union[str, List] = None, filter_resortID: int = None) -> dict:\n",
    "    \"\"\"Get data from EpicMix API. Defaults to all resorts.  Option to filter for a\n",
    "    specific resort or topic.\n",
    "    \"\"\"\n",
    "    API_URL = 'http://www.epicmix.com/vailresorts/sites/epicmix/api/mobile/'\n",
    "    # keys are used in the requests, the values and used in the response\n",
    "    DATA_LIST = {'lifts': 'lifts',\n",
    "                 'weather': 'snowconditions', 'terrain': 'terrains'}\n",
    "    json_data = dict()\n",
    "\n",
    "    # Create lists to filter by topic\n",
    "    if filter_topic is not None:\n",
    "        filtered_data_list = {k: v for k,\n",
    "                              v in DATA_LIST.items() if k in filter_topic}\n",
    "    else:\n",
    "        filtered_data_list = DATA_LIST\n",
    "\n",
    "    for d, name in filtered_data_list.items():\n",
    "        res = requests.get(API_URL + d + '.ashx')\n",
    "        res.raise_for_status()\n",
    "        data = json.loads(res.text)[name]\n",
    "        data = list(filter(lambda x: filter_resort(x, filter_resortID), data))\n",
    "        json_data[d] = json.dumps(\n",
    "            {'timestamp': str(datetime.now(TZ)), d: data})\n",
    "\n",
    "    return json_data\n",
    "\n",
    "\n",
    "def s3_object_exists(fname):\n",
    "    \"\"\"Check if an s3 object exists.  Returns `True` if the object exists.\"\"\"\n",
    "    try:\n",
    "        bucket.Object(fname)\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            print(f\"{fname} doesn't exist\")\n",
    "        else:\n",
    "            raise\n",
    "    return True\n",
    "\n",
    "\n",
    "def load_dataframe_from_parquet_on_s3(fname):\n",
    "    \"\"\" Load a dataframe from a Parquet file on S3. \"\"\"\n",
    "    if s3_object_exists(fname):\n",
    "        read_file = f\"s3://{BUCKET_NAME}/{fname}\"\n",
    "        pf = ParquetFile(read_file, open_with=myopen)\n",
    "        df = pf.to_pandas()\n",
    "\n",
    "        # Reshift the timezone because parquet engines don't handle shifted timezones\n",
    "        df.loc[:, 'timestamp'] = df.loc[:, 'timestamp'].dt.tz_convert(TZ)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "class api_data():\n",
    "    def __init__(self, topic: str, current_json: str):\n",
    "        self.topic = topic\n",
    "        self.current_json = current_json\n",
    "        # May not exist yet\n",
    "        self.prior_fname = topic + PRIOR_SUFFIX\n",
    "        self.prior_object = bucket.Object(self.prior_fname)\n",
    "        self.check_prior_object()\n",
    "\n",
    "    def check_prior_object(self):\n",
    "        \"\"\"Get prior data json\"\"\"\n",
    "        try:\n",
    "            self.prior_object.load()\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] == \"404\":\n",
    "                print(f\"Prior json for {self.topic} doesn't exist\")\n",
    "                self.prior_exists = False\n",
    "            else:\n",
    "                # Something else has gone wrong.\n",
    "                raise\n",
    "        else:\n",
    "            self.prior_exists = True\n",
    "        return self.prior_exists\n",
    "\n",
    "    def get_prior_data_json(self):\n",
    "        \"\"\"Get prior data json from S3.\"\"\"\n",
    "        if self.prior_exists == True:\n",
    "            prior = self.prior_object.get()['Body'].read().decode('utf-8')\n",
    "            self.prior_json = json.loads(prior)\n",
    "            print(f\"Loaded prior {self.topic} json data from S3\")\n",
    "            return self.prior_json\n",
    "        else:\n",
    "            print(f\"Prior json for {self.topic} doesn't exist\")\n",
    "\n",
    "    def data_changed(self):\n",
    "        \"\"\"Compare current data json with prior data json without their timestamps.  The timestamps\n",
    "        on the current json will always be more recent even when none of the other data has changed.\n",
    "        \"\"\"\n",
    "        if self.prior_json[self.topic] == self.current_json[self.topic]:\n",
    "            print(\n",
    "                f\"No differences between current and prior {self.topic} data were found.\")\n",
    "            return False\n",
    "        else:\n",
    "            print(\n",
    "                f\"Found differences between current and prior {self.topic} data.\")\n",
    "            return True\n",
    "\n",
    "    def save_prior_data(self):\n",
    "        \"\"\"Save the current data as prior data on S3.\"\"\"\n",
    "        bucket.put_object(Key=self.prior_fname,\n",
    "                          Body=bytes(json.dumps(self.current_json).encode('UTF-8')))\n",
    "\n",
    "\n",
    "class ParquetWriter():\n",
    "    \"\"\"Identifies new data and writes it to Parquet file on S3.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Get current data\n",
    "        self.data_current_all = get_data()  # String.\n",
    "\n",
    "    def write_new_data_all(self):\n",
    "        \"\"\"Writes new data for each type (i.e. 'lift', 'weather', 'terrian')\n",
    "        of data returned by the API.\n",
    "        \"\"\"\n",
    "        for topic in self.data_current_all:\n",
    "            current_json = json.loads(self.data_current_all[topic])\n",
    "            data = api_data(topic, current_json)\n",
    "            self.write_new_data(data)\n",
    "\n",
    "    def write_new_data(self, api_data):\n",
    "        \"\"\"If current data has changed since the last update of Parquet file is, add it\n",
    "        to the Parquet file.  Save the current data as json to serve as the prior for\n",
    "        the next comparison.\n",
    "        \"\"\"\n",
    "\n",
    "        if api_data.prior_exists:\n",
    "            api_data.get_prior_data_json()\n",
    "            if api_data.data_changed():\n",
    "                # Get a df with the chages between the prior and current json data\n",
    "                df = jsons_to_df(\n",
    "                    [api_data.prior_json, api_data.current_json], record_path=api_data.topic)\n",
    "                write_dataframe_to_parquet_on_s3(\n",
    "                    df, api_data.topic, api_data.topic + HISTORY_SUFFIX)\n",
    "\n",
    "                # save current data json as prior\n",
    "                api_data.save_prior_data()\n",
    "                print(\n",
    "                    f\"Replaced data in {api_data.prior_object.key} with current data.\")\n",
    "        else:\n",
    "            print(f\"Prior json for {api_data.topic} doesn't exist\")\n",
    "            # Create the prior file\n",
    "            api_data.save_prior_data()\n",
    "            print(f\"Created {api_data.prior_fname}\")\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded prior lifts json data from S3\n",
      "No differences between current and prior lifts data were found.\n",
      "\n",
      "\n",
      "Loaded prior weather json data from S3\n",
      "Found differences between current and prior weather data.\n",
      "File weather_history_DEV.parquet found on S3.\n",
      "Writing 2 records to weather_history_DEV.parquet.\n",
      "Replaced data in weather_prior_DEV.json with current data.\n",
      "\n",
      "\n",
      "Loaded prior terrain json data from S3\n",
      "No differences between current and prior terrain data were found.\n",
      "\n",
      "\n",
      "CPU times: user 289 ms, sys: 35.8 ms, total: 325 ms\n",
      "Wall time: 2.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pr = ParquetWriter()\n",
    "pr.write_new_data_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "while True:\n",
    "    print('\\n\\n\\n' + time.ctime() + ':\\n---------------------')\n",
    "    pr = ParquetWriter()\n",
    "    pr.write_new_data_all()\n",
    "    time.sleep(3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warnings**\n",
    "\n",
    "See https://github.com/dask/fastparquet/issues/477 for fastparquet warnings about `RangeIndex._start, RangeIndex._stop, RangeIndex._step`\n",
    "\n",
    "\n",
    "    /Users/paul/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:90: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
    "    /Users/paul/anaconda3/lib/python3.7/site-packages/fastparquet/writer.py:655: FutureWarning: RangeIndex._start is deprecated and will be removed in a future version. Use RangeIndex.start instead\n",
    "      index_cols = [{'name': index_cols.name, 'start': index_cols._start,\n",
    "    /Users/paul/anaconda3/lib/python3.7/site-packages/fastparquet/writer.py:656: FutureWarning: RangeIndex._stop is deprecated and will be removed in a future version. Use RangeIndex.stop instead\n",
    "      'stop': index_cols._stop, 'step': index_cols._step,\n",
    "    /Users/paul/anaconda3/lib/python3.7/site-packages/fastparquet/writer.py:656: FutureWarning: RangeIndex._step is deprecated and will be removed in a future version. Use RangeIndex.step instead\n",
    "      'stop': index_cols._stop, 'step': index_cols._step,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parquet and save as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parq_df = load_dataframe_from_parquet_on_s3('lifts' + HISTORY_SUFFIX)\n",
    "lifts_status_changes_parq_df = get_status_durations(parq_df)\n",
    "lifts_status_changes_parq_df.to_csv(\n",
    "    DATA_DIR + \"lifts_status_changes_parq.csv\", date_format='%c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_DIR = \"../data/test/\"\n",
    "TEST_VALIDATION_DATA_DIR = TEST_DATA_DIR + \"valid/\"\n",
    "lifts_json_test_file = TEST_DATA_DIR + \"lifts_test.json\"\n",
    "terrain_json_test_file = TEST_DATA_DIR + \"terrain_test.json\"\n",
    "weather_json_test_file = TEST_DATA_DIR + \"weather_test.json\"\n",
    "merged_lifts_json_test_file = TEST_DATA_DIR + \"merged_lifts_test.json\"\n",
    "merged_terrain_json_test_file = TEST_DATA_DIR + \"merged_terrain_test.json\"\n",
    "merged_weather_json_test_file = TEST_DATA_DIR + \"merged_weather_test.json\"\n",
    "merged_whis_lifts_json_test_file = TEST_DATA_DIR + \"merged_whis_lifts_test.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find column combinations to identify entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combo: ('resortID', 'runID', 'runName')\tDuplicates: 1\n",
      "Combo: ('resortID', 'runID', 'terrainName')\tDuplicates: 0\n",
      "Combo: ('resortID', 'runName', 'terrainName')\tDuplicates: 2\n",
      "Combo: ('runID', 'runName', 'terrainName')\tDuplicates: 0\n",
      "Combo: ['resortID', 'runID', 'runName', 'terrainName']\tDuplicates: 0\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Move to EDA notebook\n",
    "# Do we really need to use all the terrain columns in topic_ID_col_names to uniquely identify each run?\n",
    "ID_cols = ['resortID', 'runID', 'runName', 'terrainName']\n",
    "df = load_json_as_df(terrain_json_test_file, 'terrain')\n",
    "\n",
    "for combo in combinations(ID_cols, len(ID_cols)-1):\n",
    "    print(f\"Combo: {combo}\\tDuplicates: {df.duplicated(combo).sum()}\")\n",
    "    \n",
    "print(f\"Combo: {ID_cols}\\tDuplicates: {df.duplicated(ID_cols).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will always get one duplicate lift entry because the Peak 2 Peak Gondola is returned twice in the API data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combo: ('resortID', 'liftName')\tDuplicates: 1\n",
      "Combo: ('resortID', 'liftID')\tDuplicates: 2\n",
      "Combo: ('liftName', 'liftID')\tDuplicates: 1\n",
      "Combo: ['resortID', 'liftName', 'liftID']\tDuplicates: 1\n"
     ]
    }
   ],
   "source": [
    "ID_cols = ['resortID', 'liftName', 'liftID']\n",
    "df = load_json_as_df(lifts_json_test_file, 'lifts')\n",
    "\n",
    "for combo in combinations(ID_cols, len(ID_cols)-1):\n",
    "    print(f\"Combo: {combo}\\tDuplicates: {df.duplicated(combo).sum()}\")\n",
    "\n",
    "print(f\"Combo: {ID_cols}\\tDuplicates: {df.duplicated(ID_cols).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_ID_col_names (__main__.TestNotebook)\n",
      "Make sure that records can be uniquely identified by using ID columns for each topic ... ok\n",
      "test_get_data (__main__.TestNotebook) ... ok\n",
      "test_get_data_changes (__main__.TestNotebook) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 3.942s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x11c860a58>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new\n",
    "\n",
    "# TBD add test descriptions?\n",
    "\n",
    "\n",
    "class TestNotebook(unittest.TestCase):\n",
    "\n",
    "    topic_names = ['lifts', 'terrain', 'weather']\n",
    "\n",
    "    def test_get_data(self):\n",
    "        self.assertEqual(\n",
    "            list(get_data(filter_topic=['lifts']).keys()), ['lifts'],\n",
    "            'Returned dictionary was not filtered for the right topic'\n",
    "        )\n",
    "        self.assertEqual(\n",
    "            list(get_data(filter_topic=['lifts', 'terrain']).keys()),\n",
    "            ['lifts', 'terrain'],\n",
    "            'Returned dictionary was not filtered for the right topics'\n",
    "        )\n",
    "        self.assertEqual(\n",
    "            set(get_data().keys()), {'lifts', 'weather', 'terrain'},\n",
    "            'Returned dictionary was not filtered for all topics'\n",
    "        )\n",
    "\n",
    "    def test_get_data_changes(self):\n",
    "\n",
    "        # 'lifts' needs to be listed twice under 'topic' because there are two tests\n",
    "        # run on lift data.  The test using merged_whis_lifts_json_test_file has more than\n",
    "        # 2 timepoints in order to test the special code that is used to handle that case.\n",
    "        tests_df = pd.DataFrame({\n",
    "            'test_file': [merged_whis_lifts_json_test_file,\n",
    "                          merged_lifts_json_test_file,\n",
    "                          merged_terrain_json_test_file,\n",
    "                          merged_weather_json_test_file],\n",
    "            'topic': ['lifts', 'lifts', 'terrain', 'weather'],\n",
    "            'validation_fname_prefix': ['get_data_changes_merged_whis_lifts',\n",
    "                                        'get_data_changes_merged_lifts',\n",
    "                                        'get_data_changes_merged_terrain',\n",
    "                                        'get_data_changes_merged_weather']\n",
    "        })\n",
    "\n",
    "        for row in tests_df.iterrows():\n",
    "\n",
    "            test_file = row[1]['test_file']\n",
    "            df = load_json_as_df(test_file, row[1]['topic'])\n",
    "            df = df.sample(frac=1)  # Shuffle the data\n",
    "\n",
    "            for keep_oldest in [True, False]:\n",
    "                if keep_oldest == True:\n",
    "                    validation_fname_suffix = '_keep_oldest_valid.json'\n",
    "                else:\n",
    "                    validation_fname_suffix = '_drop_oldest_valid.json'\n",
    "\n",
    "                tested_df = get_data_changes(\n",
    "                    df, row[1]['topic'], keep_oldest=keep_oldest)\n",
    "\n",
    "                valid_file = TEST_VALIDATION_DATA_DIR + \\\n",
    "                    row[1]['validation_fname_prefix'] + validation_fname_suffix\n",
    "                valid_df = pd.read_pickle(valid_file)\n",
    "\n",
    "                #  Sort and reindex before comparison because we are not testing the indexes\n",
    "                # or row orders that the functions return.\n",
    "                tested_df.sort_values(\n",
    "                    tested_df.columns.to_list(), ignore_index=True, inplace=True)\n",
    "                valid_df.sort_values(\n",
    "                    valid_df.columns.to_list(), ignore_index=True, inplace=True)\n",
    "\n",
    "                pd.testing.assert_frame_equal(\n",
    "                    tested_df, valid_df,\n",
    "                    f\"Result from {test_file} did not match validation dataframe {valid_file}.\"\n",
    "                )\n",
    "\n",
    "    def test_ID_col_names(self):\n",
    "        \"\"\"Make sure that records can be uniquely identified by using ID columns for each topic\n",
    "        (in combination with timestamp)\"\"\"\n",
    "        files = [merged_lifts_json_test_file,\n",
    "                 merged_terrain_json_test_file, merged_weather_json_test_file]\n",
    "\n",
    "        for file, topic in zip(files, self.topic_names):\n",
    "\n",
    "            df = load_json_as_df(file, topic)\n",
    "\n",
    "            # Drop any rows that are complete duplicates. This is required for the Peak 2 Peak\n",
    "            # Gondola because it is duplicated in the lifts data.  Maybe others as well.\n",
    "            df.drop_duplicates(inplace=True)\n",
    "            \n",
    "            record_id_cols = include_timestamp_in_colnames(topic_ID_col_names[topic])\n",
    "            self.assertTrue(records_are_unique(df, record_id_cols),\n",
    "                            f\"{record_id_cols} are not sufficient to uniquely identify the {topic} records.\")\n",
    "\n",
    "    \n",
    "# To add:    \n",
    "# test records_are_unique() raises warning\n",
    "    \n",
    "    \n",
    "    # Assert df has no NaN or NaTs:\n",
    "    # assert parq_df.isnull().sum().sum() == 0\n",
    "\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>liftID</th>\n",
       "      <th>resortID</th>\n",
       "      <th>liftName</th>\n",
       "      <th>status</th>\n",
       "      <th>timeToRide</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Avanti Express Lift #2</td>\n",
       "      <td>O</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-02-14 17:43:53.685619-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Wildwood Express #3</td>\n",
       "      <td>O</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-02-14 17:43:53.685619-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Mountain Top Express #4</td>\n",
       "      <td>O</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-02-14 17:43:53.685619-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>High Noon Express #5</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-02-14 17:43:53.685619-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Riva Bahn Express #6</td>\n",
       "      <td>O</td>\n",
       "      <td>9</td>\n",
       "      <td>2020-02-14 17:43:53.685619-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>Sunshine Quad</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-02-14 17:43:53.685619-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>Glades Peak Quad</td>\n",
       "      <td>X</td>\n",
       "      <td>8</td>\n",
       "      <td>2020-02-14 17:43:53.685619-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>Green Ridge Triple</td>\n",
       "      <td>X</td>\n",
       "      <td>8</td>\n",
       "      <td>2020-02-14 17:43:53.685619-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>Orion's Belt Carpet</td>\n",
       "      <td>X</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-14 17:43:53.685619-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>South Ridge Quad B</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-02-14 17:43:53.685619-08:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>323 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    liftID resortID                 liftName status timeToRide  \\\n",
       "0        2        1   Avanti Express Lift #2      O          7   \n",
       "1        3        1      Wildwood Express #3      O          5   \n",
       "2        4        1  Mountain Top Express #4      O          4   \n",
       "3        5        1     High Noon Express #5      O          6   \n",
       "4        6        1     Riva Bahn Express #6      O          9   \n",
       "..     ...      ...                      ...    ...        ...   \n",
       "318     15       17            Sunshine Quad      O          6   \n",
       "319      4       17         Glades Peak Quad      X          8   \n",
       "320     16       17       Green Ridge Triple      X          8   \n",
       "321      6       17      Orion's Belt Carpet      X          1   \n",
       "322     17       17       South Ridge Quad B      X          6   \n",
       "\n",
       "                           timestamp  \n",
       "0   2020-02-14 17:43:53.685619-08:00  \n",
       "1   2020-02-14 17:43:53.685619-08:00  \n",
       "2   2020-02-14 17:43:53.685619-08:00  \n",
       "3   2020-02-14 17:43:53.685619-08:00  \n",
       "4   2020-02-14 17:43:53.685619-08:00  \n",
       "..                               ...  \n",
       "318 2020-02-14 17:43:53.685619-08:00  \n",
       "319 2020-02-14 17:43:53.685619-08:00  \n",
       "320 2020-02-14 17:43:53.685619-08:00  \n",
       "321 2020-02-14 17:43:53.685619-08:00  \n",
       "322 2020-02-14 17:43:53.685619-08:00  \n",
       "\n",
       "[323 rows x 6 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TBD: test dtypes if not covered by type hint testing\n",
    "test_json = load_prior_json_from_s3('lifts')\n",
    "jsons_to_df(test_json, 'lifts')#.dtypes\n",
    "\n",
    "# TBD Make sure categories are complete... by sorting columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "runID                                   category\n",
       "resortID                                category\n",
       "groomed                                 category\n",
       "runName                                 category\n",
       "runType                                 category\n",
       "status                                  category\n",
       "terrainName                             category\n",
       "timestamp      datetime64[ns, America/Vancouver]\n",
       "dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic = 'terrain'\n",
    "\n",
    "# test that datatypes are correct if not covered by hint testing?\n",
    "parq_df = load_dataframe_from_parquet_on_s3(topic + HISTORY_SUFFIX)\n",
    "parq_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that the status categories are complete for lifts and terrain\n",
    "# maybe like this or direct access the index\n",
    "test_terrain = load_dataframe_from_parquet_on_s3(topic + HISTORY_SUFFIX)\n",
    "#test_terrain.status.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "runID                                   category\n",
       "resortID                                category\n",
       "groomed                                 category\n",
       "runName                                 category\n",
       "runType                                 category\n",
       "status                                  category\n",
       "terrainName                             category\n",
       "timestamp      datetime64[ns, America/Vancouver]\n",
       "dtype: object"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parq_df = set_df_datatypes(parq_df, topic)\n",
    "parq_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted by ['resortID', 'runID', 'terrainName', 'timestamp']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>runID</th>\n",
       "      <th>resortID</th>\n",
       "      <th>groomed</th>\n",
       "      <th>runName</th>\n",
       "      <th>runType</th>\n",
       "      <th>status</th>\n",
       "      <th>terrainName</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>Apres Vous</td>\n",
       "      <td>3</td>\n",
       "      <td>O</td>\n",
       "      <td>Back Bowls</td>\n",
       "      <td>2020-02-13 15:06:38.957648-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Big Rock Park</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "      <td>Blue Sky Basin</td>\n",
       "      <td>2020-02-13 15:06:38.957648-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>Bolshoi Ballroom</td>\n",
       "      <td>3</td>\n",
       "      <td>O</td>\n",
       "      <td>China Bowl</td>\n",
       "      <td>2020-02-13 15:06:38.957648-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>Blue Ox</td>\n",
       "      <td>3</td>\n",
       "      <td>O</td>\n",
       "      <td>Golden Peak</td>\n",
       "      <td>2020-02-13 15:06:38.957648-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>Baccarat</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "      <td>Lionshead</td>\n",
       "      <td>2020-02-13 15:06:38.957648-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>1749</td>\n",
       "      <td>17</td>\n",
       "      <td>No</td>\n",
       "      <td>Whispering Pines</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "      <td>Main Face</td>\n",
       "      <td>2020-02-13 15:06:38.957648-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912</th>\n",
       "      <td>1750</td>\n",
       "      <td>17</td>\n",
       "      <td>No</td>\n",
       "      <td>Whistler</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "      <td>Main Face</td>\n",
       "      <td>2020-02-13 15:06:38.957648-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>1751</td>\n",
       "      <td>17</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Zip</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>Main Face</td>\n",
       "      <td>2020-02-13 15:06:38.957648-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914</th>\n",
       "      <td>1752</td>\n",
       "      <td>17</td>\n",
       "      <td>No</td>\n",
       "      <td>Broken Arrow</td>\n",
       "      <td>3</td>\n",
       "      <td>X</td>\n",
       "      <td>Main Face</td>\n",
       "      <td>2020-02-13 15:06:38.957648-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>1753</td>\n",
       "      <td>17</td>\n",
       "      <td>No</td>\n",
       "      <td>Rum Run</td>\n",
       "      <td>2</td>\n",
       "      <td>X</td>\n",
       "      <td>Main Face</td>\n",
       "      <td>2020-02-13 15:06:38.957648-08:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2200 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      runID resortID groomed           runName runType status     terrainName  \\\n",
       "index                                                                           \n",
       "127      10        1      No        Apres Vous       3      O      Back Bowls   \n",
       "171      10        1     Yes     Big Rock Park       2      O  Blue Sky Basin   \n",
       "153      10        1      No  Bolshoi Ballroom       3      O      China Bowl   \n",
       "0        10        1      No           Blue Ox       3      O     Golden Peak   \n",
       "87       10        1      No          Baccarat       2      O       Lionshead   \n",
       "...     ...      ...     ...               ...     ...    ...             ...   \n",
       "1911   1749       17      No  Whispering Pines       2      O       Main Face   \n",
       "1912   1750       17      No          Whistler       2      O       Main Face   \n",
       "1913   1751       17     Yes               Zip       1      O       Main Face   \n",
       "1914   1752       17      No      Broken Arrow       3      X       Main Face   \n",
       "1915   1753       17      No           Rum Run       2      X       Main Face   \n",
       "\n",
       "                             timestamp  \n",
       "index                                   \n",
       "127   2020-02-13 15:06:38.957648-08:00  \n",
       "171   2020-02-13 15:06:38.957648-08:00  \n",
       "153   2020-02-13 15:06:38.957648-08:00  \n",
       "0     2020-02-13 15:06:38.957648-08:00  \n",
       "87    2020-02-13 15:06:38.957648-08:00  \n",
       "...                                ...  \n",
       "1911  2020-02-13 15:06:38.957648-08:00  \n",
       "1912  2020-02-13 15:06:38.957648-08:00  \n",
       "1913  2020-02-13 15:06:38.957648-08:00  \n",
       "1914  2020-02-13 15:06:38.957648-08:00  \n",
       "1915  2020-02-13 15:06:38.957648-08:00  \n",
       "\n",
       "[2200 rows x 8 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Sort by timestamp and ID columns\n",
    "record_id_cols = include_timestamp_in_colnames(topic_ID_col_names[topic])\n",
    "print(f\"sorted by {record_id_cols}\")\n",
    "parq_df.sort_values(record_id_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example code: Show data for whistler\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(parq_df.sort_values(sort).query('resortID == 13'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example code: Filter dataframe by date\n",
    "parq_df = parq_df[parq_df['timestamp'] > '2020-02-08']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a entities that has the same runName and resortID, but different terrainNames:\n",
    "parq_df.query('runName == \"Bear Paw\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can uniquely identify each run via a combination of: `resortID`, `runID` and `terrainName`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't need to include `runName`:\n",
    "assert(\n",
    "    parq_df.drop_duplicates(subset=['resortID', 'runID', 'terrainName', 'timestamp'], keep=False).equals(\n",
    "    parq_df.drop_duplicates(subset=['resortID', 'runID', 'runName', 'terrainName', 'timestamp'], keep=False))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD: testing\n",
    "- no duplicate rows\n",
    "- no duplicate information in adjacent rows by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load live whistler parquet data\n",
    "\n",
    "%time parq_df = load_dataframe_from_parquet_on_s3(HISTORY_FNAME)\n",
    "\n",
    "lifts_status_changes_parq_df = get_status_durations(parq_df)\n",
    "lifts_status_changes_parq_df.to_csv(DATA_DIR + \"lifts_status_changes_parq.csv\", date_format='%c')\n",
    "lifts_status_changes_parq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parq_df.status.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*parq_df.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parq_df.sort_values([\"liftName\", \"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue when running get_status_durations(parq_df)\n",
    "Resulting in error:\n",
    "\n",
    "    ~/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype)\n",
    "        705 \n",
    "        706         if len(codes) and (codes.max() >= len(dtype.categories) or codes.min() < -1):\n",
    "    --> 707             raise ValueError(\"codes need to be between -1 and \" \"len(categories)-1\")\n",
    "        708 \n",
    "        709         return cls(codes, dtype=dtype, fastpath=True)\n",
    "\n",
    "    ValueError: codes need to be between -1 and len(categories)-1\n",
    "\n",
    "\n",
    "Same error seen when running `parq_df[['status']].sort_values(by=['status'])`\n",
    "\n",
    "This was caused by missing categories (`H`) in the `status` column (and maybe others)\n",
    "\n",
    "#### Code to inspect issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for issue\n",
    "parq_df[['status']].sort_values(by=['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X', 'H', 'O'], dtype='object')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parq_df.status.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0\n"
     ]
    }
   ],
   "source": [
    "# All the category codes present in the column\n",
    "print(*parq_df.status.cat.codes.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2200"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parq_df.status.cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should be false\n",
    "parq_df.status.cat.codes.max() >= len(parq_df.status.dtype.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'liftName'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-3c6f860f755c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Should be false\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparq_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliftName\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5271\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5272\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5273\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'liftName'"
     ]
    }
   ],
   "source": [
    "# Should be false\n",
    "parq_df.liftName.cat.codes.min() < -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([  10,   11,   12,   13,   14,   15,   16,   17,   18,   19,\n",
      "            ...\n",
      "            1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753],\n",
      "           dtype='int64', length=610)\n",
      "Int64Index([1, 2, 3, 4, 5, 7, 9, 12, 13, 14, 15, 16, 17], dtype='int64')\n",
      "Index(['No', 'Yes'], dtype='object')\n",
      "Index(['$100 Saddle', '1/2 Load', '1/4 Load', '10th Mountain',\n",
      "       '117th Street Small/Medium Rail Garden', '18' Half Pipe', '1876',\n",
      "       '1st Bowl', '2 Lift Line', '3 Kings',\n",
      "       ...\n",
      "       'Yarrow', 'Yellow Brick Road', 'Yonder', 'Yonder Gully', 'Zachary',\n",
      "       'Zeke’s Road', 'Zig Zag', 'Zip', 'Zoom Room', 'Zot'],\n",
      "      dtype='object', length=1844)\n",
      "Int64Index([0, 1, 2, 3, 4, 5, 7], dtype='int64')\n",
      "Index(['X', 'H', 'O'], dtype='object')\n",
      "Index(['7th Heaven', 'A51 Terrain Park', 'Arrowhead', 'Bachelor Gulch',\n",
      "       'Back Bowls', 'Back Side', 'Backside', 'Beaver Creek',\n",
      "       'Big Red - Franz's - Garbanzo', 'Birds Of Prey', 'Blue Sky Basin',\n",
      "       'Bonanza/McConkey's/Pioneer', 'Bowls', 'California', 'China Bowl',\n",
      "       'Creekside', 'Crystal Zone', 'Dercum Mountain',\n",
      "       'Dreamcatcher/Dreamscape/Day Break', 'Elkhorn', 'Emerald - Garbanzo',\n",
      "       'Excalibur – Blackcomb Gondola Lower',\n",
      "       'Excelerator – Blackcomb Gondola Upper', 'Front Side', 'Frontside',\n",
      "       'Glacier', 'Golden Peak', 'Grouse Mountain', 'Harmony', 'High Lift',\n",
      "       'Iron Mountain/Quicksilver Gondola/Timberline/Flat Iron',\n",
      "       'Jackson Gore', 'Jersey Cream', 'Jupiter', 'King Con/Eagle/Silver Star',\n",
      "       'Larkspur Bowl', 'Learning Area', 'Lionshead', 'Lookout Mountain',\n",
      "       'Main Face', 'Mt. Mansfield', 'Mt. Pluto', 'Nevada', 'North Face',\n",
      "       'North Peak', 'Northwest Territory', 'Orange Bubble/Saddleback',\n",
      "       'Outback', 'PayDay/Town/Crescent', 'Peak 10', 'Peak 6', 'Peak 7',\n",
      "       'Peak 7 Alpine', 'Peak 8', 'Peak 8 Imperial', 'Peak 9',\n",
      "       'Resort Skiways (Access to/from Homesites and Lodging)', 'Rose Bowl',\n",
      "       'Silverlode/Motherlode/Thaynes', 'Solitude', 'South Peak',\n",
      "       'South Ridge', 'Southface', 'Spruce', 'Spruce Peak Trails',\n",
      "       'Strawberry Park', 'Summit', 'Sun Bowl', 'Super Condor Express',\n",
      "       'Symphony Amphitheatre', 'T-Bar', 'Ten Peaks', 'Terrain Park',\n",
      "       'Terrain Parks', 'The Backside', 'The Peak - T Bar', 'Timber Creek',\n",
      "       'Tombstone/Peak 5/Ninety-Nine 90', 'Top Of Gondola', 'Vail Village',\n",
      "       'Village - Olympic - Fitzsimmons', 'Village at Northstar'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can only use .cat accessor with a 'category' dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-319fc11a6563>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparq_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparq_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5267\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5268\u001b[0m         ):\n\u001b[0;32m-> 5269\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5270\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5271\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0maccessor_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m# http://www.pydanny.com/cached-property.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   2508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2510\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2511\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2512\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m_validate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   2517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2518\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2519\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can only use .cat accessor with a 'category' dtype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_delegate_property_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .cat accessor with a 'category' dtype"
     ]
    }
   ],
   "source": [
    "for c in parq_df.columns:\n",
    "    print(parq_df[c].cat.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "parq_df[\"timestamp\"] = pd.to_datetime(pd.Series(np.asarray(parq_df[\"timestamp\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file = f\"s3://{BUCKET_NAME}/{fname}.parquet\"\n",
    "pf = ParquetFile(read_file, open_with=myopen)\n",
    "\n",
    "# Check the categories for a specific row group\n",
    "pf.grab_cats(columns='status', row_group_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If partitioning by column, gives known values for each column\n",
    "pf.cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Possible solutions\n",
    "1. Remove partitioning by date column when writing to parquet\n",
    "**2. Set status categories manually via `set_categories`. (and any other columns with the same issue.  See https://github.com/dask/dask/issues/2944**\n",
    "3. Leave problem columns as text-based when writing and loading from parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Testing timestamps for file loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "read_file = f\"s3://{BUCKET_NAME}/{fname}.parquet\"\n",
    "pf = ParquetFile(read_file, open_with=myopen)\n",
    "test = pf.to_pandas()[\"timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# If needed: to convert for categorical datetime to regular datetime\n",
    "df[\"timestamp\"] = pd.to_datetime(pd.Series(np.asarray(df[\"timestamp\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test.dt = test.dt.tz_convert(tz= 'America/Vancouver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "/Users/paul/anaconda3/lib/python3.7/site-packages/pandas/core/series.py:597: FutureWarning: Converting timezone-aware DatetimeArray to timezone-naive ndarray with 'datetime64[ns]' dtype. In the future, this will return an ndarray with 'object' dtype where each element is a 'pandas.Timestamp' with the correct 'tz'.\n",
    "\tTo accept the future behavior, pass 'dtype=object'.\n",
    "\tTo keep the old behavior, pass 'dtype=\"datetime64[ns]\"'.\n",
    "\n",
    "\n",
    "more info: https://pandas-docs.github.io/pandas-docs-travis/whatsnew/v0.24.0.html#converting-timezone-aware-series-and-index-to-numpy-arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "load_dataframe_from_parquet_on_s3(fname).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Testing local parquet saves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def save_parquet(df, fname):\n",
    "    # parquet engines don't handle shifted timezones\n",
    "    df.loc[:, 'timestamp'] = df.loc[:, 'timestamp'].dt.tz_convert(pytz.utc)\n",
    "\n",
    "    # Note: May need snappy-python as a req to run on AWS Lambda\n",
    "    df.to_parquet(DATA_DIR + fname + '.parquet',\n",
    "                  engine='fastparquet',\n",
    "                  partition_on=['timestamp'],\n",
    "                  file_scheme='mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_parquet(df[0:3].copy(), 'wb_lifts_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.iloc[20:22, :].copy().to_parquet(DATA_DIR + 'wb_lifts_history' + '.parquet',\n",
    "              engine='fastparquet',\n",
    "              partition_on=['timestamp'],\n",
    "              file_scheme='mixed',\n",
    "              append=True)\n",
    "# Catch exception that is doesn't exist here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# todo: change time_diff to \"duration\"\n",
    "# test on lambda\n",
    "# make datatype dict for and general set datatypes function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prior_json_from_s3(topic: str) -> dict:\n",
    "    \"\"\"E.g. load_prior_json_from_s3('weather')\"\"\"\n",
    "    prior_object = bucket.Object(topic + PRIOR_SUFFIX)\n",
    "    prior = prior_object.get()['Body'].read().decode('utf-8')\n",
    "    return json.loads(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_difference(df1, df2, which=None):\n",
    "    \"\"\"Find rows which are different between two DataFrames.\"\"\"\n",
    "    comparison_df = df1.merge(df2,\n",
    "                              indicator=True,\n",
    "                              how='outer')\n",
    "    if which is None:\n",
    "        diff_df = comparison_df[comparison_df['_merge'] != 'both']\n",
    "    else:\n",
    "        diff_df = comparison_df[comparison_df['_merge'] == which]\n",
    "    return diff_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD: Add to scraping \n",
    "# check that weather_prior_json['weather'][0].keys() matches list of expected columns (in case new ones are in use)\n",
    "# Add as exception handling for the other topics as well\n",
    "load_prior_json_from_s3('weather')['weather'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a json from S3\n",
    "terrain_prior_json = load_prior_json_from_s3('terrain')\n",
    "\n",
    "# Convert json to a dataframe normally wer can use jsons_to_df()\n",
    "terrain_prior_df = pd.json_normalize(\n",
    "    data=terrain_prior_json,\n",
    "    record_path=['terrain'],\n",
    "    meta='timestamp'\n",
    ")\n",
    "\n",
    "terrain_prior_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_lifts_history():\n",
    "    bucket.Object(HISTORY_FNAME).delete()\n",
    "\n",
    "def del_lifts_prior():\n",
    "    bucket.Object(PRIOR_STATUS_FNAME).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_lifts_history()\n",
    "del_lifts_prior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 Object Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete prior jsons\n",
    "bucket.Object('lifts' + PRIOR_SUFFIX).delete()\n",
    "bucket.Object('terrain' + PRIOR_SUFFIX).delete()\n",
    "bucket.Object('weather' + PRIOR_SUFFIX).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete parquet history files\n",
    "bucket.objects.filter(Prefix='lifts' + HISTORY_SUFFIX + '/').delete()\n",
    "bucket.objects.filter(Prefix='terrain' + HISTORY_SUFFIX + '/').delete()\n",
    "bucket.objects.filter(Prefix='weather' + HISTORY_SUFFIX + '/').delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- Terrain data: runs need to be identified via combination of `resortID`, `runName` and `runID`\n",
    "- There are run IDs that repeat for the same resort (e.g. for Vail resortID == 1, runID == 10\n",
    "- TBD: Are the combination of `resortID`, `runID`, and `runType` always unique?\n",
    "\n",
    "## To do\n",
    "- return data object with filter_by_resort() method\n",
    "- live_data class?  Subclass for each subject?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "287.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "437px",
    "left": "869px",
    "right": "20px",
    "top": "121px",
    "width": "391px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
